{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lh</th>\n",
       "      <th>rh</th>\n",
       "      <th>ls</th>\n",
       "      <th>rs</th>\n",
       "      <th>w</th>\n",
       "      <th>ll</th>\n",
       "      <th>rl</th>\n",
       "      <th>1x</th>\n",
       "      <th>1y</th>\n",
       "      <th>2x</th>\n",
       "      <th>...</th>\n",
       "      <th>21x</th>\n",
       "      <th>21y</th>\n",
       "      <th>22x</th>\n",
       "      <th>22y</th>\n",
       "      <th>23x</th>\n",
       "      <th>23y</th>\n",
       "      <th>24x</th>\n",
       "      <th>24y</th>\n",
       "      <th>25x</th>\n",
       "      <th>25y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2104</td>\n",
       "      <td>6495</td>\n",
       "      <td>289</td>\n",
       "      <td>0</td>\n",
       "      <td>370</td>\n",
       "      <td>1001.090</td>\n",
       "      <td>224.108</td>\n",
       "      <td>998.361</td>\n",
       "      <td>...</td>\n",
       "      <td>1113.03</td>\n",
       "      <td>1015.90</td>\n",
       "      <td>1051.21</td>\n",
       "      <td>977.727</td>\n",
       "      <td>833.350</td>\n",
       "      <td>998.114</td>\n",
       "      <td>827.522</td>\n",
       "      <td>977.748</td>\n",
       "      <td>892.342</td>\n",
       "      <td>965.911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2595</td>\n",
       "      <td>6472</td>\n",
       "      <td>288</td>\n",
       "      <td>0</td>\n",
       "      <td>371</td>\n",
       "      <td>998.301</td>\n",
       "      <td>233.068</td>\n",
       "      <td>998.183</td>\n",
       "      <td>...</td>\n",
       "      <td>1110.15</td>\n",
       "      <td>1015.89</td>\n",
       "      <td>1051.20</td>\n",
       "      <td>980.579</td>\n",
       "      <td>821.632</td>\n",
       "      <td>998.192</td>\n",
       "      <td>815.856</td>\n",
       "      <td>980.623</td>\n",
       "      <td>895.222</td>\n",
       "      <td>968.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>6502</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>370</td>\n",
       "      <td>998.301</td>\n",
       "      <td>233.068</td>\n",
       "      <td>998.183</td>\n",
       "      <td>...</td>\n",
       "      <td>1110.15</td>\n",
       "      <td>1015.89</td>\n",
       "      <td>1051.20</td>\n",
       "      <td>980.579</td>\n",
       "      <td>821.632</td>\n",
       "      <td>998.192</td>\n",
       "      <td>815.856</td>\n",
       "      <td>980.623</td>\n",
       "      <td>895.222</td>\n",
       "      <td>968.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2152</td>\n",
       "      <td>6620</td>\n",
       "      <td>285</td>\n",
       "      <td>0</td>\n",
       "      <td>370</td>\n",
       "      <td>998.290</td>\n",
       "      <td>235.914</td>\n",
       "      <td>998.241</td>\n",
       "      <td>...</td>\n",
       "      <td>1110.13</td>\n",
       "      <td>1015.91</td>\n",
       "      <td>1048.35</td>\n",
       "      <td>977.770</td>\n",
       "      <td>818.732</td>\n",
       "      <td>998.183</td>\n",
       "      <td>815.843</td>\n",
       "      <td>980.596</td>\n",
       "      <td>895.207</td>\n",
       "      <td>968.786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2373</td>\n",
       "      <td>5716</td>\n",
       "      <td>282</td>\n",
       "      <td>0</td>\n",
       "      <td>368</td>\n",
       "      <td>998.338</td>\n",
       "      <td>235.914</td>\n",
       "      <td>998.328</td>\n",
       "      <td>...</td>\n",
       "      <td>1110.14</td>\n",
       "      <td>1015.91</td>\n",
       "      <td>1048.41</td>\n",
       "      <td>977.806</td>\n",
       "      <td>818.758</td>\n",
       "      <td>998.214</td>\n",
       "      <td>815.848</td>\n",
       "      <td>980.633</td>\n",
       "      <td>895.233</td>\n",
       "      <td>965.924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lh  rh    ls    rs    w  ll   rl        1x       1y       2x  ...      21x  \\\n",
       "0   0   0  2104  6495  289   0  370  1001.090  224.108  998.361  ...  1113.03   \n",
       "1   0   0  2595  6472  288   0  371   998.301  233.068  998.183  ...  1110.15   \n",
       "2   0   0  1987  6502  286   0  370   998.301  233.068  998.183  ...  1110.15   \n",
       "3   0   0  2152  6620  285   0  370   998.290  235.914  998.241  ...  1110.13   \n",
       "4   0   0  2373  5716  282   0  368   998.338  235.914  998.328  ...  1110.14   \n",
       "\n",
       "       21y      22x      22y      23x      23y      24x      24y      25x  \\\n",
       "0  1015.90  1051.21  977.727  833.350  998.114  827.522  977.748  892.342   \n",
       "1  1015.89  1051.20  980.579  821.632  998.192  815.856  980.623  895.222   \n",
       "2  1015.89  1051.20  980.579  821.632  998.192  815.856  980.623  895.222   \n",
       "3  1015.91  1048.35  977.770  818.732  998.183  815.843  980.596  895.207   \n",
       "4  1015.91  1048.41  977.806  818.758  998.214  815.848  980.633  895.233   \n",
       "\n",
       "       25y  \n",
       "0  965.911  \n",
       "1  968.837  \n",
       "2  968.837  \n",
       "3  968.786  \n",
       "4  965.924  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('action1.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = []\n",
    "\n",
    "# Store csv file in a Pandas DataFrame\n",
    "df = pd.read_csv('action1.csv')\n",
    "\n",
    "# Scaling the input data\n",
    "sc = MinMaxScaler()\n",
    "label_sc = MinMaxScaler()\n",
    "data = sc.fit_transform(df.values)\n",
    "\n",
    "# Obtaining the Scale for the labels(usage data) so that output can be re-scaled to actual value during evaluation\n",
    "label_sc.fit(df.iloc[:,7:57].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3367, 10, 57)\n",
      "(3367, 50)\n",
      "(3367, 10, 57)\n",
      "(3367, 50)\n"
     ]
    }
   ],
   "source": [
    "# Define lookback period and split inputs/labels\n",
    "lookback = 10\n",
    "inputs = np.zeros((len(data)-lookback,lookback,df.shape[1]))\n",
    "labels = np.zeros((len(data)-lookback,50))\n",
    "print(inputs.shape)\n",
    "print(labels.shape)\n",
    "for i in range(lookback, len(data)):\n",
    "    inputs[i-lookback] = data[i-lookback:i]\n",
    "    labels[i-lookback] = data[i][7:57]\n",
    "inputs = inputs.reshape(-1,lookback,df.shape[1])\n",
    "labels = labels.reshape(-1,50)\n",
    "print(inputs.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train/test portions and combining all data from different files into a single array\n",
    "test_portion = int(0.1*len(inputs))\n",
    "train_x = inputs[:-test_portion]\n",
    "train_y = labels[:-test_portion]\n",
    "test_x = inputs[-test_portion:]\n",
    "test_y = labels[-test_portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10, 57])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "input_dim = next(iter(train_loader))[0].shape\n",
    "print(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, learn_rate, hidden_dim=256, EPOCHS=5, model_type=\"GRU\"):\n",
    "    \n",
    "    # Setting common hyperparameters\n",
    "    input_dim = next(iter(train_loader))[0].shape[2]\n",
    "    output_dim = 50\n",
    "    n_layers = 2\n",
    "    # Instantiating the model\n",
    "    model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Defining loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "    \n",
    "    model.train()\n",
    "    print(\"Starting Training of {} model\".format(model_type))\n",
    "    epoch_times = []\n",
    "    # Start training loop\n",
    "    for epoch in range(1,EPOCHS+1):\n",
    "        start_time = time.clock()\n",
    "        h = model.init_hidden(batch_size)\n",
    "        avg_loss = 0.\n",
    "        counter = 0\n",
    "        for x, label in train_loader:\n",
    "            counter += 1\n",
    "            h = h.data\n",
    "            model.zero_grad()\n",
    "            \n",
    "            out, h = model(x.to(device).float(), h)\n",
    "            loss = criterion(out, label.to(device).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "            #if counter%20 == 0:\n",
    "            print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, counter, len(train_loader), avg_loss/counter))\n",
    "        current_time = time.clock()\n",
    "        print(\"Epoch {}/{} Done, Total Loss: {}\".format(epoch, EPOCHS, avg_loss/len(train_loader)))\n",
    "        print(\"Time Elapsed for Epoch: {} seconds\".format(str(current_time-start_time)))\n",
    "        epoch_times.append(current_time-start_time)\n",
    "    print(\"Total Training Time: {} seconds\".format(str(sum(epoch_times))))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_x, test_y, label_sc):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    start_time = time.clock()\n",
    "    \n",
    "    inp = torch.from_numpy(np.array(test_x))\n",
    "    labs = torch.from_numpy(np.array(test_y))\n",
    "    h = model.init_hidden(inp.shape[0])\n",
    "    out, h = model(inp.to(device).float(), h)\n",
    "    outputs.append(label_sc.inverse_transform(out.cpu().detach().numpy()).reshape(-1))\n",
    "    targets.append(label_sc.inverse_transform(labs.numpy()).reshape(-1))\n",
    "    \n",
    "    print(\"Evaluation Time: {}\".format(str(time.clock()-start_time)))\n",
    "    sMAPE = 0\n",
    "    for i in range(len(outputs)):\n",
    "        sMAPE += np.mean(abs(outputs[i]-targets[i])/(targets[i]+outputs[i])/2)/len(outputs)\n",
    "    print(\"sMAPE: {}%\".format(sMAPE*100))\n",
    "    return outputs, targets, sMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training of GRU model\n",
      "Epoch 1......Step: 1/303....... Average Loss for Epoch: 0.3875701129436493\n",
      "Epoch 1......Step: 2/303....... Average Loss for Epoch: 0.3509846329689026\n",
      "Epoch 1......Step: 3/303....... Average Loss for Epoch: 0.3139396260182063\n",
      "Epoch 1......Step: 4/303....... Average Loss for Epoch: 0.28065475448966026\n",
      "Epoch 1......Step: 5/303....... Average Loss for Epoch: 0.24609697014093398\n",
      "Epoch 1......Step: 6/303....... Average Loss for Epoch: 0.21879583348830542\n",
      "Epoch 1......Step: 7/303....... Average Loss for Epoch: 0.2000917728458132\n",
      "Epoch 1......Step: 8/303....... Average Loss for Epoch: 0.1849540313705802\n",
      "Epoch 1......Step: 9/303....... Average Loss for Epoch: 0.17049672620164025\n",
      "Epoch 1......Step: 10/303....... Average Loss for Epoch: 0.15773952268064023\n",
      "Epoch 1......Step: 11/303....... Average Loss for Epoch: 0.14706529846245592\n",
      "Epoch 1......Step: 12/303....... Average Loss for Epoch: 0.13887699724485478\n",
      "Epoch 1......Step: 13/303....... Average Loss for Epoch: 0.1314824495751124\n",
      "Epoch 1......Step: 14/303....... Average Loss for Epoch: 0.12509612632649286\n",
      "Epoch 1......Step: 15/303....... Average Loss for Epoch: 0.12018631299336752\n",
      "Epoch 1......Step: 16/303....... Average Loss for Epoch: 0.11488613323308527\n",
      "Epoch 1......Step: 17/303....... Average Loss for Epoch: 0.1109619879109018\n",
      "Epoch 1......Step: 18/303....... Average Loss for Epoch: 0.10749710434012943\n",
      "Epoch 1......Step: 19/303....... Average Loss for Epoch: 0.10354491813402426\n",
      "Epoch 1......Step: 20/303....... Average Loss for Epoch: 0.10002156421542167\n",
      "Epoch 1......Step: 21/303....... Average Loss for Epoch: 0.09678773670679047\n",
      "Epoch 1......Step: 22/303....... Average Loss for Epoch: 0.09420525181022557\n",
      "Epoch 1......Step: 23/303....... Average Loss for Epoch: 0.09169915998759477\n",
      "Epoch 1......Step: 24/303....... Average Loss for Epoch: 0.08939701939622562\n",
      "Epoch 1......Step: 25/303....... Average Loss for Epoch: 0.08701966278254986\n",
      "Epoch 1......Step: 26/303....... Average Loss for Epoch: 0.0849609633621115\n",
      "Epoch 1......Step: 27/303....... Average Loss for Epoch: 0.08338285382423136\n",
      "Epoch 1......Step: 28/303....... Average Loss for Epoch: 0.0819462118005114\n",
      "Epoch 1......Step: 29/303....... Average Loss for Epoch: 0.08057504384939013\n",
      "Epoch 1......Step: 30/303....... Average Loss for Epoch: 0.07906146515160799\n",
      "Epoch 1......Step: 31/303....... Average Loss for Epoch: 0.07752174817987027\n",
      "Epoch 1......Step: 32/303....... Average Loss for Epoch: 0.07608955999603495\n",
      "Epoch 1......Step: 33/303....... Average Loss for Epoch: 0.07501231986239101\n",
      "Epoch 1......Step: 34/303....... Average Loss for Epoch: 0.07376559412873843\n",
      "Epoch 1......Step: 35/303....... Average Loss for Epoch: 0.07260771548109395\n",
      "Epoch 1......Step: 36/303....... Average Loss for Epoch: 0.07152689905423257\n",
      "Epoch 1......Step: 37/303....... Average Loss for Epoch: 0.07040509536258273\n",
      "Epoch 1......Step: 38/303....... Average Loss for Epoch: 0.06928808859696514\n",
      "Epoch 1......Step: 39/303....... Average Loss for Epoch: 0.06826674837905627\n",
      "Epoch 1......Step: 40/303....... Average Loss for Epoch: 0.06728904875926674\n",
      "Epoch 1......Step: 41/303....... Average Loss for Epoch: 0.0664723625906357\n",
      "Epoch 1......Step: 42/303....... Average Loss for Epoch: 0.0655253930904326\n",
      "Epoch 1......Step: 43/303....... Average Loss for Epoch: 0.0648198172363431\n",
      "Epoch 1......Step: 44/303....... Average Loss for Epoch: 0.06410434194417163\n",
      "Epoch 1......Step: 45/303....... Average Loss for Epoch: 0.0632127806958225\n",
      "Epoch 1......Step: 46/303....... Average Loss for Epoch: 0.06242060803038919\n",
      "Epoch 1......Step: 47/303....... Average Loss for Epoch: 0.06176176560210421\n",
      "Epoch 1......Step: 48/303....... Average Loss for Epoch: 0.061275344768849514\n",
      "Epoch 1......Step: 49/303....... Average Loss for Epoch: 0.06052391968515455\n",
      "Epoch 1......Step: 50/303....... Average Loss for Epoch: 0.05988937087357044\n",
      "Epoch 1......Step: 51/303....... Average Loss for Epoch: 0.059267802894407626\n",
      "Epoch 1......Step: 52/303....... Average Loss for Epoch: 0.05851663165510847\n",
      "Epoch 1......Step: 53/303....... Average Loss for Epoch: 0.057845765162470204\n",
      "Epoch 1......Step: 54/303....... Average Loss for Epoch: 0.05732081833950899\n",
      "Epoch 1......Step: 55/303....... Average Loss for Epoch: 0.05673625012013045\n",
      "Epoch 1......Step: 56/303....... Average Loss for Epoch: 0.05637350841425359\n",
      "Epoch 1......Step: 57/303....... Average Loss for Epoch: 0.05590755626428546\n",
      "Epoch 1......Step: 58/303....... Average Loss for Epoch: 0.055313551380973436\n",
      "Epoch 1......Step: 59/303....... Average Loss for Epoch: 0.054766782448958544\n",
      "Epoch 1......Step: 60/303....... Average Loss for Epoch: 0.05421813310434421\n",
      "Epoch 1......Step: 61/303....... Average Loss for Epoch: 0.053786982644776826\n",
      "Epoch 1......Step: 62/303....... Average Loss for Epoch: 0.05343540229143635\n",
      "Epoch 1......Step: 63/303....... Average Loss for Epoch: 0.05296150234247011\n",
      "Epoch 1......Step: 64/303....... Average Loss for Epoch: 0.05268349591642618\n",
      "Epoch 1......Step: 65/303....... Average Loss for Epoch: 0.05230054133213483\n",
      "Epoch 1......Step: 66/303....... Average Loss for Epoch: 0.05199300712256721\n",
      "Epoch 1......Step: 67/303....... Average Loss for Epoch: 0.0517180639861235\n",
      "Epoch 1......Step: 68/303....... Average Loss for Epoch: 0.05132760367739726\n",
      "Epoch 1......Step: 69/303....... Average Loss for Epoch: 0.051010090884739075\n",
      "Epoch 1......Step: 70/303....... Average Loss for Epoch: 0.05058094590370144\n",
      "Epoch 1......Step: 71/303....... Average Loss for Epoch: 0.050156353277639606\n",
      "Epoch 1......Step: 72/303....... Average Loss for Epoch: 0.04975011435130404\n",
      "Epoch 1......Step: 73/303....... Average Loss for Epoch: 0.04942640876525069\n",
      "Epoch 1......Step: 74/303....... Average Loss for Epoch: 0.049143776524107195\n",
      "Epoch 1......Step: 75/303....... Average Loss for Epoch: 0.048953670139114065\n",
      "Epoch 1......Step: 76/303....... Average Loss for Epoch: 0.048660374123995244\n",
      "Epoch 1......Step: 77/303....... Average Loss for Epoch: 0.0483034159262459\n",
      "Epoch 1......Step: 78/303....... Average Loss for Epoch: 0.047942276805257186\n",
      "Epoch 1......Step: 79/303....... Average Loss for Epoch: 0.04769060147714011\n",
      "Epoch 1......Step: 80/303....... Average Loss for Epoch: 0.04763451535254717\n",
      "Epoch 1......Step: 81/303....... Average Loss for Epoch: 0.04742108743039914\n",
      "Epoch 1......Step: 82/303....... Average Loss for Epoch: 0.04723452688081235\n",
      "Epoch 1......Step: 83/303....... Average Loss for Epoch: 0.04705642601362912\n",
      "Epoch 1......Step: 84/303....... Average Loss for Epoch: 0.04682090993793238\n",
      "Epoch 1......Step: 85/303....... Average Loss for Epoch: 0.046586705985314704\n",
      "Epoch 1......Step: 86/303....... Average Loss for Epoch: 0.04635518076721319\n",
      "Epoch 1......Step: 87/303....... Average Loss for Epoch: 0.04615780115983952\n",
      "Epoch 1......Step: 88/303....... Average Loss for Epoch: 0.04597353143617511\n",
      "Epoch 1......Step: 89/303....... Average Loss for Epoch: 0.04576222244859411\n",
      "Epoch 1......Step: 90/303....... Average Loss for Epoch: 0.045469791255891324\n",
      "Epoch 1......Step: 91/303....... Average Loss for Epoch: 0.04521798504168516\n",
      "Epoch 1......Step: 92/303....... Average Loss for Epoch: 0.045033035030507526\n",
      "Epoch 1......Step: 93/303....... Average Loss for Epoch: 0.04477467347857773\n",
      "Epoch 1......Step: 94/303....... Average Loss for Epoch: 0.04462062654660103\n",
      "Epoch 1......Step: 95/303....... Average Loss for Epoch: 0.0443419573730544\n",
      "Epoch 1......Step: 96/303....... Average Loss for Epoch: 0.044064258496897914\n",
      "Epoch 1......Step: 97/303....... Average Loss for Epoch: 0.04397203377688054\n",
      "Epoch 1......Step: 98/303....... Average Loss for Epoch: 0.04378380746181522\n",
      "Epoch 1......Step: 99/303....... Average Loss for Epoch: 0.043589307145789416\n",
      "Epoch 1......Step: 100/303....... Average Loss for Epoch: 0.043357691522687675\n",
      "Epoch 1......Step: 101/303....... Average Loss for Epoch: 0.04311721930556958\n",
      "Epoch 1......Step: 102/303....... Average Loss for Epoch: 0.04294289148175249\n",
      "Epoch 1......Step: 103/303....... Average Loss for Epoch: 0.04274747281977274\n",
      "Epoch 1......Step: 104/303....... Average Loss for Epoch: 0.04257106272360453\n",
      "Epoch 1......Step: 105/303....... Average Loss for Epoch: 0.04238774804841904\n",
      "Epoch 1......Step: 106/303....... Average Loss for Epoch: 0.042233785034491204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1......Step: 107/303....... Average Loss for Epoch: 0.04202242799660313\n",
      "Epoch 1......Step: 108/303....... Average Loss for Epoch: 0.041848241041103997\n",
      "Epoch 1......Step: 109/303....... Average Loss for Epoch: 0.04164307026209634\n",
      "Epoch 1......Step: 110/303....... Average Loss for Epoch: 0.04150917709551074\n",
      "Epoch 1......Step: 111/303....... Average Loss for Epoch: 0.041379698157847464\n",
      "Epoch 1......Step: 112/303....... Average Loss for Epoch: 0.04130356981685119\n",
      "Epoch 1......Step: 113/303....... Average Loss for Epoch: 0.04116707016606774\n",
      "Epoch 1......Step: 114/303....... Average Loss for Epoch: 0.040969513634448514\n",
      "Epoch 1......Step: 115/303....... Average Loss for Epoch: 0.04081545638973298\n",
      "Epoch 1......Step: 116/303....... Average Loss for Epoch: 0.04072976409425509\n",
      "Epoch 1......Step: 117/303....... Average Loss for Epoch: 0.04061866295324941\n",
      "Epoch 1......Step: 118/303....... Average Loss for Epoch: 0.04044642228380603\n",
      "Epoch 1......Step: 119/303....... Average Loss for Epoch: 0.040268987871143\n",
      "Epoch 1......Step: 120/303....... Average Loss for Epoch: 0.04010324287228286\n",
      "Epoch 1......Step: 121/303....... Average Loss for Epoch: 0.03994355269145867\n",
      "Epoch 1......Step: 122/303....... Average Loss for Epoch: 0.0397974108880172\n",
      "Epoch 1......Step: 123/303....... Average Loss for Epoch: 0.03969669891748487\n",
      "Epoch 1......Step: 124/303....... Average Loss for Epoch: 0.03953942755657819\n",
      "Epoch 1......Step: 125/303....... Average Loss for Epoch: 0.03941717298328876\n",
      "Epoch 1......Step: 126/303....... Average Loss for Epoch: 0.039239150501551134\n",
      "Epoch 1......Step: 127/303....... Average Loss for Epoch: 0.039165443953449335\n",
      "Epoch 1......Step: 128/303....... Average Loss for Epoch: 0.03905878582736477\n",
      "Epoch 1......Step: 129/303....... Average Loss for Epoch: 0.03892281177894089\n",
      "Epoch 1......Step: 130/303....... Average Loss for Epoch: 0.0387732836489494\n",
      "Epoch 1......Step: 131/303....... Average Loss for Epoch: 0.03862647486153905\n",
      "Epoch 1......Step: 132/303....... Average Loss for Epoch: 0.03859785516661676\n",
      "Epoch 1......Step: 133/303....... Average Loss for Epoch: 0.03847516695023479\n",
      "Epoch 1......Step: 134/303....... Average Loss for Epoch: 0.038349747449270825\n",
      "Epoch 1......Step: 135/303....... Average Loss for Epoch: 0.03822294143890893\n",
      "Epoch 1......Step: 136/303....... Average Loss for Epoch: 0.03813522452872027\n",
      "Epoch 1......Step: 137/303....... Average Loss for Epoch: 0.03799436725404141\n",
      "Epoch 1......Step: 138/303....... Average Loss for Epoch: 0.037845475545180016\n",
      "Epoch 1......Step: 139/303....... Average Loss for Epoch: 0.03775560012985048\n",
      "Epoch 1......Step: 140/303....... Average Loss for Epoch: 0.03763310929228152\n",
      "Epoch 1......Step: 141/303....... Average Loss for Epoch: 0.03755365604045966\n",
      "Epoch 1......Step: 142/303....... Average Loss for Epoch: 0.03742079341023321\n",
      "Epoch 1......Step: 143/303....... Average Loss for Epoch: 0.03731068060896196\n",
      "Epoch 1......Step: 144/303....... Average Loss for Epoch: 0.0371947861664618\n",
      "Epoch 1......Step: 145/303....... Average Loss for Epoch: 0.03707787072093322\n",
      "Epoch 1......Step: 146/303....... Average Loss for Epoch: 0.036958624226079415\n",
      "Epoch 1......Step: 147/303....... Average Loss for Epoch: 0.036833535639100336\n",
      "Epoch 1......Step: 148/303....... Average Loss for Epoch: 0.03674962370329209\n",
      "Epoch 1......Step: 149/303....... Average Loss for Epoch: 0.03666810018804249\n",
      "Epoch 1......Step: 150/303....... Average Loss for Epoch: 0.03656388903657595\n",
      "Epoch 1......Step: 151/303....... Average Loss for Epoch: 0.03646010452795108\n",
      "Epoch 1......Step: 152/303....... Average Loss for Epoch: 0.03642147866469857\n",
      "Epoch 1......Step: 153/303....... Average Loss for Epoch: 0.036449766638719176\n",
      "Epoch 1......Step: 154/303....... Average Loss for Epoch: 0.03635766583361796\n",
      "Epoch 1......Step: 155/303....... Average Loss for Epoch: 0.03623841234272526\n",
      "Epoch 1......Step: 156/303....... Average Loss for Epoch: 0.036179598014897264\n",
      "Epoch 1......Step: 157/303....... Average Loss for Epoch: 0.0360944590228758\n",
      "Epoch 1......Step: 158/303....... Average Loss for Epoch: 0.036004493991502476\n",
      "Epoch 1......Step: 159/303....... Average Loss for Epoch: 0.035920700058341026\n",
      "Epoch 1......Step: 160/303....... Average Loss for Epoch: 0.03586393692530691\n",
      "Epoch 1......Step: 161/303....... Average Loss for Epoch: 0.03579653674586219\n",
      "Epoch 1......Step: 162/303....... Average Loss for Epoch: 0.03577143262013977\n",
      "Epoch 1......Step: 163/303....... Average Loss for Epoch: 0.03570448490648182\n",
      "Epoch 1......Step: 164/303....... Average Loss for Epoch: 0.03560771045797482\n",
      "Epoch 1......Step: 165/303....... Average Loss for Epoch: 0.035510330963315385\n",
      "Epoch 1......Step: 166/303....... Average Loss for Epoch: 0.03545664387743875\n",
      "Epoch 1......Step: 167/303....... Average Loss for Epoch: 0.035447348556118814\n",
      "Epoch 1......Step: 168/303....... Average Loss for Epoch: 0.03537600725844857\n",
      "Epoch 1......Step: 169/303....... Average Loss for Epoch: 0.035394855449802774\n",
      "Epoch 1......Step: 170/303....... Average Loss for Epoch: 0.035311116398695636\n",
      "Epoch 1......Step: 171/303....... Average Loss for Epoch: 0.035212143373332526\n",
      "Epoch 1......Step: 172/303....... Average Loss for Epoch: 0.035194543959183054\n",
      "Epoch 1......Step: 173/303....... Average Loss for Epoch: 0.0352367761657934\n",
      "Epoch 1......Step: 174/303....... Average Loss for Epoch: 0.035177491937132405\n",
      "Epoch 1......Step: 175/303....... Average Loss for Epoch: 0.03515609101525375\n",
      "Epoch 1......Step: 176/303....... Average Loss for Epoch: 0.035089606609703464\n",
      "Epoch 1......Step: 177/303....... Average Loss for Epoch: 0.03503532896060391\n",
      "Epoch 1......Step: 178/303....... Average Loss for Epoch: 0.03495285100188483\n",
      "Epoch 1......Step: 179/303....... Average Loss for Epoch: 0.034857553195770226\n",
      "Epoch 1......Step: 180/303....... Average Loss for Epoch: 0.034788276565571624\n",
      "Epoch 1......Step: 181/303....... Average Loss for Epoch: 0.03469930716805695\n",
      "Epoch 1......Step: 182/303....... Average Loss for Epoch: 0.03461607336834237\n",
      "Epoch 1......Step: 183/303....... Average Loss for Epoch: 0.03454845147258271\n",
      "Epoch 1......Step: 184/303....... Average Loss for Epoch: 0.034475410792409726\n",
      "Epoch 1......Step: 185/303....... Average Loss for Epoch: 0.034436903148889544\n",
      "Epoch 1......Step: 186/303....... Average Loss for Epoch: 0.03435870591470952\n",
      "Epoch 1......Step: 187/303....... Average Loss for Epoch: 0.03430777870835786\n",
      "Epoch 1......Step: 188/303....... Average Loss for Epoch: 0.034260607116479186\n",
      "Epoch 1......Step: 189/303....... Average Loss for Epoch: 0.034233880490458835\n",
      "Epoch 1......Step: 190/303....... Average Loss for Epoch: 0.034155822564896784\n",
      "Epoch 1......Step: 191/303....... Average Loss for Epoch: 0.0340818852888352\n",
      "Epoch 1......Step: 192/303....... Average Loss for Epoch: 0.03406077376954878\n",
      "Epoch 1......Step: 193/303....... Average Loss for Epoch: 0.03400538772962254\n",
      "Epoch 1......Step: 194/303....... Average Loss for Epoch: 0.033928709661530464\n",
      "Epoch 1......Step: 195/303....... Average Loss for Epoch: 0.03390128836990931\n",
      "Epoch 1......Step: 196/303....... Average Loss for Epoch: 0.033819858060807596\n",
      "Epoch 1......Step: 197/303....... Average Loss for Epoch: 0.033748552751571397\n",
      "Epoch 1......Step: 198/303....... Average Loss for Epoch: 0.03382517260057156\n",
      "Epoch 1......Step: 199/303....... Average Loss for Epoch: 0.033819140295437235\n",
      "Epoch 1......Step: 200/303....... Average Loss for Epoch: 0.03381643230095506\n",
      "Epoch 1......Step: 201/303....... Average Loss for Epoch: 0.03376584324930141\n",
      "Epoch 1......Step: 202/303....... Average Loss for Epoch: 0.03367528135860615\n",
      "Epoch 1......Step: 203/303....... Average Loss for Epoch: 0.03368845891225808\n",
      "Epoch 1......Step: 204/303....... Average Loss for Epoch: 0.03363337760389436\n",
      "Epoch 1......Step: 205/303....... Average Loss for Epoch: 0.03355184248307856\n",
      "Epoch 1......Step: 206/303....... Average Loss for Epoch: 0.0334924638488339\n",
      "Epoch 1......Step: 207/303....... Average Loss for Epoch: 0.033428125857299075\n",
      "Epoch 1......Step: 208/303....... Average Loss for Epoch: 0.033449645387008786\n",
      "Epoch 1......Step: 209/303....... Average Loss for Epoch: 0.033406585258872884\n",
      "Epoch 1......Step: 210/303....... Average Loss for Epoch: 0.03335286379747447\n",
      "Epoch 1......Step: 211/303....... Average Loss for Epoch: 0.03330273902423291\n",
      "Epoch 1......Step: 212/303....... Average Loss for Epoch: 0.03326921743990959\n",
      "Epoch 1......Step: 213/303....... Average Loss for Epoch: 0.03322276868310892\n",
      "Epoch 1......Step: 214/303....... Average Loss for Epoch: 0.033192766473463205\n",
      "Epoch 1......Step: 215/303....... Average Loss for Epoch: 0.03317251191582791\n",
      "Epoch 1......Step: 216/303....... Average Loss for Epoch: 0.03313508851419168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1......Step: 217/303....... Average Loss for Epoch: 0.033083215450292906\n",
      "Epoch 1......Step: 218/303....... Average Loss for Epoch: 0.03309670390578311\n",
      "Epoch 1......Step: 219/303....... Average Loss for Epoch: 0.03305005872426512\n",
      "Epoch 1......Step: 220/303....... Average Loss for Epoch: 0.032981715698472476\n",
      "Epoch 1......Step: 221/303....... Average Loss for Epoch: 0.03293455532874187\n",
      "Epoch 1......Step: 222/303....... Average Loss for Epoch: 0.0328967192352892\n",
      "Epoch 1......Step: 223/303....... Average Loss for Epoch: 0.032845767139720275\n",
      "Epoch 1......Step: 224/303....... Average Loss for Epoch: 0.032806078803592494\n",
      "Epoch 1......Step: 225/303....... Average Loss for Epoch: 0.032768627951542534\n",
      "Epoch 1......Step: 226/303....... Average Loss for Epoch: 0.03271700037165289\n",
      "Epoch 1......Step: 227/303....... Average Loss for Epoch: 0.0326549910205839\n",
      "Epoch 1......Step: 228/303....... Average Loss for Epoch: 0.03263618416365301\n",
      "Epoch 1......Step: 229/303....... Average Loss for Epoch: 0.03258991930389769\n",
      "Epoch 1......Step: 230/303....... Average Loss for Epoch: 0.0325804620495309\n",
      "Epoch 1......Step: 231/303....... Average Loss for Epoch: 0.032515814890722174\n",
      "Epoch 1......Step: 232/303....... Average Loss for Epoch: 0.032470852963562155\n",
      "Epoch 1......Step: 233/303....... Average Loss for Epoch: 0.03242753003224283\n",
      "Epoch 1......Step: 234/303....... Average Loss for Epoch: 0.03238118416828732\n",
      "Epoch 1......Step: 235/303....... Average Loss for Epoch: 0.0323338037396365\n",
      "Epoch 1......Step: 236/303....... Average Loss for Epoch: 0.03228687153245181\n",
      "Epoch 1......Step: 237/303....... Average Loss for Epoch: 0.03222975035036918\n",
      "Epoch 1......Step: 238/303....... Average Loss for Epoch: 0.032170587782787176\n",
      "Epoch 1......Step: 239/303....... Average Loss for Epoch: 0.03218383096532592\n",
      "Epoch 1......Step: 240/303....... Average Loss for Epoch: 0.03217411655156563\n",
      "Epoch 1......Step: 241/303....... Average Loss for Epoch: 0.032164489773113696\n",
      "Epoch 1......Step: 242/303....... Average Loss for Epoch: 0.03210351915552843\n",
      "Epoch 1......Step: 243/303....... Average Loss for Epoch: 0.032096573132485026\n",
      "Epoch 1......Step: 244/303....... Average Loss for Epoch: 0.03203964437220673\n",
      "Epoch 1......Step: 245/303....... Average Loss for Epoch: 0.03197411852983796\n",
      "Epoch 1......Step: 246/303....... Average Loss for Epoch: 0.031964547954863164\n",
      "Epoch 1......Step: 247/303....... Average Loss for Epoch: 0.03192756492418316\n",
      "Epoch 1......Step: 248/303....... Average Loss for Epoch: 0.031904062301281\n",
      "Epoch 1......Step: 249/303....... Average Loss for Epoch: 0.03190432240565618\n",
      "Epoch 1......Step: 250/303....... Average Loss for Epoch: 0.03184688921272755\n",
      "Epoch 1......Step: 251/303....... Average Loss for Epoch: 0.03178778674408972\n",
      "Epoch 1......Step: 252/303....... Average Loss for Epoch: 0.03174750476572958\n",
      "Epoch 1......Step: 253/303....... Average Loss for Epoch: 0.03171211339003248\n",
      "Epoch 1......Step: 254/303....... Average Loss for Epoch: 0.03165730417478742\n",
      "Epoch 1......Step: 255/303....... Average Loss for Epoch: 0.03164406122968477\n",
      "Epoch 1......Step: 256/303....... Average Loss for Epoch: 0.031600311362126376\n",
      "Epoch 1......Step: 257/303....... Average Loss for Epoch: 0.031551372680218764\n",
      "Epoch 1......Step: 258/303....... Average Loss for Epoch: 0.03151470671819393\n",
      "Epoch 1......Step: 259/303....... Average Loss for Epoch: 0.03148210536271449\n",
      "Epoch 1......Step: 260/303....... Average Loss for Epoch: 0.03144879589000574\n",
      "Epoch 1......Step: 261/303....... Average Loss for Epoch: 0.03140290793730838\n",
      "Epoch 1......Step: 262/303....... Average Loss for Epoch: 0.03137341020131157\n",
      "Epoch 1......Step: 263/303....... Average Loss for Epoch: 0.03137988640356653\n",
      "Epoch 1......Step: 264/303....... Average Loss for Epoch: 0.03134825874373994\n",
      "Epoch 1......Step: 265/303....... Average Loss for Epoch: 0.03129970800342425\n",
      "Epoch 1......Step: 266/303....... Average Loss for Epoch: 0.031245065867004537\n",
      "Epoch 1......Step: 267/303....... Average Loss for Epoch: 0.03123021527622523\n",
      "Epoch 1......Step: 268/303....... Average Loss for Epoch: 0.031226308807841878\n",
      "Epoch 1......Step: 269/303....... Average Loss for Epoch: 0.03118888767900077\n",
      "Epoch 1......Step: 270/303....... Average Loss for Epoch: 0.031162400074579096\n",
      "Epoch 1......Step: 271/303....... Average Loss for Epoch: 0.031154644548398103\n",
      "Epoch 1......Step: 272/303....... Average Loss for Epoch: 0.031114993329324266\n",
      "Epoch 1......Step: 273/303....... Average Loss for Epoch: 0.031060990990026967\n",
      "Epoch 1......Step: 274/303....... Average Loss for Epoch: 0.031031510928632135\n",
      "Epoch 1......Step: 275/303....... Average Loss for Epoch: 0.03097930013456128\n",
      "Epoch 1......Step: 276/303....... Average Loss for Epoch: 0.030929354964283066\n",
      "Epoch 1......Step: 277/303....... Average Loss for Epoch: 0.030883224566705822\n",
      "Epoch 1......Step: 278/303....... Average Loss for Epoch: 0.03085260408789777\n",
      "Epoch 1......Step: 279/303....... Average Loss for Epoch: 0.030829423370914648\n",
      "Epoch 1......Step: 280/303....... Average Loss for Epoch: 0.030811874302370208\n",
      "Epoch 1......Step: 281/303....... Average Loss for Epoch: 0.030769430266633577\n",
      "Epoch 1......Step: 282/303....... Average Loss for Epoch: 0.030721420646770626\n",
      "Epoch 1......Step: 283/303....... Average Loss for Epoch: 0.030689074022715167\n",
      "Epoch 1......Step: 284/303....... Average Loss for Epoch: 0.030642911129381875\n",
      "Epoch 1......Step: 285/303....... Average Loss for Epoch: 0.03059815554634521\n",
      "Epoch 1......Step: 286/303....... Average Loss for Epoch: 0.03059787464074113\n",
      "Epoch 1......Step: 287/303....... Average Loss for Epoch: 0.03061616165686775\n",
      "Epoch 1......Step: 288/303....... Average Loss for Epoch: 0.03057458576384104\n",
      "Epoch 1......Step: 289/303....... Average Loss for Epoch: 0.03052886713386407\n",
      "Epoch 1......Step: 290/303....... Average Loss for Epoch: 0.030525423926783018\n",
      "Epoch 1......Step: 291/303....... Average Loss for Epoch: 0.03051830447979809\n",
      "Epoch 1......Step: 292/303....... Average Loss for Epoch: 0.030500071780271316\n",
      "Epoch 1......Step: 293/303....... Average Loss for Epoch: 0.030470622792433146\n",
      "Epoch 1......Step: 294/303....... Average Loss for Epoch: 0.03043952346684373\n",
      "Epoch 1......Step: 295/303....... Average Loss for Epoch: 0.030425588155196887\n",
      "Epoch 1......Step: 296/303....... Average Loss for Epoch: 0.03043619413683946\n",
      "Epoch 1......Step: 297/303....... Average Loss for Epoch: 0.03038929159492756\n",
      "Epoch 1......Step: 298/303....... Average Loss for Epoch: 0.030337585455069205\n",
      "Epoch 1......Step: 299/303....... Average Loss for Epoch: 0.0302980658302339\n",
      "Epoch 1......Step: 300/303....... Average Loss for Epoch: 0.030282656836013\n",
      "Epoch 1......Step: 301/303....... Average Loss for Epoch: 0.03023726464035701\n",
      "Epoch 1......Step: 302/303....... Average Loss for Epoch: 0.03019705807037701\n",
      "Epoch 1......Step: 303/303....... Average Loss for Epoch: 0.030181506267524395\n",
      "Epoch 1/5 Done, Total Loss: 0.030181506267524395\n",
      "Time Elapsed for Epoch: 6.501153400000001 seconds\n",
      "Epoch 2......Step: 1/303....... Average Loss for Epoch: 0.016458939760923386\n",
      "Epoch 2......Step: 2/303....... Average Loss for Epoch: 0.0204850435256958\n",
      "Epoch 2......Step: 3/303....... Average Loss for Epoch: 0.020872826998432476\n",
      "Epoch 2......Step: 4/303....... Average Loss for Epoch: 0.022675286047160625\n",
      "Epoch 2......Step: 5/303....... Average Loss for Epoch: 0.02245590090751648\n",
      "Epoch 2......Step: 6/303....... Average Loss for Epoch: 0.021609029732644558\n",
      "Epoch 2......Step: 7/303....... Average Loss for Epoch: 0.021045144647359848\n",
      "Epoch 2......Step: 8/303....... Average Loss for Epoch: 0.020871988963335752\n",
      "Epoch 2......Step: 9/303....... Average Loss for Epoch: 0.019987222945524588\n",
      "Epoch 2......Step: 10/303....... Average Loss for Epoch: 0.01998226037248969\n",
      "Epoch 2......Step: 11/303....... Average Loss for Epoch: 0.02134355047548359\n",
      "Epoch 2......Step: 12/303....... Average Loss for Epoch: 0.02120084664784372\n",
      "Epoch 2......Step: 13/303....... Average Loss for Epoch: 0.021277914898326762\n",
      "Epoch 2......Step: 14/303....... Average Loss for Epoch: 0.02193168011893119\n",
      "Epoch 2......Step: 15/303....... Average Loss for Epoch: 0.02172459668169419\n",
      "Epoch 2......Step: 16/303....... Average Loss for Epoch: 0.021259556349832565\n",
      "Epoch 2......Step: 17/303....... Average Loss for Epoch: 0.02089673905249904\n",
      "Epoch 2......Step: 18/303....... Average Loss for Epoch: 0.02098117944680982\n",
      "Epoch 2......Step: 19/303....... Average Loss for Epoch: 0.02110776069917177\n",
      "Epoch 2......Step: 20/303....... Average Loss for Epoch: 0.020884006377309562\n",
      "Epoch 2......Step: 21/303....... Average Loss for Epoch: 0.0209024887354601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2......Step: 22/303....... Average Loss for Epoch: 0.02071008678864349\n",
      "Epoch 2......Step: 23/303....... Average Loss for Epoch: 0.020980947529492172\n",
      "Epoch 2......Step: 24/303....... Average Loss for Epoch: 0.021118541558583576\n",
      "Epoch 2......Step: 25/303....... Average Loss for Epoch: 0.021373300179839134\n",
      "Epoch 2......Step: 26/303....... Average Loss for Epoch: 0.021291315627212707\n",
      "Epoch 2......Step: 27/303....... Average Loss for Epoch: 0.021408274079914445\n",
      "Epoch 2......Step: 28/303....... Average Loss for Epoch: 0.021371545868792703\n",
      "Epoch 2......Step: 29/303....... Average Loss for Epoch: 0.021159436425258374\n",
      "Epoch 2......Step: 30/303....... Average Loss for Epoch: 0.0215206457922856\n",
      "Epoch 2......Step: 31/303....... Average Loss for Epoch: 0.021464107918643182\n",
      "Epoch 2......Step: 32/303....... Average Loss for Epoch: 0.02147885039448738\n",
      "Epoch 2......Step: 33/303....... Average Loss for Epoch: 0.021398923388033203\n",
      "Epoch 2......Step: 34/303....... Average Loss for Epoch: 0.021271331509684816\n",
      "Epoch 2......Step: 35/303....... Average Loss for Epoch: 0.02125771003110068\n",
      "Epoch 2......Step: 36/303....... Average Loss for Epoch: 0.021253291827936966\n",
      "Epoch 2......Step: 37/303....... Average Loss for Epoch: 0.02107769505095643\n",
      "Epoch 2......Step: 38/303....... Average Loss for Epoch: 0.02127376338467002\n",
      "Epoch 2......Step: 39/303....... Average Loss for Epoch: 0.021252589992796764\n",
      "Epoch 2......Step: 40/303....... Average Loss for Epoch: 0.021062745759263635\n",
      "Epoch 2......Step: 41/303....... Average Loss for Epoch: 0.020986407177477347\n",
      "Epoch 2......Step: 42/303....... Average Loss for Epoch: 0.02085112833550998\n",
      "Epoch 2......Step: 43/303....... Average Loss for Epoch: 0.02081804579600345\n",
      "Epoch 2......Step: 44/303....... Average Loss for Epoch: 0.020748614079572937\n",
      "Epoch 2......Step: 45/303....... Average Loss for Epoch: 0.020709590944978925\n",
      "Epoch 2......Step: 46/303....... Average Loss for Epoch: 0.020537713056673176\n",
      "Epoch 2......Step: 47/303....... Average Loss for Epoch: 0.02039552927809827\n",
      "Epoch 2......Step: 48/303....... Average Loss for Epoch: 0.02047719022569557\n",
      "Epoch 2......Step: 49/303....... Average Loss for Epoch: 0.02042836646492384\n",
      "Epoch 2......Step: 50/303....... Average Loss for Epoch: 0.020582505092024802\n",
      "Epoch 2......Step: 51/303....... Average Loss for Epoch: 0.02057869104193706\n",
      "Epoch 2......Step: 52/303....... Average Loss for Epoch: 0.020465379455485024\n",
      "Epoch 2......Step: 53/303....... Average Loss for Epoch: 0.02067914135964974\n",
      "Epoch 2......Step: 54/303....... Average Loss for Epoch: 0.021039143635856884\n",
      "Epoch 2......Step: 55/303....... Average Loss for Epoch: 0.02139961010014469\n",
      "Epoch 2......Step: 56/303....... Average Loss for Epoch: 0.02146378972767187\n",
      "Epoch 2......Step: 57/303....... Average Loss for Epoch: 0.021419686330645754\n",
      "Epoch 2......Step: 58/303....... Average Loss for Epoch: 0.02164151193574071\n",
      "Epoch 2......Step: 59/303....... Average Loss for Epoch: 0.021539294823877893\n",
      "Epoch 2......Step: 60/303....... Average Loss for Epoch: 0.02178643080405891\n",
      "Epoch 2......Step: 61/303....... Average Loss for Epoch: 0.02186780134368627\n",
      "Epoch 2......Step: 62/303....... Average Loss for Epoch: 0.021784817244136525\n",
      "Epoch 2......Step: 63/303....... Average Loss for Epoch: 0.02173375121007363\n",
      "Epoch 2......Step: 64/303....... Average Loss for Epoch: 0.02182675631775055\n",
      "Epoch 2......Step: 65/303....... Average Loss for Epoch: 0.02180998083204031\n",
      "Epoch 2......Step: 66/303....... Average Loss for Epoch: 0.02187447657695774\n",
      "Epoch 2......Step: 67/303....... Average Loss for Epoch: 0.02181202485990613\n",
      "Epoch 2......Step: 68/303....... Average Loss for Epoch: 0.0216858092805042\n",
      "Epoch 2......Step: 69/303....... Average Loss for Epoch: 0.02163102958297384\n",
      "Epoch 2......Step: 70/303....... Average Loss for Epoch: 0.02157387182648693\n",
      "Epoch 2......Step: 71/303....... Average Loss for Epoch: 0.02162089949132691\n",
      "Epoch 2......Step: 72/303....... Average Loss for Epoch: 0.021639738775168855\n",
      "Epoch 2......Step: 73/303....... Average Loss for Epoch: 0.02159835592116395\n",
      "Epoch 2......Step: 74/303....... Average Loss for Epoch: 0.021761750047271315\n",
      "Epoch 2......Step: 75/303....... Average Loss for Epoch: 0.021692739153901738\n",
      "Epoch 2......Step: 76/303....... Average Loss for Epoch: 0.0216516358042626\n",
      "Epoch 2......Step: 77/303....... Average Loss for Epoch: 0.02158297755598248\n",
      "Epoch 2......Step: 78/303....... Average Loss for Epoch: 0.021516107715284213\n",
      "Epoch 2......Step: 79/303....... Average Loss for Epoch: 0.021521600694218768\n",
      "Epoch 2......Step: 80/303....... Average Loss for Epoch: 0.021592068509198724\n",
      "Epoch 2......Step: 81/303....... Average Loss for Epoch: 0.0216447473216204\n",
      "Epoch 2......Step: 82/303....... Average Loss for Epoch: 0.021606800850571657\n",
      "Epoch 2......Step: 83/303....... Average Loss for Epoch: 0.02154638945876834\n",
      "Epoch 2......Step: 84/303....... Average Loss for Epoch: 0.021563365301560788\n",
      "Epoch 2......Step: 85/303....... Average Loss for Epoch: 0.021484965009286122\n",
      "Epoch 2......Step: 86/303....... Average Loss for Epoch: 0.021466822374265556\n",
      "Epoch 2......Step: 87/303....... Average Loss for Epoch: 0.021376867125602973\n",
      "Epoch 2......Step: 88/303....... Average Loss for Epoch: 0.021391399289396675\n",
      "Epoch 2......Step: 89/303....... Average Loss for Epoch: 0.021495195105671883\n",
      "Epoch 2......Step: 90/303....... Average Loss for Epoch: 0.021464453844560517\n",
      "Epoch 2......Step: 91/303....... Average Loss for Epoch: 0.021431146291913568\n",
      "Epoch 2......Step: 92/303....... Average Loss for Epoch: 0.021514096155600702\n",
      "Epoch 2......Step: 93/303....... Average Loss for Epoch: 0.021578554724974018\n",
      "Epoch 2......Step: 94/303....... Average Loss for Epoch: 0.02154226058182564\n",
      "Epoch 2......Step: 95/303....... Average Loss for Epoch: 0.021539878845214844\n",
      "Epoch 2......Step: 96/303....... Average Loss for Epoch: 0.02147762409488981\n",
      "Epoch 2......Step: 97/303....... Average Loss for Epoch: 0.021490513990373956\n",
      "Epoch 2......Step: 98/303....... Average Loss for Epoch: 0.021492006713334396\n",
      "Epoch 2......Step: 99/303....... Average Loss for Epoch: 0.021517005419791346\n",
      "Epoch 2......Step: 100/303....... Average Loss for Epoch: 0.02149457635357976\n",
      "Epoch 2......Step: 101/303....... Average Loss for Epoch: 0.021436367331459972\n",
      "Epoch 2......Step: 102/303....... Average Loss for Epoch: 0.02149441390864405\n",
      "Epoch 2......Step: 103/303....... Average Loss for Epoch: 0.021452546607956147\n",
      "Epoch 2......Step: 104/303....... Average Loss for Epoch: 0.021578803103273876\n",
      "Epoch 2......Step: 105/303....... Average Loss for Epoch: 0.021521959054682935\n",
      "Epoch 2......Step: 106/303....... Average Loss for Epoch: 0.021483163814991713\n",
      "Epoch 2......Step: 107/303....... Average Loss for Epoch: 0.02156820521629024\n",
      "Epoch 2......Step: 108/303....... Average Loss for Epoch: 0.021508090568844368\n",
      "Epoch 2......Step: 109/303....... Average Loss for Epoch: 0.02150449829193157\n",
      "Epoch 2......Step: 110/303....... Average Loss for Epoch: 0.021483400455591353\n",
      "Epoch 2......Step: 111/303....... Average Loss for Epoch: 0.021633573539286584\n",
      "Epoch 2......Step: 112/303....... Average Loss for Epoch: 0.021606276601752534\n",
      "Epoch 2......Step: 113/303....... Average Loss for Epoch: 0.02156666512147779\n",
      "Epoch 2......Step: 114/303....... Average Loss for Epoch: 0.021536246685539943\n",
      "Epoch 2......Step: 115/303....... Average Loss for Epoch: 0.02147840494694917\n",
      "Epoch 2......Step: 116/303....... Average Loss for Epoch: 0.021463477502352203\n",
      "Epoch 2......Step: 117/303....... Average Loss for Epoch: 0.02146092026980005\n",
      "Epoch 2......Step: 118/303....... Average Loss for Epoch: 0.021404538792952642\n",
      "Epoch 2......Step: 119/303....... Average Loss for Epoch: 0.02139971895190347\n",
      "Epoch 2......Step: 120/303....... Average Loss for Epoch: 0.02135090809315443\n",
      "Epoch 2......Step: 121/303....... Average Loss for Epoch: 0.021443301088307515\n",
      "Epoch 2......Step: 122/303....... Average Loss for Epoch: 0.021427997067326406\n",
      "Epoch 2......Step: 123/303....... Average Loss for Epoch: 0.02139176767531449\n",
      "Epoch 2......Step: 124/303....... Average Loss for Epoch: 0.021401808041358186\n",
      "Epoch 2......Step: 125/303....... Average Loss for Epoch: 0.02136562079191208\n",
      "Epoch 2......Step: 126/303....... Average Loss for Epoch: 0.021365813644868985\n",
      "Epoch 2......Step: 127/303....... Average Loss for Epoch: 0.021318346291311144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2......Step: 128/303....... Average Loss for Epoch: 0.02147937729023397\n",
      "Epoch 2......Step: 129/303....... Average Loss for Epoch: 0.021445490751964176\n",
      "Epoch 2......Step: 130/303....... Average Loss for Epoch: 0.02150952391899549\n",
      "Epoch 2......Step: 131/303....... Average Loss for Epoch: 0.021445271601715617\n",
      "Epoch 2......Step: 132/303....... Average Loss for Epoch: 0.021559762239286847\n",
      "Epoch 2......Step: 133/303....... Average Loss for Epoch: 0.02157650031569533\n",
      "Epoch 2......Step: 134/303....... Average Loss for Epoch: 0.021601825320064577\n",
      "Epoch 2......Step: 135/303....... Average Loss for Epoch: 0.021607598890033032\n",
      "Epoch 2......Step: 136/303....... Average Loss for Epoch: 0.02158946524072877\n",
      "Epoch 2......Step: 137/303....... Average Loss for Epoch: 0.021603317666173415\n",
      "Epoch 2......Step: 138/303....... Average Loss for Epoch: 0.021590263333063627\n",
      "Epoch 2......Step: 139/303....... Average Loss for Epoch: 0.02155662057988292\n",
      "Epoch 2......Step: 140/303....... Average Loss for Epoch: 0.021570744678112014\n",
      "Epoch 2......Step: 141/303....... Average Loss for Epoch: 0.021570395579204916\n",
      "Epoch 2......Step: 142/303....... Average Loss for Epoch: 0.02161569116164891\n",
      "Epoch 2......Step: 143/303....... Average Loss for Epoch: 0.021672531890754517\n",
      "Epoch 2......Step: 144/303....... Average Loss for Epoch: 0.021634921916605283\n",
      "Epoch 2......Step: 145/303....... Average Loss for Epoch: 0.021613375913223317\n",
      "Epoch 2......Step: 146/303....... Average Loss for Epoch: 0.021608443235442656\n",
      "Epoch 2......Step: 147/303....... Average Loss for Epoch: 0.021614336167924665\n",
      "Epoch 2......Step: 148/303....... Average Loss for Epoch: 0.021625947414872213\n",
      "Epoch 2......Step: 149/303....... Average Loss for Epoch: 0.021624296572334414\n",
      "Epoch 2......Step: 150/303....... Average Loss for Epoch: 0.021569028683006763\n",
      "Epoch 2......Step: 151/303....... Average Loss for Epoch: 0.02158330087334115\n",
      "Epoch 2......Step: 152/303....... Average Loss for Epoch: 0.02156299243583099\n",
      "Epoch 2......Step: 153/303....... Average Loss for Epoch: 0.021514695904710713\n",
      "Epoch 2......Step: 154/303....... Average Loss for Epoch: 0.02151037228910567\n",
      "Epoch 2......Step: 155/303....... Average Loss for Epoch: 0.021528418506345442\n",
      "Epoch 2......Step: 156/303....... Average Loss for Epoch: 0.02152402751529828\n",
      "Epoch 2......Step: 157/303....... Average Loss for Epoch: 0.021490069023173325\n",
      "Epoch 2......Step: 158/303....... Average Loss for Epoch: 0.021541985859976538\n",
      "Epoch 2......Step: 159/303....... Average Loss for Epoch: 0.02155859206082686\n",
      "Epoch 2......Step: 160/303....... Average Loss for Epoch: 0.021525344951078294\n",
      "Epoch 2......Step: 161/303....... Average Loss for Epoch: 0.021504201099091434\n",
      "Epoch 2......Step: 162/303....... Average Loss for Epoch: 0.0214891500633072\n",
      "Epoch 2......Step: 163/303....... Average Loss for Epoch: 0.02151871561729835\n",
      "Epoch 2......Step: 164/303....... Average Loss for Epoch: 0.021504990790584464\n",
      "Epoch 2......Step: 165/303....... Average Loss for Epoch: 0.021490579391970778\n",
      "Epoch 2......Step: 166/303....... Average Loss for Epoch: 0.02149946856346116\n",
      "Epoch 2......Step: 167/303....... Average Loss for Epoch: 0.021475475939537237\n",
      "Epoch 2......Step: 168/303....... Average Loss for Epoch: 0.02142609495647429\n",
      "Epoch 2......Step: 169/303....... Average Loss for Epoch: 0.02138636876228293\n",
      "Epoch 2......Step: 170/303....... Average Loss for Epoch: 0.021382771903539405\n",
      "Epoch 2......Step: 171/303....... Average Loss for Epoch: 0.021389207742803278\n",
      "Epoch 2......Step: 172/303....... Average Loss for Epoch: 0.021324322889234092\n",
      "Epoch 2......Step: 173/303....... Average Loss for Epoch: 0.02138998375053523\n",
      "Epoch 2......Step: 174/303....... Average Loss for Epoch: 0.021348403943381434\n",
      "Epoch 2......Step: 175/303....... Average Loss for Epoch: 0.02136362722409623\n",
      "Epoch 2......Step: 176/303....... Average Loss for Epoch: 0.021389294262255797\n",
      "Epoch 2......Step: 177/303....... Average Loss for Epoch: 0.02136772842078054\n",
      "Epoch 2......Step: 178/303....... Average Loss for Epoch: 0.021396555838415796\n",
      "Epoch 2......Step: 179/303....... Average Loss for Epoch: 0.02141768153881727\n",
      "Epoch 2......Step: 180/303....... Average Loss for Epoch: 0.021493923762399288\n",
      "Epoch 2......Step: 181/303....... Average Loss for Epoch: 0.021466423747083428\n",
      "Epoch 2......Step: 182/303....... Average Loss for Epoch: 0.021418068691023757\n",
      "Epoch 2......Step: 183/303....... Average Loss for Epoch: 0.021412228921153506\n",
      "Epoch 2......Step: 184/303....... Average Loss for Epoch: 0.02138250708863463\n",
      "Epoch 2......Step: 185/303....... Average Loss for Epoch: 0.02140756653168717\n",
      "Epoch 2......Step: 186/303....... Average Loss for Epoch: 0.021444986293953593\n",
      "Epoch 2......Step: 187/303....... Average Loss for Epoch: 0.021425509426922083\n",
      "Epoch 2......Step: 188/303....... Average Loss for Epoch: 0.02141716220277421\n",
      "Epoch 2......Step: 189/303....... Average Loss for Epoch: 0.02139909753684329\n",
      "Epoch 2......Step: 190/303....... Average Loss for Epoch: 0.021394976630414786\n",
      "Epoch 2......Step: 191/303....... Average Loss for Epoch: 0.02144123337850833\n",
      "Epoch 2......Step: 192/303....... Average Loss for Epoch: 0.02141044998522072\n",
      "Epoch 2......Step: 193/303....... Average Loss for Epoch: 0.021452835567990425\n",
      "Epoch 2......Step: 194/303....... Average Loss for Epoch: 0.021427633783290375\n",
      "Epoch 2......Step: 195/303....... Average Loss for Epoch: 0.021435756747348187\n",
      "Epoch 2......Step: 196/303....... Average Loss for Epoch: 0.021450937758865102\n",
      "Epoch 2......Step: 197/303....... Average Loss for Epoch: 0.021446141319793794\n",
      "Epoch 2......Step: 198/303....... Average Loss for Epoch: 0.02143517692074782\n",
      "Epoch 2......Step: 199/303....... Average Loss for Epoch: 0.02139932319203663\n",
      "Epoch 2......Step: 200/303....... Average Loss for Epoch: 0.02137992191594094\n",
      "Epoch 2......Step: 201/303....... Average Loss for Epoch: 0.021346353782127746\n",
      "Epoch 2......Step: 202/303....... Average Loss for Epoch: 0.02135799492312835\n",
      "Epoch 2......Step: 203/303....... Average Loss for Epoch: 0.021335188794899457\n",
      "Epoch 2......Step: 204/303....... Average Loss for Epoch: 0.02131557543122885\n",
      "Epoch 2......Step: 205/303....... Average Loss for Epoch: 0.02131639984322757\n",
      "Epoch 2......Step: 206/303....... Average Loss for Epoch: 0.021394666157735203\n",
      "Epoch 2......Step: 207/303....... Average Loss for Epoch: 0.021373624764922737\n",
      "Epoch 2......Step: 208/303....... Average Loss for Epoch: 0.021362436577104606\n",
      "Epoch 2......Step: 209/303....... Average Loss for Epoch: 0.021370623813291485\n",
      "Epoch 2......Step: 210/303....... Average Loss for Epoch: 0.021394629554734344\n",
      "Epoch 2......Step: 211/303....... Average Loss for Epoch: 0.021401019104849105\n",
      "Epoch 2......Step: 212/303....... Average Loss for Epoch: 0.021378489641718706\n",
      "Epoch 2......Step: 213/303....... Average Loss for Epoch: 0.021378379979743643\n",
      "Epoch 2......Step: 214/303....... Average Loss for Epoch: 0.02135273122237386\n",
      "Epoch 2......Step: 215/303....... Average Loss for Epoch: 0.021318628162492154\n",
      "Epoch 2......Step: 216/303....... Average Loss for Epoch: 0.021290620711321633\n",
      "Epoch 2......Step: 217/303....... Average Loss for Epoch: 0.021268217142001826\n",
      "Epoch 2......Step: 218/303....... Average Loss for Epoch: 0.02124527866584719\n",
      "Epoch 2......Step: 219/303....... Average Loss for Epoch: 0.021230667344715497\n",
      "Epoch 2......Step: 220/303....... Average Loss for Epoch: 0.021201494268395685\n",
      "Epoch 2......Step: 221/303....... Average Loss for Epoch: 0.021210825045454018\n",
      "Epoch 2......Step: 222/303....... Average Loss for Epoch: 0.021174288870817102\n",
      "Epoch 2......Step: 223/303....... Average Loss for Epoch: 0.021197729969539184\n",
      "Epoch 2......Step: 224/303....... Average Loss for Epoch: 0.02123275350563095\n",
      "Epoch 2......Step: 225/303....... Average Loss for Epoch: 0.021204672219852606\n",
      "Epoch 2......Step: 226/303....... Average Loss for Epoch: 0.021195993391978266\n",
      "Epoch 2......Step: 227/303....... Average Loss for Epoch: 0.02117641062972173\n",
      "Epoch 2......Step: 228/303....... Average Loss for Epoch: 0.02115604572668018\n",
      "Epoch 2......Step: 229/303....... Average Loss for Epoch: 0.021127073350464154\n",
      "Epoch 2......Step: 230/303....... Average Loss for Epoch: 0.02111653702981446\n",
      "Epoch 2......Step: 231/303....... Average Loss for Epoch: 0.021119869386704713\n",
      "Epoch 2......Step: 232/303....... Average Loss for Epoch: 0.021138794531499774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2......Step: 233/303....... Average Loss for Epoch: 0.021147149702925794\n",
      "Epoch 2......Step: 234/303....... Average Loss for Epoch: 0.021125633488449022\n",
      "Epoch 2......Step: 235/303....... Average Loss for Epoch: 0.02108802876256882\n",
      "Epoch 2......Step: 236/303....... Average Loss for Epoch: 0.021096108173477953\n",
      "Epoch 2......Step: 237/303....... Average Loss for Epoch: 0.021078574129297763\n",
      "Epoch 2......Step: 238/303....... Average Loss for Epoch: 0.021102553178720614\n",
      "Epoch 2......Step: 239/303....... Average Loss for Epoch: 0.02107318809107881\n",
      "Epoch 2......Step: 240/303....... Average Loss for Epoch: 0.021069937862921505\n",
      "Epoch 2......Step: 241/303....... Average Loss for Epoch: 0.02106154629288249\n",
      "Epoch 2......Step: 242/303....... Average Loss for Epoch: 0.021044846989737808\n",
      "Epoch 2......Step: 243/303....... Average Loss for Epoch: 0.021057456122412357\n",
      "Epoch 2......Step: 244/303....... Average Loss for Epoch: 0.02103814537270514\n",
      "Epoch 2......Step: 245/303....... Average Loss for Epoch: 0.02107018231205186\n",
      "Epoch 2......Step: 246/303....... Average Loss for Epoch: 0.021096367150454258\n",
      "Epoch 2......Step: 247/303....... Average Loss for Epoch: 0.021087761029510606\n",
      "Epoch 2......Step: 248/303....... Average Loss for Epoch: 0.0210706812156845\n",
      "Epoch 2......Step: 249/303....... Average Loss for Epoch: 0.02105429863938725\n",
      "Epoch 2......Step: 250/303....... Average Loss for Epoch: 0.02104762025550008\n",
      "Epoch 2......Step: 251/303....... Average Loss for Epoch: 0.02101899589972192\n",
      "Epoch 2......Step: 252/303....... Average Loss for Epoch: 0.021017729060813076\n",
      "Epoch 2......Step: 253/303....... Average Loss for Epoch: 0.02099975526862936\n",
      "Epoch 2......Step: 254/303....... Average Loss for Epoch: 0.020960000215288925\n",
      "Epoch 2......Step: 255/303....... Average Loss for Epoch: 0.020966230818600048\n",
      "Epoch 2......Step: 256/303....... Average Loss for Epoch: 0.020976491268811515\n",
      "Epoch 2......Step: 257/303....... Average Loss for Epoch: 0.020962016508418068\n",
      "Epoch 2......Step: 258/303....... Average Loss for Epoch: 0.020947415204790903\n",
      "Epoch 2......Step: 259/303....... Average Loss for Epoch: 0.02095896564064454\n",
      "Epoch 2......Step: 260/303....... Average Loss for Epoch: 0.020943287233463847\n",
      "Epoch 2......Step: 261/303....... Average Loss for Epoch: 0.020961953873007463\n",
      "Epoch 2......Step: 262/303....... Average Loss for Epoch: 0.020940958692884173\n",
      "Epoch 2......Step: 263/303....... Average Loss for Epoch: 0.020911748042974635\n",
      "Epoch 2......Step: 264/303....... Average Loss for Epoch: 0.020929649253516938\n",
      "Epoch 2......Step: 265/303....... Average Loss for Epoch: 0.02091241775537437\n",
      "Epoch 2......Step: 266/303....... Average Loss for Epoch: 0.02089713768039207\n",
      "Epoch 2......Step: 267/303....... Average Loss for Epoch: 0.02087855309750257\n",
      "Epoch 2......Step: 268/303....... Average Loss for Epoch: 0.020877130600665487\n",
      "Epoch 2......Step: 269/303....... Average Loss for Epoch: 0.02086523391971136\n",
      "Epoch 2......Step: 270/303....... Average Loss for Epoch: 0.020848814018622593\n",
      "Epoch 2......Step: 271/303....... Average Loss for Epoch: 0.02085327755701058\n",
      "Epoch 2......Step: 272/303....... Average Loss for Epoch: 0.020836010828669017\n",
      "Epoch 2......Step: 273/303....... Average Loss for Epoch: 0.020820670614476167\n",
      "Epoch 2......Step: 274/303....... Average Loss for Epoch: 0.020860259774664457\n",
      "Epoch 2......Step: 275/303....... Average Loss for Epoch: 0.020824169120327994\n",
      "Epoch 2......Step: 276/303....... Average Loss for Epoch: 0.020817371615060212\n",
      "Epoch 2......Step: 277/303....... Average Loss for Epoch: 0.02080098408129779\n",
      "Epoch 2......Step: 278/303....... Average Loss for Epoch: 0.02082064175281486\n",
      "Epoch 2......Step: 279/303....... Average Loss for Epoch: 0.02083285671982607\n",
      "Epoch 2......Step: 280/303....... Average Loss for Epoch: 0.02082114052360079\n",
      "Epoch 2......Step: 281/303....... Average Loss for Epoch: 0.020838185253278005\n",
      "Epoch 2......Step: 282/303....... Average Loss for Epoch: 0.020833450537967556\n",
      "Epoch 2......Step: 283/303....... Average Loss for Epoch: 0.02086294086011581\n",
      "Epoch 2......Step: 284/303....... Average Loss for Epoch: 0.020866852683264395\n",
      "Epoch 2......Step: 285/303....... Average Loss for Epoch: 0.020851087841417707\n",
      "Epoch 2......Step: 286/303....... Average Loss for Epoch: 0.020843289763230334\n",
      "Epoch 2......Step: 287/303....... Average Loss for Epoch: 0.02085118952048261\n",
      "Epoch 2......Step: 288/303....... Average Loss for Epoch: 0.02088327861403943\n",
      "Epoch 2......Step: 289/303....... Average Loss for Epoch: 0.02086953203609555\n",
      "Epoch 2......Step: 290/303....... Average Loss for Epoch: 0.020885821184593027\n",
      "Epoch 2......Step: 291/303....... Average Loss for Epoch: 0.020862551537243156\n",
      "Epoch 2......Step: 292/303....... Average Loss for Epoch: 0.020860429167466825\n",
      "Epoch 2......Step: 293/303....... Average Loss for Epoch: 0.020849842657637067\n",
      "Epoch 2......Step: 294/303....... Average Loss for Epoch: 0.02085383439461897\n",
      "Epoch 2......Step: 295/303....... Average Loss for Epoch: 0.020863072375246025\n",
      "Epoch 2......Step: 296/303....... Average Loss for Epoch: 0.020892319511748046\n",
      "Epoch 2......Step: 297/303....... Average Loss for Epoch: 0.02090655011673668\n",
      "Epoch 2......Step: 298/303....... Average Loss for Epoch: 0.020925456911150442\n",
      "Epoch 2......Step: 299/303....... Average Loss for Epoch: 0.02090951479834358\n",
      "Epoch 2......Step: 300/303....... Average Loss for Epoch: 0.020911165789390604\n",
      "Epoch 2......Step: 301/303....... Average Loss for Epoch: 0.020962066852496904\n",
      "Epoch 2......Step: 302/303....... Average Loss for Epoch: 0.02095187430436544\n",
      "Epoch 2......Step: 303/303....... Average Loss for Epoch: 0.020936771474237687\n",
      "Epoch 2/5 Done, Total Loss: 0.020936771474237687\n",
      "Time Elapsed for Epoch: 6.612668500000001 seconds\n",
      "Epoch 3......Step: 1/303....... Average Loss for Epoch: 0.02182317152619362\n",
      "Epoch 3......Step: 2/303....... Average Loss for Epoch: 0.026183663867413998\n",
      "Epoch 3......Step: 3/303....... Average Loss for Epoch: 0.028026380265752476\n",
      "Epoch 3......Step: 4/303....... Average Loss for Epoch: 0.02595211612060666\n",
      "Epoch 3......Step: 5/303....... Average Loss for Epoch: 0.026134789735078812\n",
      "Epoch 3......Step: 6/303....... Average Loss for Epoch: 0.026389131943384807\n",
      "Epoch 3......Step: 7/303....... Average Loss for Epoch: 0.025802199063556536\n",
      "Epoch 3......Step: 8/303....... Average Loss for Epoch: 0.02449351013638079\n",
      "Epoch 3......Step: 9/303....... Average Loss for Epoch: 0.023820339391628902\n",
      "Epoch 3......Step: 10/303....... Average Loss for Epoch: 0.024136983230710028\n",
      "Epoch 3......Step: 11/303....... Average Loss for Epoch: 0.023876555941321632\n",
      "Epoch 3......Step: 12/303....... Average Loss for Epoch: 0.02300896045441429\n",
      "Epoch 3......Step: 13/303....... Average Loss for Epoch: 0.022514378938537378\n",
      "Epoch 3......Step: 14/303....... Average Loss for Epoch: 0.02193003919507776\n",
      "Epoch 3......Step: 15/303....... Average Loss for Epoch: 0.022249262655774753\n",
      "Epoch 3......Step: 16/303....... Average Loss for Epoch: 0.022026745253242552\n",
      "Epoch 3......Step: 17/303....... Average Loss for Epoch: 0.021659987485584092\n",
      "Epoch 3......Step: 18/303....... Average Loss for Epoch: 0.021727611414260335\n",
      "Epoch 3......Step: 19/303....... Average Loss for Epoch: 0.02136793762053314\n",
      "Epoch 3......Step: 20/303....... Average Loss for Epoch: 0.02156330058351159\n",
      "Epoch 3......Step: 21/303....... Average Loss for Epoch: 0.021231608908800853\n",
      "Epoch 3......Step: 22/303....... Average Loss for Epoch: 0.02118018912998113\n",
      "Epoch 3......Step: 23/303....... Average Loss for Epoch: 0.021128245831831642\n",
      "Epoch 3......Step: 24/303....... Average Loss for Epoch: 0.021141394584750135\n",
      "Epoch 3......Step: 25/303....... Average Loss for Epoch: 0.02095422774553299\n",
      "Epoch 3......Step: 26/303....... Average Loss for Epoch: 0.020701584298736773\n",
      "Epoch 3......Step: 27/303....... Average Loss for Epoch: 0.02049995227544396\n",
      "Epoch 3......Step: 28/303....... Average Loss for Epoch: 0.020315725289817368\n",
      "Epoch 3......Step: 29/303....... Average Loss for Epoch: 0.020289700542544496\n",
      "Epoch 3......Step: 30/303....... Average Loss for Epoch: 0.02006545178592205\n",
      "Epoch 3......Step: 31/303....... Average Loss for Epoch: 0.019987901732806238\n",
      "Epoch 3......Step: 32/303....... Average Loss for Epoch: 0.01977318388526328\n",
      "Epoch 3......Step: 33/303....... Average Loss for Epoch: 0.01966309601045919\n",
      "Epoch 3......Step: 34/303....... Average Loss for Epoch: 0.019654321840361637\n",
      "Epoch 3......Step: 35/303....... Average Loss for Epoch: 0.01938981469720602\n",
      "Epoch 3......Step: 36/303....... Average Loss for Epoch: 0.019410843883330624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3......Step: 37/303....... Average Loss for Epoch: 0.01970745630662989\n",
      "Epoch 3......Step: 38/303....... Average Loss for Epoch: 0.019664534294095478\n",
      "Epoch 3......Step: 39/303....... Average Loss for Epoch: 0.019558946578166425\n",
      "Epoch 3......Step: 40/303....... Average Loss for Epoch: 0.01974508692510426\n",
      "Epoch 3......Step: 41/303....... Average Loss for Epoch: 0.019534018935590255\n",
      "Epoch 3......Step: 42/303....... Average Loss for Epoch: 0.019523679323139646\n",
      "Epoch 3......Step: 43/303....... Average Loss for Epoch: 0.019580509123760602\n",
      "Epoch 3......Step: 44/303....... Average Loss for Epoch: 0.019592491994527252\n",
      "Epoch 3......Step: 45/303....... Average Loss for Epoch: 0.019512532444463836\n",
      "Epoch 3......Step: 46/303....... Average Loss for Epoch: 0.019435223110991974\n",
      "Epoch 3......Step: 47/303....... Average Loss for Epoch: 0.019307093932590585\n",
      "Epoch 3......Step: 48/303....... Average Loss for Epoch: 0.01926536097501715\n",
      "Epoch 3......Step: 49/303....... Average Loss for Epoch: 0.01923025228387239\n",
      "Epoch 3......Step: 50/303....... Average Loss for Epoch: 0.01922141507267952\n",
      "Epoch 3......Step: 51/303....... Average Loss for Epoch: 0.01930654012397224\n",
      "Epoch 3......Step: 52/303....... Average Loss for Epoch: 0.0192578208919328\n",
      "Epoch 3......Step: 53/303....... Average Loss for Epoch: 0.019523382854630362\n",
      "Epoch 3......Step: 54/303....... Average Loss for Epoch: 0.019564083604900924\n",
      "Epoch 3......Step: 55/303....... Average Loss for Epoch: 0.019426230849190193\n",
      "Epoch 3......Step: 56/303....... Average Loss for Epoch: 0.019489766331389546\n",
      "Epoch 3......Step: 57/303....... Average Loss for Epoch: 0.019480280186000624\n",
      "Epoch 3......Step: 58/303....... Average Loss for Epoch: 0.01967261914677661\n",
      "Epoch 3......Step: 59/303....... Average Loss for Epoch: 0.019579834588882277\n",
      "Epoch 3......Step: 60/303....... Average Loss for Epoch: 0.019687659991905092\n",
      "Epoch 3......Step: 61/303....... Average Loss for Epoch: 0.019737923594161134\n",
      "Epoch 3......Step: 62/303....... Average Loss for Epoch: 0.019687426712123618\n",
      "Epoch 3......Step: 63/303....... Average Loss for Epoch: 0.019620268428254695\n",
      "Epoch 3......Step: 64/303....... Average Loss for Epoch: 0.019593720367993228\n",
      "Epoch 3......Step: 65/303....... Average Loss for Epoch: 0.019557161844120576\n",
      "Epoch 3......Step: 66/303....... Average Loss for Epoch: 0.019514837429266085\n",
      "Epoch 3......Step: 67/303....... Average Loss for Epoch: 0.019531575420787976\n",
      "Epoch 3......Step: 68/303....... Average Loss for Epoch: 0.019466313355437973\n",
      "Epoch 3......Step: 69/303....... Average Loss for Epoch: 0.0193758144893724\n",
      "Epoch 3......Step: 70/303....... Average Loss for Epoch: 0.019427443628332444\n",
      "Epoch 3......Step: 71/303....... Average Loss for Epoch: 0.019377522944459612\n",
      "Epoch 3......Step: 72/303....... Average Loss for Epoch: 0.019346522438960772\n",
      "Epoch 3......Step: 73/303....... Average Loss for Epoch: 0.019355860283623818\n",
      "Epoch 3......Step: 74/303....... Average Loss for Epoch: 0.019321455607643805\n",
      "Epoch 3......Step: 75/303....... Average Loss for Epoch: 0.019284233065942923\n",
      "Epoch 3......Step: 76/303....... Average Loss for Epoch: 0.01931261059239899\n",
      "Epoch 3......Step: 77/303....... Average Loss for Epoch: 0.019282988079085753\n",
      "Epoch 3......Step: 78/303....... Average Loss for Epoch: 0.01919555155417094\n",
      "Epoch 3......Step: 79/303....... Average Loss for Epoch: 0.01925212390060666\n",
      "Epoch 3......Step: 80/303....... Average Loss for Epoch: 0.019222447415813804\n",
      "Epoch 3......Step: 81/303....... Average Loss for Epoch: 0.019216042172945577\n",
      "Epoch 3......Step: 82/303....... Average Loss for Epoch: 0.019126736683907304\n",
      "Epoch 3......Step: 83/303....... Average Loss for Epoch: 0.01918967260070235\n",
      "Epoch 3......Step: 84/303....... Average Loss for Epoch: 0.01920097803563944\n",
      "Epoch 3......Step: 85/303....... Average Loss for Epoch: 0.019265787708846963\n",
      "Epoch 3......Step: 86/303....... Average Loss for Epoch: 0.01926619313700601\n",
      "Epoch 3......Step: 87/303....... Average Loss for Epoch: 0.019254041032801414\n",
      "Epoch 3......Step: 88/303....... Average Loss for Epoch: 0.019443674806759438\n",
      "Epoch 3......Step: 89/303....... Average Loss for Epoch: 0.01944337043325218\n",
      "Epoch 3......Step: 90/303....... Average Loss for Epoch: 0.019492376585387522\n",
      "Epoch 3......Step: 91/303....... Average Loss for Epoch: 0.01953940733161929\n",
      "Epoch 3......Step: 92/303....... Average Loss for Epoch: 0.019506621245375794\n",
      "Epoch 3......Step: 93/303....... Average Loss for Epoch: 0.01948192191901066\n",
      "Epoch 3......Step: 94/303....... Average Loss for Epoch: 0.01945957813927151\n",
      "Epoch 3......Step: 95/303....... Average Loss for Epoch: 0.019447354354748602\n",
      "Epoch 3......Step: 96/303....... Average Loss for Epoch: 0.019411008814737823\n",
      "Epoch 3......Step: 97/303....... Average Loss for Epoch: 0.019380406841405275\n",
      "Epoch 3......Step: 98/303....... Average Loss for Epoch: 0.019417330689196075\n",
      "Epoch 3......Step: 99/303....... Average Loss for Epoch: 0.0195012709238764\n",
      "Epoch 3......Step: 100/303....... Average Loss for Epoch: 0.019465459017083048\n",
      "Epoch 3......Step: 101/303....... Average Loss for Epoch: 0.019439316815742763\n",
      "Epoch 3......Step: 102/303....... Average Loss for Epoch: 0.01938588229720207\n",
      "Epoch 3......Step: 103/303....... Average Loss for Epoch: 0.01938730236866231\n",
      "Epoch 3......Step: 104/303....... Average Loss for Epoch: 0.019428933890034947\n",
      "Epoch 3......Step: 105/303....... Average Loss for Epoch: 0.019417400365429265\n",
      "Epoch 3......Step: 106/303....... Average Loss for Epoch: 0.01944140734080717\n",
      "Epoch 3......Step: 107/303....... Average Loss for Epoch: 0.019405999934631532\n",
      "Epoch 3......Step: 108/303....... Average Loss for Epoch: 0.01936141054008018\n",
      "Epoch 3......Step: 109/303....... Average Loss for Epoch: 0.01932089695484813\n",
      "Epoch 3......Step: 110/303....... Average Loss for Epoch: 0.019382794878699563\n",
      "Epoch 3......Step: 111/303....... Average Loss for Epoch: 0.0193840202179041\n",
      "Epoch 3......Step: 112/303....... Average Loss for Epoch: 0.0193778669594654\n",
      "Epoch 3......Step: 113/303....... Average Loss for Epoch: 0.01935892720269946\n",
      "Epoch 3......Step: 114/303....... Average Loss for Epoch: 0.01931199575202507\n",
      "Epoch 3......Step: 115/303....... Average Loss for Epoch: 0.01932162512903628\n",
      "Epoch 3......Step: 116/303....... Average Loss for Epoch: 0.01931261997027644\n",
      "Epoch 3......Step: 117/303....... Average Loss for Epoch: 0.01929622934733191\n",
      "Epoch 3......Step: 118/303....... Average Loss for Epoch: 0.019320071311825414\n",
      "Epoch 3......Step: 119/303....... Average Loss for Epoch: 0.01934698537117293\n",
      "Epoch 3......Step: 120/303....... Average Loss for Epoch: 0.019314225188766916\n",
      "Epoch 3......Step: 121/303....... Average Loss for Epoch: 0.019339133777524813\n",
      "Epoch 3......Step: 122/303....... Average Loss for Epoch: 0.01935947202451405\n",
      "Epoch 3......Step: 123/303....... Average Loss for Epoch: 0.019313303019275994\n",
      "Epoch 3......Step: 124/303....... Average Loss for Epoch: 0.019371735600514278\n",
      "Epoch 3......Step: 125/303....... Average Loss for Epoch: 0.01936670722812414\n",
      "Epoch 3......Step: 126/303....... Average Loss for Epoch: 0.01934601654047294\n",
      "Epoch 3......Step: 127/303....... Average Loss for Epoch: 0.019372803863049962\n",
      "Epoch 3......Step: 128/303....... Average Loss for Epoch: 0.019431710410572123\n",
      "Epoch 3......Step: 129/303....... Average Loss for Epoch: 0.019466734419093114\n",
      "Epoch 3......Step: 130/303....... Average Loss for Epoch: 0.019459781587983553\n",
      "Epoch 3......Step: 131/303....... Average Loss for Epoch: 0.019405798787492833\n",
      "Epoch 3......Step: 132/303....... Average Loss for Epoch: 0.019435405194985145\n",
      "Epoch 3......Step: 133/303....... Average Loss for Epoch: 0.019498355728679133\n",
      "Epoch 3......Step: 134/303....... Average Loss for Epoch: 0.019529333379842452\n",
      "Epoch 3......Step: 135/303....... Average Loss for Epoch: 0.019534906979512285\n",
      "Epoch 3......Step: 136/303....... Average Loss for Epoch: 0.019549731315825793\n",
      "Epoch 3......Step: 137/303....... Average Loss for Epoch: 0.019549469647072527\n",
      "Epoch 3......Step: 138/303....... Average Loss for Epoch: 0.019525194608106995\n",
      "Epoch 3......Step: 139/303....... Average Loss for Epoch: 0.019579264462637386\n",
      "Epoch 3......Step: 140/303....... Average Loss for Epoch: 0.019552627578377724\n",
      "Epoch 3......Step: 141/303....... Average Loss for Epoch: 0.019639838539750862\n",
      "Epoch 3......Step: 142/303....... Average Loss for Epoch: 0.019623437306096017\n",
      "Epoch 3......Step: 143/303....... Average Loss for Epoch: 0.01960735631312107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3......Step: 144/303....... Average Loss for Epoch: 0.01960819827703138\n",
      "Epoch 3......Step: 145/303....... Average Loss for Epoch: 0.019561235343330895\n",
      "Epoch 3......Step: 146/303....... Average Loss for Epoch: 0.01954236527751774\n",
      "Epoch 3......Step: 147/303....... Average Loss for Epoch: 0.019559141216786945\n",
      "Epoch 3......Step: 148/303....... Average Loss for Epoch: 0.01957951983864847\n",
      "Epoch 3......Step: 149/303....... Average Loss for Epoch: 0.019577012322343035\n",
      "Epoch 3......Step: 150/303....... Average Loss for Epoch: 0.01960252957418561\n",
      "Epoch 3......Step: 151/303....... Average Loss for Epoch: 0.019611825185846414\n",
      "Epoch 3......Step: 152/303....... Average Loss for Epoch: 0.019589280335869836\n",
      "Epoch 3......Step: 153/303....... Average Loss for Epoch: 0.0196589211951576\n",
      "Epoch 3......Step: 154/303....... Average Loss for Epoch: 0.019689844757947442\n",
      "Epoch 3......Step: 155/303....... Average Loss for Epoch: 0.01965418519271958\n",
      "Epoch 3......Step: 156/303....... Average Loss for Epoch: 0.019622975202181783\n",
      "Epoch 3......Step: 157/303....... Average Loss for Epoch: 0.01962289032022095\n",
      "Epoch 3......Step: 158/303....... Average Loss for Epoch: 0.019598052736749\n",
      "Epoch 3......Step: 159/303....... Average Loss for Epoch: 0.019567939640262967\n",
      "Epoch 3......Step: 160/303....... Average Loss for Epoch: 0.019517005904344843\n",
      "Epoch 3......Step: 161/303....... Average Loss for Epoch: 0.01949092628931777\n",
      "Epoch 3......Step: 162/303....... Average Loss for Epoch: 0.01948726417519796\n",
      "Epoch 3......Step: 163/303....... Average Loss for Epoch: 0.019454958774377962\n",
      "Epoch 3......Step: 164/303....... Average Loss for Epoch: 0.019411638515388092\n",
      "Epoch 3......Step: 165/303....... Average Loss for Epoch: 0.01942084034283956\n",
      "Epoch 3......Step: 166/303....... Average Loss for Epoch: 0.01945638726184885\n",
      "Epoch 3......Step: 167/303....... Average Loss for Epoch: 0.01944037748801851\n",
      "Epoch 3......Step: 168/303....... Average Loss for Epoch: 0.019457038691533462\n",
      "Epoch 3......Step: 169/303....... Average Loss for Epoch: 0.019454680317076\n",
      "Epoch 3......Step: 170/303....... Average Loss for Epoch: 0.019425125420093538\n",
      "Epoch 3......Step: 171/303....... Average Loss for Epoch: 0.01940839542675088\n",
      "Epoch 3......Step: 172/303....... Average Loss for Epoch: 0.019396758465053038\n",
      "Epoch 3......Step: 173/303....... Average Loss for Epoch: 0.01944124713712345\n",
      "Epoch 3......Step: 174/303....... Average Loss for Epoch: 0.019482807138527947\n",
      "Epoch 3......Step: 175/303....... Average Loss for Epoch: 0.01945071210286447\n",
      "Epoch 3......Step: 176/303....... Average Loss for Epoch: 0.019429012704428962\n",
      "Epoch 3......Step: 177/303....... Average Loss for Epoch: 0.01940939684351117\n",
      "Epoch 3......Step: 178/303....... Average Loss for Epoch: 0.01940649784996771\n",
      "Epoch 3......Step: 179/303....... Average Loss for Epoch: 0.019374497869577487\n",
      "Epoch 3......Step: 180/303....... Average Loss for Epoch: 0.019467323325160477\n",
      "Epoch 3......Step: 181/303....... Average Loss for Epoch: 0.019524422294115492\n",
      "Epoch 3......Step: 182/303....... Average Loss for Epoch: 0.01952939481566568\n",
      "Epoch 3......Step: 183/303....... Average Loss for Epoch: 0.019526093210923214\n",
      "Epoch 3......Step: 184/303....... Average Loss for Epoch: 0.019505387010133785\n",
      "Epoch 3......Step: 185/303....... Average Loss for Epoch: 0.01947268428834709\n",
      "Epoch 3......Step: 186/303....... Average Loss for Epoch: 0.019452445699723177\n",
      "Epoch 3......Step: 187/303....... Average Loss for Epoch: 0.01941146833553193\n",
      "Epoch 3......Step: 188/303....... Average Loss for Epoch: 0.01938512087601455\n",
      "Epoch 3......Step: 189/303....... Average Loss for Epoch: 0.0194212121139995\n",
      "Epoch 3......Step: 190/303....... Average Loss for Epoch: 0.01944641925296501\n",
      "Epoch 3......Step: 191/303....... Average Loss for Epoch: 0.01946030433791938\n",
      "Epoch 3......Step: 192/303....... Average Loss for Epoch: 0.019432686734944582\n",
      "Epoch 3......Step: 193/303....... Average Loss for Epoch: 0.019425191902048847\n",
      "Epoch 3......Step: 194/303....... Average Loss for Epoch: 0.019411707949853436\n",
      "Epoch 3......Step: 195/303....... Average Loss for Epoch: 0.019370909536687228\n",
      "Epoch 3......Step: 196/303....... Average Loss for Epoch: 0.01934325500695529\n",
      "Epoch 3......Step: 197/303....... Average Loss for Epoch: 0.01932441385056313\n",
      "Epoch 3......Step: 198/303....... Average Loss for Epoch: 0.01930570479651744\n",
      "Epoch 3......Step: 199/303....... Average Loss for Epoch: 0.019359318862421128\n",
      "Epoch 3......Step: 200/303....... Average Loss for Epoch: 0.01932116816751659\n",
      "Epoch 3......Step: 201/303....... Average Loss for Epoch: 0.01931579880861204\n",
      "Epoch 3......Step: 202/303....... Average Loss for Epoch: 0.019386079097812127\n",
      "Epoch 3......Step: 203/303....... Average Loss for Epoch: 0.019346789479842915\n",
      "Epoch 3......Step: 204/303....... Average Loss for Epoch: 0.019341212615151617\n",
      "Epoch 3......Step: 205/303....... Average Loss for Epoch: 0.019359875206903715\n",
      "Epoch 3......Step: 206/303....... Average Loss for Epoch: 0.01934862990402481\n",
      "Epoch 3......Step: 207/303....... Average Loss for Epoch: 0.01939364250947312\n",
      "Epoch 3......Step: 208/303....... Average Loss for Epoch: 0.019405280347340383\n",
      "Epoch 3......Step: 209/303....... Average Loss for Epoch: 0.019408669108836846\n",
      "Epoch 3......Step: 210/303....... Average Loss for Epoch: 0.019406148099473546\n",
      "Epoch 3......Step: 211/303....... Average Loss for Epoch: 0.019431523307804812\n",
      "Epoch 3......Step: 212/303....... Average Loss for Epoch: 0.01943605833353018\n",
      "Epoch 3......Step: 213/303....... Average Loss for Epoch: 0.019419797613097468\n",
      "Epoch 3......Step: 214/303....... Average Loss for Epoch: 0.019452004197323434\n",
      "Epoch 3......Step: 215/303....... Average Loss for Epoch: 0.019437515960876332\n",
      "Epoch 3......Step: 216/303....... Average Loss for Epoch: 0.01941957193668242\n",
      "Epoch 3......Step: 217/303....... Average Loss for Epoch: 0.019404864865719997\n",
      "Epoch 3......Step: 218/303....... Average Loss for Epoch: 0.01942646284715845\n",
      "Epoch 3......Step: 219/303....... Average Loss for Epoch: 0.019442310267591585\n",
      "Epoch 3......Step: 220/303....... Average Loss for Epoch: 0.0194699720479548\n",
      "Epoch 3......Step: 221/303....... Average Loss for Epoch: 0.019470815306605257\n",
      "Epoch 3......Step: 222/303....... Average Loss for Epoch: 0.01947441950507529\n",
      "Epoch 3......Step: 223/303....... Average Loss for Epoch: 0.019511203294110404\n",
      "Epoch 3......Step: 224/303....... Average Loss for Epoch: 0.01948958299804612\n",
      "Epoch 3......Step: 225/303....... Average Loss for Epoch: 0.019483991484675143\n",
      "Epoch 3......Step: 226/303....... Average Loss for Epoch: 0.019459195517465076\n",
      "Epoch 3......Step: 227/303....... Average Loss for Epoch: 0.019518646292618194\n",
      "Epoch 3......Step: 228/303....... Average Loss for Epoch: 0.01951623871399645\n",
      "Epoch 3......Step: 229/303....... Average Loss for Epoch: 0.019501697182590264\n",
      "Epoch 3......Step: 230/303....... Average Loss for Epoch: 0.019516480948937976\n",
      "Epoch 3......Step: 231/303....... Average Loss for Epoch: 0.019477993729665424\n",
      "Epoch 3......Step: 232/303....... Average Loss for Epoch: 0.019442677196789663\n",
      "Epoch 3......Step: 233/303....... Average Loss for Epoch: 0.01941994761912224\n",
      "Epoch 3......Step: 234/303....... Average Loss for Epoch: 0.019400028826302696\n",
      "Epoch 3......Step: 235/303....... Average Loss for Epoch: 0.019409384431832648\n",
      "Epoch 3......Step: 236/303....... Average Loss for Epoch: 0.01940405287906149\n",
      "Epoch 3......Step: 237/303....... Average Loss for Epoch: 0.019425133734407053\n",
      "Epoch 3......Step: 238/303....... Average Loss for Epoch: 0.019403832240671923\n",
      "Epoch 3......Step: 239/303....... Average Loss for Epoch: 0.019421357735523868\n",
      "Epoch 3......Step: 240/303....... Average Loss for Epoch: 0.019396523615190137\n",
      "Epoch 3......Step: 241/303....... Average Loss for Epoch: 0.019383343218736877\n",
      "Epoch 3......Step: 242/303....... Average Loss for Epoch: 0.019375502104940247\n",
      "Epoch 3......Step: 243/303....... Average Loss for Epoch: 0.019416277404154032\n",
      "Epoch 3......Step: 244/303....... Average Loss for Epoch: 0.019464799153358966\n",
      "Epoch 3......Step: 245/303....... Average Loss for Epoch: 0.01946801435658518\n",
      "Epoch 3......Step: 246/303....... Average Loss for Epoch: 0.01949067071186212\n",
      "Epoch 3......Step: 247/303....... Average Loss for Epoch: 0.01948213038460808\n",
      "Epoch 3......Step: 248/303....... Average Loss for Epoch: 0.01950191956148633\n",
      "Epoch 3......Step: 249/303....... Average Loss for Epoch: 0.01951957224348343\n",
      "Epoch 3......Step: 250/303....... Average Loss for Epoch: 0.0195039640404284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3......Step: 251/303....... Average Loss for Epoch: 0.01949152007951323\n",
      "Epoch 3......Step: 252/303....... Average Loss for Epoch: 0.019510549275086275\n",
      "Epoch 3......Step: 253/303....... Average Loss for Epoch: 0.01951315151269846\n",
      "Epoch 3......Step: 254/303....... Average Loss for Epoch: 0.0194887740383526\n",
      "Epoch 3......Step: 255/303....... Average Loss for Epoch: 0.019461062229147143\n",
      "Epoch 3......Step: 256/303....... Average Loss for Epoch: 0.019441874224867206\n",
      "Epoch 3......Step: 257/303....... Average Loss for Epoch: 0.019424691506396936\n",
      "Epoch 3......Step: 258/303....... Average Loss for Epoch: 0.019398317275236745\n",
      "Epoch 3......Step: 259/303....... Average Loss for Epoch: 0.019390455565800078\n",
      "Epoch 3......Step: 260/303....... Average Loss for Epoch: 0.0193748384141005\n",
      "Epoch 3......Step: 261/303....... Average Loss for Epoch: 0.01936052673250094\n",
      "Epoch 3......Step: 262/303....... Average Loss for Epoch: 0.019369289293202734\n",
      "Epoch 3......Step: 263/303....... Average Loss for Epoch: 0.01934240975845676\n",
      "Epoch 3......Step: 264/303....... Average Loss for Epoch: 0.019330913772467862\n",
      "Epoch 3......Step: 265/303....... Average Loss for Epoch: 0.019314592844753894\n",
      "Epoch 3......Step: 266/303....... Average Loss for Epoch: 0.019354441378237608\n",
      "Epoch 3......Step: 267/303....... Average Loss for Epoch: 0.019331775916119415\n",
      "Epoch 3......Step: 268/303....... Average Loss for Epoch: 0.019335806310705063\n",
      "Epoch 3......Step: 269/303....... Average Loss for Epoch: 0.019323559481387465\n",
      "Epoch 3......Step: 270/303....... Average Loss for Epoch: 0.01929288470979642\n",
      "Epoch 3......Step: 271/303....... Average Loss for Epoch: 0.01934378206853607\n",
      "Epoch 3......Step: 272/303....... Average Loss for Epoch: 0.01939823688350289\n",
      "Epoch 3......Step: 273/303....... Average Loss for Epoch: 0.019399034101584237\n",
      "Epoch 3......Step: 274/303....... Average Loss for Epoch: 0.019386166716878213\n",
      "Epoch 3......Step: 275/303....... Average Loss for Epoch: 0.01939424004405737\n",
      "Epoch 3......Step: 276/303....... Average Loss for Epoch: 0.019376400500481974\n",
      "Epoch 3......Step: 277/303....... Average Loss for Epoch: 0.01937558005701764\n",
      "Epoch 3......Step: 278/303....... Average Loss for Epoch: 0.019381117926602312\n",
      "Epoch 3......Step: 279/303....... Average Loss for Epoch: 0.019364763478544877\n",
      "Epoch 3......Step: 280/303....... Average Loss for Epoch: 0.019418205241007465\n",
      "Epoch 3......Step: 281/303....... Average Loss for Epoch: 0.01939719867995307\n",
      "Epoch 3......Step: 282/303....... Average Loss for Epoch: 0.01937915773811281\n",
      "Epoch 3......Step: 283/303....... Average Loss for Epoch: 0.01938764630510824\n",
      "Epoch 3......Step: 284/303....... Average Loss for Epoch: 0.019362810294104502\n",
      "Epoch 3......Step: 285/303....... Average Loss for Epoch: 0.019361550423006215\n",
      "Epoch 3......Step: 286/303....... Average Loss for Epoch: 0.019355768021977655\n",
      "Epoch 3......Step: 287/303....... Average Loss for Epoch: 0.01932598969766072\n",
      "Epoch 3......Step: 288/303....... Average Loss for Epoch: 0.0193340855378968\n",
      "Epoch 3......Step: 289/303....... Average Loss for Epoch: 0.019319986634497823\n",
      "Epoch 3......Step: 290/303....... Average Loss for Epoch: 0.019335600772294504\n",
      "Epoch 3......Step: 291/303....... Average Loss for Epoch: 0.019313854850265374\n",
      "Epoch 3......Step: 292/303....... Average Loss for Epoch: 0.01931903430592422\n",
      "Epoch 3......Step: 293/303....... Average Loss for Epoch: 0.019302314102853116\n",
      "Epoch 3......Step: 294/303....... Average Loss for Epoch: 0.01927788013300928\n",
      "Epoch 3......Step: 295/303....... Average Loss for Epoch: 0.019271400798175294\n",
      "Epoch 3......Step: 296/303....... Average Loss for Epoch: 0.01925079747908623\n",
      "Epoch 3......Step: 297/303....... Average Loss for Epoch: 0.01923123811079998\n",
      "Epoch 3......Step: 298/303....... Average Loss for Epoch: 0.01922161057716448\n",
      "Epoch 3......Step: 299/303....... Average Loss for Epoch: 0.01924232681932856\n",
      "Epoch 3......Step: 300/303....... Average Loss for Epoch: 0.01922795734057824\n",
      "Epoch 3......Step: 301/303....... Average Loss for Epoch: 0.01921036047396866\n",
      "Epoch 3......Step: 302/303....... Average Loss for Epoch: 0.019253968378368593\n",
      "Epoch 3......Step: 303/303....... Average Loss for Epoch: 0.019268109921132377\n",
      "Epoch 3/5 Done, Total Loss: 0.019268109921132377\n",
      "Time Elapsed for Epoch: 9.718312000000001 seconds\n",
      "Epoch 4......Step: 1/303....... Average Loss for Epoch: 0.02779277227818966\n",
      "Epoch 4......Step: 2/303....... Average Loss for Epoch: 0.02239417005330324\n",
      "Epoch 4......Step: 3/303....... Average Loss for Epoch: 0.02294112058977286\n",
      "Epoch 4......Step: 4/303....... Average Loss for Epoch: 0.02105985232628882\n",
      "Epoch 4......Step: 5/303....... Average Loss for Epoch: 0.020917057804763318\n",
      "Epoch 4......Step: 6/303....... Average Loss for Epoch: 0.021147299092262983\n",
      "Epoch 4......Step: 7/303....... Average Loss for Epoch: 0.020615669632596628\n",
      "Epoch 4......Step: 8/303....... Average Loss for Epoch: 0.02014978660736233\n",
      "Epoch 4......Step: 9/303....... Average Loss for Epoch: 0.0209889638548096\n",
      "Epoch 4......Step: 10/303....... Average Loss for Epoch: 0.02137309229001403\n",
      "Epoch 4......Step: 11/303....... Average Loss for Epoch: 0.021434844302182846\n",
      "Epoch 4......Step: 12/303....... Average Loss for Epoch: 0.020996032205099862\n",
      "Epoch 4......Step: 13/303....... Average Loss for Epoch: 0.020617857073935177\n",
      "Epoch 4......Step: 14/303....... Average Loss for Epoch: 0.020280504133552313\n",
      "Epoch 4......Step: 15/303....... Average Loss for Epoch: 0.019971186357239882\n",
      "Epoch 4......Step: 16/303....... Average Loss for Epoch: 0.019721138931345195\n",
      "Epoch 4......Step: 17/303....... Average Loss for Epoch: 0.019420236896942642\n",
      "Epoch 4......Step: 18/303....... Average Loss for Epoch: 0.019259510044422414\n",
      "Epoch 4......Step: 19/303....... Average Loss for Epoch: 0.019185546687559077\n",
      "Epoch 4......Step: 20/303....... Average Loss for Epoch: 0.019228674937039614\n",
      "Epoch 4......Step: 21/303....... Average Loss for Epoch: 0.018982490879439172\n",
      "Epoch 4......Step: 22/303....... Average Loss for Epoch: 0.01913185375319286\n",
      "Epoch 4......Step: 23/303....... Average Loss for Epoch: 0.018878503743073215\n",
      "Epoch 4......Step: 24/303....... Average Loss for Epoch: 0.018940073903650045\n",
      "Epoch 4......Step: 25/303....... Average Loss for Epoch: 0.018690582066774368\n",
      "Epoch 4......Step: 26/303....... Average Loss for Epoch: 0.018522997888234947\n",
      "Epoch 4......Step: 27/303....... Average Loss for Epoch: 0.01828664517099107\n",
      "Epoch 4......Step: 28/303....... Average Loss for Epoch: 0.018276475070576583\n",
      "Epoch 4......Step: 29/303....... Average Loss for Epoch: 0.018056926028481846\n",
      "Epoch 4......Step: 30/303....... Average Loss for Epoch: 0.01856116106112798\n",
      "Epoch 4......Step: 31/303....... Average Loss for Epoch: 0.01858713207465987\n",
      "Epoch 4......Step: 32/303....... Average Loss for Epoch: 0.01847590086981654\n",
      "Epoch 4......Step: 33/303....... Average Loss for Epoch: 0.01840761811895804\n",
      "Epoch 4......Step: 34/303....... Average Loss for Epoch: 0.01840961625909104\n",
      "Epoch 4......Step: 35/303....... Average Loss for Epoch: 0.01840571000107697\n",
      "Epoch 4......Step: 36/303....... Average Loss for Epoch: 0.01856171050005489\n",
      "Epoch 4......Step: 37/303....... Average Loss for Epoch: 0.01845638363345249\n",
      "Epoch 4......Step: 38/303....... Average Loss for Epoch: 0.018462407843847024\n",
      "Epoch 4......Step: 39/303....... Average Loss for Epoch: 0.01842847700493458\n",
      "Epoch 4......Step: 40/303....... Average Loss for Epoch: 0.018274890491738915\n",
      "Epoch 4......Step: 41/303....... Average Loss for Epoch: 0.018226768367174195\n",
      "Epoch 4......Step: 42/303....... Average Loss for Epoch: 0.01839190153848557\n",
      "Epoch 4......Step: 43/303....... Average Loss for Epoch: 0.018391360767012418\n",
      "Epoch 4......Step: 44/303....... Average Loss for Epoch: 0.018264950083738022\n",
      "Epoch 4......Step: 45/303....... Average Loss for Epoch: 0.018398854550388123\n",
      "Epoch 4......Step: 46/303....... Average Loss for Epoch: 0.018304598355746788\n",
      "Epoch 4......Step: 47/303....... Average Loss for Epoch: 0.018348840005854343\n",
      "Epoch 4......Step: 48/303....... Average Loss for Epoch: 0.01837604686928292\n",
      "Epoch 4......Step: 49/303....... Average Loss for Epoch: 0.01829561281341071\n",
      "Epoch 4......Step: 50/303....... Average Loss for Epoch: 0.018382567819207905\n",
      "Epoch 4......Step: 51/303....... Average Loss for Epoch: 0.018299970100177268\n",
      "Epoch 4......Step: 52/303....... Average Loss for Epoch: 0.018337845175455395\n",
      "Epoch 4......Step: 53/303....... Average Loss for Epoch: 0.018292484541406046\n",
      "Epoch 4......Step: 54/303....... Average Loss for Epoch: 0.01822166748482872\n",
      "Epoch 4......Step: 55/303....... Average Loss for Epoch: 0.01810437374832955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4......Step: 56/303....... Average Loss for Epoch: 0.018098589016257653\n",
      "Epoch 4......Step: 57/303....... Average Loss for Epoch: 0.017960863890485804\n",
      "Epoch 4......Step: 58/303....... Average Loss for Epoch: 0.017951630104076247\n",
      "Epoch 4......Step: 59/303....... Average Loss for Epoch: 0.017904901466632293\n",
      "Epoch 4......Step: 60/303....... Average Loss for Epoch: 0.018007608689367773\n",
      "Epoch 4......Step: 61/303....... Average Loss for Epoch: 0.018005677880566628\n",
      "Epoch 4......Step: 62/303....... Average Loss for Epoch: 0.018109570197280375\n",
      "Epoch 4......Step: 63/303....... Average Loss for Epoch: 0.018137576590691293\n",
      "Epoch 4......Step: 64/303....... Average Loss for Epoch: 0.01818921821541153\n",
      "Epoch 4......Step: 65/303....... Average Loss for Epoch: 0.0182409412585772\n",
      "Epoch 4......Step: 66/303....... Average Loss for Epoch: 0.01821191840325341\n",
      "Epoch 4......Step: 67/303....... Average Loss for Epoch: 0.01825945490776603\n",
      "Epoch 4......Step: 68/303....... Average Loss for Epoch: 0.018186755135984105\n",
      "Epoch 4......Step: 69/303....... Average Loss for Epoch: 0.01818094733238652\n",
      "Epoch 4......Step: 70/303....... Average Loss for Epoch: 0.018246521867279496\n",
      "Epoch 4......Step: 71/303....... Average Loss for Epoch: 0.018263935058994193\n",
      "Epoch 4......Step: 72/303....... Average Loss for Epoch: 0.018212032452639606\n",
      "Epoch 4......Step: 73/303....... Average Loss for Epoch: 0.018116157680545766\n",
      "Epoch 4......Step: 74/303....... Average Loss for Epoch: 0.018250709953340324\n",
      "Epoch 4......Step: 75/303....... Average Loss for Epoch: 0.01819652861605088\n",
      "Epoch 4......Step: 76/303....... Average Loss for Epoch: 0.01826366766257898\n",
      "Epoch 4......Step: 77/303....... Average Loss for Epoch: 0.018240396671190665\n",
      "Epoch 4......Step: 78/303....... Average Loss for Epoch: 0.018226727329863187\n",
      "Epoch 4......Step: 79/303....... Average Loss for Epoch: 0.01828025006748076\n",
      "Epoch 4......Step: 80/303....... Average Loss for Epoch: 0.018214269122108818\n",
      "Epoch 4......Step: 81/303....... Average Loss for Epoch: 0.018156881639618934\n",
      "Epoch 4......Step: 82/303....... Average Loss for Epoch: 0.018189576590751728\n",
      "Epoch 4......Step: 83/303....... Average Loss for Epoch: 0.018122310413563825\n",
      "Epoch 4......Step: 84/303....... Average Loss for Epoch: 0.01811654452190158\n",
      "Epoch 4......Step: 85/303....... Average Loss for Epoch: 0.018153074044076834\n",
      "Epoch 4......Step: 86/303....... Average Loss for Epoch: 0.018217504858364198\n",
      "Epoch 4......Step: 87/303....... Average Loss for Epoch: 0.018217978024876666\n",
      "Epoch 4......Step: 88/303....... Average Loss for Epoch: 0.018188234937207944\n",
      "Epoch 4......Step: 89/303....... Average Loss for Epoch: 0.018165471712441258\n",
      "Epoch 4......Step: 90/303....... Average Loss for Epoch: 0.018318780501269633\n",
      "Epoch 4......Step: 91/303....... Average Loss for Epoch: 0.01824951508583931\n",
      "Epoch 4......Step: 92/303....... Average Loss for Epoch: 0.01820048148257901\n",
      "Epoch 4......Step: 93/303....... Average Loss for Epoch: 0.018138956669117173\n",
      "Epoch 4......Step: 94/303....... Average Loss for Epoch: 0.01827164242995229\n",
      "Epoch 4......Step: 95/303....... Average Loss for Epoch: 0.01829547641896888\n",
      "Epoch 4......Step: 96/303....... Average Loss for Epoch: 0.01844195946857023\n",
      "Epoch 4......Step: 97/303....... Average Loss for Epoch: 0.01839420763948529\n",
      "Epoch 4......Step: 98/303....... Average Loss for Epoch: 0.01835001165959604\n",
      "Epoch 4......Step: 99/303....... Average Loss for Epoch: 0.018384377208698278\n",
      "Epoch 4......Step: 100/303....... Average Loss for Epoch: 0.018419541819021105\n",
      "Epoch 4......Step: 101/303....... Average Loss for Epoch: 0.018387816957022886\n",
      "Epoch 4......Step: 102/303....... Average Loss for Epoch: 0.01837493687429849\n",
      "Epoch 4......Step: 103/303....... Average Loss for Epoch: 0.01834230780492998\n",
      "Epoch 4......Step: 104/303....... Average Loss for Epoch: 0.018303951427627068\n",
      "Epoch 4......Step: 105/303....... Average Loss for Epoch: 0.01826016079811823\n",
      "Epoch 4......Step: 106/303....... Average Loss for Epoch: 0.018224248416581244\n",
      "Epoch 4......Step: 107/303....... Average Loss for Epoch: 0.018294708350690726\n",
      "Epoch 4......Step: 108/303....... Average Loss for Epoch: 0.018269041216828756\n",
      "Epoch 4......Step: 109/303....... Average Loss for Epoch: 0.018242353394012385\n",
      "Epoch 4......Step: 110/303....... Average Loss for Epoch: 0.018183986707167192\n",
      "Epoch 4......Step: 111/303....... Average Loss for Epoch: 0.018184616775797295\n",
      "Epoch 4......Step: 112/303....... Average Loss for Epoch: 0.018172682146541774\n",
      "Epoch 4......Step: 113/303....... Average Loss for Epoch: 0.018240343812292656\n",
      "Epoch 4......Step: 114/303....... Average Loss for Epoch: 0.018308985063381363\n",
      "Epoch 4......Step: 115/303....... Average Loss for Epoch: 0.018291597716186356\n",
      "Epoch 4......Step: 116/303....... Average Loss for Epoch: 0.018288837231952567\n",
      "Epoch 4......Step: 117/303....... Average Loss for Epoch: 0.01822481305999124\n",
      "Epoch 4......Step: 118/303....... Average Loss for Epoch: 0.018320956362127248\n",
      "Epoch 4......Step: 119/303....... Average Loss for Epoch: 0.018328974289553508\n",
      "Epoch 4......Step: 120/303....... Average Loss for Epoch: 0.018296004839551946\n",
      "Epoch 4......Step: 121/303....... Average Loss for Epoch: 0.018273721195080063\n",
      "Epoch 4......Step: 122/303....... Average Loss for Epoch: 0.01827998344832268\n",
      "Epoch 4......Step: 123/303....... Average Loss for Epoch: 0.018239204877820926\n",
      "Epoch 4......Step: 124/303....... Average Loss for Epoch: 0.01819903938279998\n",
      "Epoch 4......Step: 125/303....... Average Loss for Epoch: 0.018150241300463676\n",
      "Epoch 4......Step: 126/303....... Average Loss for Epoch: 0.01816863508983737\n",
      "Epoch 4......Step: 127/303....... Average Loss for Epoch: 0.018107161336527094\n",
      "Epoch 4......Step: 128/303....... Average Loss for Epoch: 0.01817373392987065\n",
      "Epoch 4......Step: 129/303....... Average Loss for Epoch: 0.018167202252634736\n",
      "Epoch 4......Step: 130/303....... Average Loss for Epoch: 0.01815596967935562\n",
      "Epoch 4......Step: 131/303....... Average Loss for Epoch: 0.018107167174968557\n",
      "Epoch 4......Step: 132/303....... Average Loss for Epoch: 0.018091027194521193\n",
      "Epoch 4......Step: 133/303....... Average Loss for Epoch: 0.01806608878104086\n",
      "Epoch 4......Step: 134/303....... Average Loss for Epoch: 0.01827185847726998\n",
      "Epoch 4......Step: 135/303....... Average Loss for Epoch: 0.018369733590494704\n",
      "Epoch 4......Step: 136/303....... Average Loss for Epoch: 0.01833548053058193\n",
      "Epoch 4......Step: 137/303....... Average Loss for Epoch: 0.01829596602079207\n",
      "Epoch 4......Step: 138/303....... Average Loss for Epoch: 0.018350501652753006\n",
      "Epoch 4......Step: 139/303....... Average Loss for Epoch: 0.018374636796095387\n",
      "Epoch 4......Step: 140/303....... Average Loss for Epoch: 0.018397668176995858\n",
      "Epoch 4......Step: 141/303....... Average Loss for Epoch: 0.01836781817362875\n",
      "Epoch 4......Step: 142/303....... Average Loss for Epoch: 0.01841099431056162\n",
      "Epoch 4......Step: 143/303....... Average Loss for Epoch: 0.018356451784241033\n",
      "Epoch 4......Step: 144/303....... Average Loss for Epoch: 0.018419081933744665\n",
      "Epoch 4......Step: 145/303....... Average Loss for Epoch: 0.018445917212500655\n",
      "Epoch 4......Step: 146/303....... Average Loss for Epoch: 0.01850145551924632\n",
      "Epoch 4......Step: 147/303....... Average Loss for Epoch: 0.018510505854830044\n",
      "Epoch 4......Step: 148/303....... Average Loss for Epoch: 0.018582331727737107\n",
      "Epoch 4......Step: 149/303....... Average Loss for Epoch: 0.018553079393436044\n",
      "Epoch 4......Step: 150/303....... Average Loss for Epoch: 0.01855613582457105\n",
      "Epoch 4......Step: 151/303....... Average Loss for Epoch: 0.018533206624996582\n",
      "Epoch 4......Step: 152/303....... Average Loss for Epoch: 0.018523450315880933\n",
      "Epoch 4......Step: 153/303....... Average Loss for Epoch: 0.018495130973557632\n",
      "Epoch 4......Step: 154/303....... Average Loss for Epoch: 0.018553209497121633\n",
      "Epoch 4......Step: 155/303....... Average Loss for Epoch: 0.018546097046665606\n",
      "Epoch 4......Step: 156/303....... Average Loss for Epoch: 0.0185404932186103\n",
      "Epoch 4......Step: 157/303....... Average Loss for Epoch: 0.018564889889661294\n",
      "Epoch 4......Step: 158/303....... Average Loss for Epoch: 0.018548536580055952\n",
      "Epoch 4......Step: 159/303....... Average Loss for Epoch: 0.018555844255354045\n",
      "Epoch 4......Step: 160/303....... Average Loss for Epoch: 0.018571610836079343\n",
      "Epoch 4......Step: 161/303....... Average Loss for Epoch: 0.018619843157042997\n",
      "Epoch 4......Step: 162/303....... Average Loss for Epoch: 0.018649769504267125\n",
      "Epoch 4......Step: 163/303....... Average Loss for Epoch: 0.01862907571935215\n",
      "Epoch 4......Step: 164/303....... Average Loss for Epoch: 0.01860318942813248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4......Step: 165/303....... Average Loss for Epoch: 0.018574391305446625\n",
      "Epoch 4......Step: 166/303....... Average Loss for Epoch: 0.018583956769252397\n",
      "Epoch 4......Step: 167/303....... Average Loss for Epoch: 0.018554318545687343\n",
      "Epoch 4......Step: 168/303....... Average Loss for Epoch: 0.01855421924985768\n",
      "Epoch 4......Step: 169/303....... Average Loss for Epoch: 0.018520922918071055\n",
      "Epoch 4......Step: 170/303....... Average Loss for Epoch: 0.01850726131678504\n",
      "Epoch 4......Step: 171/303....... Average Loss for Epoch: 0.018499415351502728\n",
      "Epoch 4......Step: 172/303....... Average Loss for Epoch: 0.01847657932206815\n",
      "Epoch 4......Step: 173/303....... Average Loss for Epoch: 0.018445589590408554\n",
      "Epoch 4......Step: 174/303....... Average Loss for Epoch: 0.018449462377519786\n",
      "Epoch 4......Step: 175/303....... Average Loss for Epoch: 0.018428633149181094\n",
      "Epoch 4......Step: 176/303....... Average Loss for Epoch: 0.01842898001301695\n",
      "Epoch 4......Step: 177/303....... Average Loss for Epoch: 0.018440812763016103\n",
      "Epoch 4......Step: 178/303....... Average Loss for Epoch: 0.018424317261643623\n",
      "Epoch 4......Step: 179/303....... Average Loss for Epoch: 0.0184252650357325\n",
      "Epoch 4......Step: 180/303....... Average Loss for Epoch: 0.018416874607404075\n",
      "Epoch 4......Step: 181/303....... Average Loss for Epoch: 0.01843061110838342\n",
      "Epoch 4......Step: 182/303....... Average Loss for Epoch: 0.018391582905059004\n",
      "Epoch 4......Step: 183/303....... Average Loss for Epoch: 0.018443103192648927\n",
      "Epoch 4......Step: 184/303....... Average Loss for Epoch: 0.01842003220530308\n",
      "Epoch 4......Step: 185/303....... Average Loss for Epoch: 0.018436544579831328\n",
      "Epoch 4......Step: 186/303....... Average Loss for Epoch: 0.01843807085226941\n",
      "Epoch 4......Step: 187/303....... Average Loss for Epoch: 0.01845039547804524\n",
      "Epoch 4......Step: 188/303....... Average Loss for Epoch: 0.01842704644188919\n",
      "Epoch 4......Step: 189/303....... Average Loss for Epoch: 0.01844222129140266\n",
      "Epoch 4......Step: 190/303....... Average Loss for Epoch: 0.01839712215097327\n",
      "Epoch 4......Step: 191/303....... Average Loss for Epoch: 0.01839213127865217\n",
      "Epoch 4......Step: 192/303....... Average Loss for Epoch: 0.01840548959444277\n",
      "Epoch 4......Step: 193/303....... Average Loss for Epoch: 0.018364074968110402\n",
      "Epoch 4......Step: 194/303....... Average Loss for Epoch: 0.01835410158178702\n",
      "Epoch 4......Step: 195/303....... Average Loss for Epoch: 0.018325638761505104\n",
      "Epoch 4......Step: 196/303....... Average Loss for Epoch: 0.018372893257408728\n",
      "Epoch 4......Step: 197/303....... Average Loss for Epoch: 0.018403320143065477\n",
      "Epoch 4......Step: 198/303....... Average Loss for Epoch: 0.018401377279349047\n",
      "Epoch 4......Step: 199/303....... Average Loss for Epoch: 0.018410530502921973\n",
      "Epoch 4......Step: 200/303....... Average Loss for Epoch: 0.018370456993579865\n",
      "Epoch 4......Step: 201/303....... Average Loss for Epoch: 0.018404283769317527\n",
      "Epoch 4......Step: 202/303....... Average Loss for Epoch: 0.01839677037874071\n",
      "Epoch 4......Step: 203/303....... Average Loss for Epoch: 0.018361511764121173\n",
      "Epoch 4......Step: 204/303....... Average Loss for Epoch: 0.018405858609898417\n",
      "Epoch 4......Step: 205/303....... Average Loss for Epoch: 0.018432449867449154\n",
      "Epoch 4......Step: 206/303....... Average Loss for Epoch: 0.018416303038163093\n",
      "Epoch 4......Step: 207/303....... Average Loss for Epoch: 0.018429946037378287\n",
      "Epoch 4......Step: 208/303....... Average Loss for Epoch: 0.01844820172454302\n",
      "Epoch 4......Step: 209/303....... Average Loss for Epoch: 0.01846014591531035\n",
      "Epoch 4......Step: 210/303....... Average Loss for Epoch: 0.0184546615130135\n",
      "Epoch 4......Step: 211/303....... Average Loss for Epoch: 0.01843680908806375\n",
      "Epoch 4......Step: 212/303....... Average Loss for Epoch: 0.018470184082376224\n",
      "Epoch 4......Step: 213/303....... Average Loss for Epoch: 0.018454447904592948\n",
      "Epoch 4......Step: 214/303....... Average Loss for Epoch: 0.01844103931597321\n",
      "Epoch 4......Step: 215/303....... Average Loss for Epoch: 0.01841815013958271\n",
      "Epoch 4......Step: 216/303....... Average Loss for Epoch: 0.018415687980854675\n",
      "Epoch 4......Step: 217/303....... Average Loss for Epoch: 0.018424673465162102\n",
      "Epoch 4......Step: 218/303....... Average Loss for Epoch: 0.018509417605680336\n",
      "Epoch 4......Step: 219/303....... Average Loss for Epoch: 0.0184943938551292\n",
      "Epoch 4......Step: 220/303....... Average Loss for Epoch: 0.018471517866816032\n",
      "Epoch 4......Step: 221/303....... Average Loss for Epoch: 0.018509163858360565\n",
      "Epoch 4......Step: 222/303....... Average Loss for Epoch: 0.018494284611094643\n",
      "Epoch 4......Step: 223/303....... Average Loss for Epoch: 0.018467691434639184\n",
      "Epoch 4......Step: 224/303....... Average Loss for Epoch: 0.018444624812608317\n",
      "Epoch 4......Step: 225/303....... Average Loss for Epoch: 0.018465215125017698\n",
      "Epoch 4......Step: 226/303....... Average Loss for Epoch: 0.018492115002158468\n",
      "Epoch 4......Step: 227/303....... Average Loss for Epoch: 0.018475959467238005\n",
      "Epoch 4......Step: 228/303....... Average Loss for Epoch: 0.018502518612270553\n",
      "Epoch 4......Step: 229/303....... Average Loss for Epoch: 0.018481969329077082\n",
      "Epoch 4......Step: 230/303....... Average Loss for Epoch: 0.018497905459093012\n",
      "Epoch 4......Step: 231/303....... Average Loss for Epoch: 0.018502190273690534\n",
      "Epoch 4......Step: 232/303....... Average Loss for Epoch: 0.018513058561125194\n",
      "Epoch 4......Step: 233/303....... Average Loss for Epoch: 0.01850394841459432\n",
      "Epoch 4......Step: 234/303....... Average Loss for Epoch: 0.01848414724565342\n",
      "Epoch 4......Step: 235/303....... Average Loss for Epoch: 0.018477391312889595\n",
      "Epoch 4......Step: 236/303....... Average Loss for Epoch: 0.018522917525842786\n",
      "Epoch 4......Step: 237/303....... Average Loss for Epoch: 0.018521333415894316\n",
      "Epoch 4......Step: 238/303....... Average Loss for Epoch: 0.01850366295010102\n",
      "Epoch 4......Step: 239/303....... Average Loss for Epoch: 0.01850983018842701\n",
      "Epoch 4......Step: 240/303....... Average Loss for Epoch: 0.01850771317258477\n",
      "Epoch 4......Step: 241/303....... Average Loss for Epoch: 0.018508329500610404\n",
      "Epoch 4......Step: 242/303....... Average Loss for Epoch: 0.0185137727136208\n",
      "Epoch 4......Step: 243/303....... Average Loss for Epoch: 0.01852735791186737\n",
      "Epoch 4......Step: 244/303....... Average Loss for Epoch: 0.01855818579950538\n",
      "Epoch 4......Step: 245/303....... Average Loss for Epoch: 0.01853925825229713\n",
      "Epoch 4......Step: 246/303....... Average Loss for Epoch: 0.018524048054908832\n",
      "Epoch 4......Step: 247/303....... Average Loss for Epoch: 0.018491049451745956\n",
      "Epoch 4......Step: 248/303....... Average Loss for Epoch: 0.018464902171774978\n",
      "Epoch 4......Step: 249/303....... Average Loss for Epoch: 0.01844732834018737\n",
      "Epoch 4......Step: 250/303....... Average Loss for Epoch: 0.018434554308652876\n",
      "Epoch 4......Step: 251/303....... Average Loss for Epoch: 0.018421149326570005\n",
      "Epoch 4......Step: 252/303....... Average Loss for Epoch: 0.01839656634477987\n",
      "Epoch 4......Step: 253/303....... Average Loss for Epoch: 0.018409936394351036\n",
      "Epoch 4......Step: 254/303....... Average Loss for Epoch: 0.018405548073204717\n",
      "Epoch 4......Step: 255/303....... Average Loss for Epoch: 0.018419622202568195\n",
      "Epoch 4......Step: 256/303....... Average Loss for Epoch: 0.018433908069710014\n",
      "Epoch 4......Step: 257/303....... Average Loss for Epoch: 0.018411756895395568\n",
      "Epoch 4......Step: 258/303....... Average Loss for Epoch: 0.018396099094416973\n",
      "Epoch 4......Step: 259/303....... Average Loss for Epoch: 0.018375247961420812\n",
      "Epoch 4......Step: 260/303....... Average Loss for Epoch: 0.018362621121251813\n",
      "Epoch 4......Step: 261/303....... Average Loss for Epoch: 0.018332167856851985\n",
      "Epoch 4......Step: 262/303....... Average Loss for Epoch: 0.018325759833749695\n",
      "Epoch 4......Step: 263/303....... Average Loss for Epoch: 0.01833650131600098\n",
      "Epoch 4......Step: 264/303....... Average Loss for Epoch: 0.018363967788349273\n",
      "Epoch 4......Step: 265/303....... Average Loss for Epoch: 0.018340515991989173\n",
      "Epoch 4......Step: 266/303....... Average Loss for Epoch: 0.01834353126053299\n",
      "Epoch 4......Step: 267/303....... Average Loss for Epoch: 0.018322644535493985\n",
      "Epoch 4......Step: 268/303....... Average Loss for Epoch: 0.018328826106500936\n",
      "Epoch 4......Step: 269/303....... Average Loss for Epoch: 0.018336395608420486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4......Step: 270/303....... Average Loss for Epoch: 0.018320428887037214\n",
      "Epoch 4......Step: 271/303....... Average Loss for Epoch: 0.01832680713137907\n",
      "Epoch 4......Step: 272/303....... Average Loss for Epoch: 0.018313324900672716\n",
      "Epoch 4......Step: 273/303....... Average Loss for Epoch: 0.018303112191704166\n",
      "Epoch 4......Step: 274/303....... Average Loss for Epoch: 0.018290594213363463\n",
      "Epoch 4......Step: 275/303....... Average Loss for Epoch: 0.018337327268990605\n",
      "Epoch 4......Step: 276/303....... Average Loss for Epoch: 0.01832549339092836\n",
      "Epoch 4......Step: 277/303....... Average Loss for Epoch: 0.01830993250133436\n",
      "Epoch 4......Step: 278/303....... Average Loss for Epoch: 0.01831890350850902\n",
      "Epoch 4......Step: 279/303....... Average Loss for Epoch: 0.018306073956325063\n",
      "Epoch 4......Step: 280/303....... Average Loss for Epoch: 0.018295815427388465\n",
      "Epoch 4......Step: 281/303....... Average Loss for Epoch: 0.01828890581519154\n",
      "Epoch 4......Step: 282/303....... Average Loss for Epoch: 0.0182935375693842\n",
      "Epoch 4......Step: 283/303....... Average Loss for Epoch: 0.018324476498913007\n",
      "Epoch 4......Step: 284/303....... Average Loss for Epoch: 0.018353783125451332\n",
      "Epoch 4......Step: 285/303....... Average Loss for Epoch: 0.018359668467912756\n",
      "Epoch 4......Step: 286/303....... Average Loss for Epoch: 0.01833413424087571\n",
      "Epoch 4......Step: 287/303....... Average Loss for Epoch: 0.018329869163991683\n",
      "Epoch 4......Step: 288/303....... Average Loss for Epoch: 0.01833505644592353\n",
      "Epoch 4......Step: 289/303....... Average Loss for Epoch: 0.018320232555656905\n",
      "Epoch 4......Step: 290/303....... Average Loss for Epoch: 0.018298150001671808\n",
      "Epoch 4......Step: 291/303....... Average Loss for Epoch: 0.018350249860448527\n",
      "Epoch 4......Step: 292/303....... Average Loss for Epoch: 0.01833802335959387\n",
      "Epoch 4......Step: 293/303....... Average Loss for Epoch: 0.018346975268255728\n",
      "Epoch 4......Step: 294/303....... Average Loss for Epoch: 0.01837093202511267\n",
      "Epoch 4......Step: 295/303....... Average Loss for Epoch: 0.018375258715981144\n",
      "Epoch 4......Step: 296/303....... Average Loss for Epoch: 0.018385042908374925\n",
      "Epoch 4......Step: 297/303....... Average Loss for Epoch: 0.018394601822953032\n",
      "Epoch 4......Step: 298/303....... Average Loss for Epoch: 0.018374394597298147\n",
      "Epoch 4......Step: 299/303....... Average Loss for Epoch: 0.018365944816690426\n",
      "Epoch 4......Step: 300/303....... Average Loss for Epoch: 0.01838627867711087\n",
      "Epoch 4......Step: 301/303....... Average Loss for Epoch: 0.018414876776502973\n",
      "Epoch 4......Step: 302/303....... Average Loss for Epoch: 0.018407575086079882\n",
      "Epoch 4......Step: 303/303....... Average Loss for Epoch: 0.018407059436903732\n",
      "Epoch 4/5 Done, Total Loss: 0.018407059436903732\n",
      "Time Elapsed for Epoch: 8.166733399999998 seconds\n",
      "Epoch 5......Step: 1/303....... Average Loss for Epoch: 0.014444730244576931\n",
      "Epoch 5......Step: 2/303....... Average Loss for Epoch: 0.017879299353808165\n",
      "Epoch 5......Step: 3/303....... Average Loss for Epoch: 0.01682764415939649\n",
      "Epoch 5......Step: 4/303....... Average Loss for Epoch: 0.02053208276629448\n",
      "Epoch 5......Step: 5/303....... Average Loss for Epoch: 0.021341440454125404\n",
      "Epoch 5......Step: 6/303....... Average Loss for Epoch: 0.02069913720091184\n",
      "Epoch 5......Step: 7/303....... Average Loss for Epoch: 0.02012882780815874\n",
      "Epoch 5......Step: 8/303....... Average Loss for Epoch: 0.019540377077646554\n",
      "Epoch 5......Step: 9/303....... Average Loss for Epoch: 0.018838756821221776\n",
      "Epoch 5......Step: 10/303....... Average Loss for Epoch: 0.018162384070456027\n",
      "Epoch 5......Step: 11/303....... Average Loss for Epoch: 0.018209404396739872\n",
      "Epoch 5......Step: 12/303....... Average Loss for Epoch: 0.017774973685542744\n",
      "Epoch 5......Step: 13/303....... Average Loss for Epoch: 0.0178338885307312\n",
      "Epoch 5......Step: 14/303....... Average Loss for Epoch: 0.017444859591445754\n",
      "Epoch 5......Step: 15/303....... Average Loss for Epoch: 0.017139202790955703\n",
      "Epoch 5......Step: 16/303....... Average Loss for Epoch: 0.016705981397535652\n",
      "Epoch 5......Step: 17/303....... Average Loss for Epoch: 0.016451495863935527\n",
      "Epoch 5......Step: 18/303....... Average Loss for Epoch: 0.016211147213147745\n",
      "Epoch 5......Step: 19/303....... Average Loss for Epoch: 0.016737692940391992\n",
      "Epoch 5......Step: 20/303....... Average Loss for Epoch: 0.01648708642460406\n",
      "Epoch 5......Step: 21/303....... Average Loss for Epoch: 0.01649185735732317\n",
      "Epoch 5......Step: 22/303....... Average Loss for Epoch: 0.016798775516111742\n",
      "Epoch 5......Step: 23/303....... Average Loss for Epoch: 0.016694540927267593\n",
      "Epoch 5......Step: 24/303....... Average Loss for Epoch: 0.016705756347316008\n",
      "Epoch 5......Step: 25/303....... Average Loss for Epoch: 0.016645444594323634\n",
      "Epoch 5......Step: 26/303....... Average Loss for Epoch: 0.01696955717097108\n",
      "Epoch 5......Step: 27/303....... Average Loss for Epoch: 0.016807201663377107\n",
      "Epoch 5......Step: 28/303....... Average Loss for Epoch: 0.016846607139866267\n",
      "Epoch 5......Step: 29/303....... Average Loss for Epoch: 0.01702407217616665\n",
      "Epoch 5......Step: 30/303....... Average Loss for Epoch: 0.016907503424833218\n",
      "Epoch 5......Step: 31/303....... Average Loss for Epoch: 0.016983430982837753\n",
      "Epoch 5......Step: 32/303....... Average Loss for Epoch: 0.016965294867986813\n",
      "Epoch 5......Step: 33/303....... Average Loss for Epoch: 0.01698140859265219\n",
      "Epoch 5......Step: 34/303....... Average Loss for Epoch: 0.017601300167905932\n",
      "Epoch 5......Step: 35/303....... Average Loss for Epoch: 0.017579444417996064\n",
      "Epoch 5......Step: 36/303....... Average Loss for Epoch: 0.017506986390799284\n",
      "Epoch 5......Step: 37/303....... Average Loss for Epoch: 0.01747404839340094\n",
      "Epoch 5......Step: 38/303....... Average Loss for Epoch: 0.017844903576923043\n",
      "Epoch 5......Step: 39/303....... Average Loss for Epoch: 0.017764894793239925\n",
      "Epoch 5......Step: 40/303....... Average Loss for Epoch: 0.017661788151599467\n",
      "Epoch 5......Step: 41/303....... Average Loss for Epoch: 0.017627061798986866\n",
      "Epoch 5......Step: 42/303....... Average Loss for Epoch: 0.017692141301397766\n",
      "Epoch 5......Step: 43/303....... Average Loss for Epoch: 0.017688294677713583\n",
      "Epoch 5......Step: 44/303....... Average Loss for Epoch: 0.017559579785235903\n",
      "Epoch 5......Step: 45/303....... Average Loss for Epoch: 0.01775432816810078\n",
      "Epoch 5......Step: 46/303....... Average Loss for Epoch: 0.017678240516587444\n",
      "Epoch 5......Step: 47/303....... Average Loss for Epoch: 0.017628640847954343\n",
      "Epoch 5......Step: 48/303....... Average Loss for Epoch: 0.01765797329911341\n",
      "Epoch 5......Step: 49/303....... Average Loss for Epoch: 0.017800956470321636\n",
      "Epoch 5......Step: 50/303....... Average Loss for Epoch: 0.01777998000383377\n",
      "Epoch 5......Step: 51/303....... Average Loss for Epoch: 0.017676170349267183\n",
      "Epoch 5......Step: 52/303....... Average Loss for Epoch: 0.017804007529496\n",
      "Epoch 5......Step: 53/303....... Average Loss for Epoch: 0.01798235437006883\n",
      "Epoch 5......Step: 54/303....... Average Loss for Epoch: 0.01796427450177294\n",
      "Epoch 5......Step: 55/303....... Average Loss for Epoch: 0.017941044457256795\n",
      "Epoch 5......Step: 56/303....... Average Loss for Epoch: 0.017918963434307704\n",
      "Epoch 5......Step: 57/303....... Average Loss for Epoch: 0.018161762946922528\n",
      "Epoch 5......Step: 58/303....... Average Loss for Epoch: 0.018117863155002224\n",
      "Epoch 5......Step: 59/303....... Average Loss for Epoch: 0.01802168543434749\n",
      "Epoch 5......Step: 60/303....... Average Loss for Epoch: 0.017945312553395827\n",
      "Epoch 5......Step: 61/303....... Average Loss for Epoch: 0.01799391784140321\n",
      "Epoch 5......Step: 62/303....... Average Loss for Epoch: 0.018425421969544505\n",
      "Epoch 5......Step: 63/303....... Average Loss for Epoch: 0.018381619116380102\n",
      "Epoch 5......Step: 64/303....... Average Loss for Epoch: 0.018307575199287385\n",
      "Epoch 5......Step: 65/303....... Average Loss for Epoch: 0.018307895585894584\n",
      "Epoch 5......Step: 66/303....... Average Loss for Epoch: 0.018420569231790123\n",
      "Epoch 5......Step: 67/303....... Average Loss for Epoch: 0.018423269322114206\n",
      "Epoch 5......Step: 68/303....... Average Loss for Epoch: 0.018447684507597897\n",
      "Epoch 5......Step: 69/303....... Average Loss for Epoch: 0.01848318743640962\n",
      "Epoch 5......Step: 70/303....... Average Loss for Epoch: 0.018477943379964147\n",
      "Epoch 5......Step: 71/303....... Average Loss for Epoch: 0.018393535416206003\n",
      "Epoch 5......Step: 72/303....... Average Loss for Epoch: 0.018457595944508083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5......Step: 73/303....... Average Loss for Epoch: 0.018459941694283322\n",
      "Epoch 5......Step: 74/303....... Average Loss for Epoch: 0.01838783289280695\n",
      "Epoch 5......Step: 75/303....... Average Loss for Epoch: 0.01857397262006998\n",
      "Epoch 5......Step: 76/303....... Average Loss for Epoch: 0.018559289422180308\n",
      "Epoch 5......Step: 77/303....... Average Loss for Epoch: 0.018603110359376902\n",
      "Epoch 5......Step: 78/303....... Average Loss for Epoch: 0.018577239870165404\n",
      "Epoch 5......Step: 79/303....... Average Loss for Epoch: 0.018562230257954025\n",
      "Epoch 5......Step: 80/303....... Average Loss for Epoch: 0.01855452061863616\n",
      "Epoch 5......Step: 81/303....... Average Loss for Epoch: 0.01860281708937736\n",
      "Epoch 5......Step: 82/303....... Average Loss for Epoch: 0.018645399783896965\n",
      "Epoch 5......Step: 83/303....... Average Loss for Epoch: 0.018570038714412464\n",
      "Epoch 5......Step: 84/303....... Average Loss for Epoch: 0.018535901648214177\n",
      "Epoch 5......Step: 85/303....... Average Loss for Epoch: 0.0184726293253548\n",
      "Epoch 5......Step: 86/303....... Average Loss for Epoch: 0.018493822775781155\n",
      "Epoch 5......Step: 87/303....... Average Loss for Epoch: 0.018476635688680344\n",
      "Epoch 5......Step: 88/303....... Average Loss for Epoch: 0.01852552406489849\n",
      "Epoch 5......Step: 89/303....... Average Loss for Epoch: 0.018484116814444573\n",
      "Epoch 5......Step: 90/303....... Average Loss for Epoch: 0.01850635984705554\n",
      "Epoch 5......Step: 91/303....... Average Loss for Epoch: 0.01858388378724947\n",
      "Epoch 5......Step: 92/303....... Average Loss for Epoch: 0.018528014644170584\n",
      "Epoch 5......Step: 93/303....... Average Loss for Epoch: 0.01859209047610401\n",
      "Epoch 5......Step: 94/303....... Average Loss for Epoch: 0.018577236365130607\n",
      "Epoch 5......Step: 95/303....... Average Loss for Epoch: 0.01852474417537451\n",
      "Epoch 5......Step: 96/303....... Average Loss for Epoch: 0.01846621248599452\n",
      "Epoch 5......Step: 97/303....... Average Loss for Epoch: 0.01844081089630262\n",
      "Epoch 5......Step: 98/303....... Average Loss for Epoch: 0.018441063228386397\n",
      "Epoch 5......Step: 99/303....... Average Loss for Epoch: 0.01848196986159592\n",
      "Epoch 5......Step: 100/303....... Average Loss for Epoch: 0.018562630405649542\n",
      "Epoch 5......Step: 101/303....... Average Loss for Epoch: 0.01851378704909936\n",
      "Epoch 5......Step: 102/303....... Average Loss for Epoch: 0.01854439707034651\n",
      "Epoch 5......Step: 103/303....... Average Loss for Epoch: 0.018535027235240035\n",
      "Epoch 5......Step: 104/303....... Average Loss for Epoch: 0.01856583372080842\n",
      "Epoch 5......Step: 105/303....... Average Loss for Epoch: 0.01854903076198839\n",
      "Epoch 5......Step: 106/303....... Average Loss for Epoch: 0.01863270304780805\n",
      "Epoch 5......Step: 107/303....... Average Loss for Epoch: 0.018598743958102765\n",
      "Epoch 5......Step: 108/303....... Average Loss for Epoch: 0.018554344678435614\n",
      "Epoch 5......Step: 109/303....... Average Loss for Epoch: 0.018627921452557823\n",
      "Epoch 5......Step: 110/303....... Average Loss for Epoch: 0.01856132528998635\n",
      "Epoch 5......Step: 111/303....... Average Loss for Epoch: 0.018559737231683086\n",
      "Epoch 5......Step: 112/303....... Average Loss for Epoch: 0.01858297931695623\n",
      "Epoch 5......Step: 113/303....... Average Loss for Epoch: 0.018534220301801654\n",
      "Epoch 5......Step: 114/303....... Average Loss for Epoch: 0.01852279732395944\n",
      "Epoch 5......Step: 115/303....... Average Loss for Epoch: 0.018469569710609705\n",
      "Epoch 5......Step: 116/303....... Average Loss for Epoch: 0.018434741726980126\n",
      "Epoch 5......Step: 117/303....... Average Loss for Epoch: 0.018522437025084455\n",
      "Epoch 5......Step: 118/303....... Average Loss for Epoch: 0.01852600765809164\n",
      "Epoch 5......Step: 119/303....... Average Loss for Epoch: 0.01865999253482378\n",
      "Epoch 5......Step: 120/303....... Average Loss for Epoch: 0.01862484196511408\n",
      "Epoch 5......Step: 121/303....... Average Loss for Epoch: 0.018602393567562103\n",
      "Epoch 5......Step: 122/303....... Average Loss for Epoch: 0.018581609302734742\n",
      "Epoch 5......Step: 123/303....... Average Loss for Epoch: 0.01857609925715904\n",
      "Epoch 5......Step: 124/303....... Average Loss for Epoch: 0.018539475487364877\n",
      "Epoch 5......Step: 125/303....... Average Loss for Epoch: 0.018485538110136986\n",
      "Epoch 5......Step: 126/303....... Average Loss for Epoch: 0.018541614865026777\n",
      "Epoch 5......Step: 127/303....... Average Loss for Epoch: 0.018517467347303713\n",
      "Epoch 5......Step: 128/303....... Average Loss for Epoch: 0.018550026405137032\n",
      "Epoch 5......Step: 129/303....... Average Loss for Epoch: 0.018547204012672108\n",
      "Epoch 5......Step: 130/303....... Average Loss for Epoch: 0.0185317299543665\n",
      "Epoch 5......Step: 131/303....... Average Loss for Epoch: 0.018534106095783584\n",
      "Epoch 5......Step: 132/303....... Average Loss for Epoch: 0.018508389366395546\n",
      "Epoch 5......Step: 133/303....... Average Loss for Epoch: 0.018538788311127433\n",
      "Epoch 5......Step: 134/303....... Average Loss for Epoch: 0.018526838772666098\n",
      "Epoch 5......Step: 135/303....... Average Loss for Epoch: 0.018551204905465798\n",
      "Epoch 5......Step: 136/303....... Average Loss for Epoch: 0.018575392992180938\n",
      "Epoch 5......Step: 137/303....... Average Loss for Epoch: 0.01854981247498824\n",
      "Epoch 5......Step: 138/303....... Average Loss for Epoch: 0.018519855482314808\n",
      "Epoch 5......Step: 139/303....... Average Loss for Epoch: 0.018496476810613123\n",
      "Epoch 5......Step: 140/303....... Average Loss for Epoch: 0.0184488524948912\n",
      "Epoch 5......Step: 141/303....... Average Loss for Epoch: 0.018479359831581724\n",
      "Epoch 5......Step: 142/303....... Average Loss for Epoch: 0.018500363559160436\n",
      "Epoch 5......Step: 143/303....... Average Loss for Epoch: 0.018533201510464396\n",
      "Epoch 5......Step: 144/303....... Average Loss for Epoch: 0.018504447109686833\n",
      "Epoch 5......Step: 145/303....... Average Loss for Epoch: 0.018530122524705428\n",
      "Epoch 5......Step: 146/303....... Average Loss for Epoch: 0.01849925611168146\n",
      "Epoch 5......Step: 147/303....... Average Loss for Epoch: 0.018470219931989707\n",
      "Epoch 5......Step: 148/303....... Average Loss for Epoch: 0.01842330746050622\n",
      "Epoch 5......Step: 149/303....... Average Loss for Epoch: 0.018467241960683924\n",
      "Epoch 5......Step: 150/303....... Average Loss for Epoch: 0.018425871537377438\n",
      "Epoch 5......Step: 151/303....... Average Loss for Epoch: 0.018402132828602726\n",
      "Epoch 5......Step: 152/303....... Average Loss for Epoch: 0.018450080784723946\n",
      "Epoch 5......Step: 153/303....... Average Loss for Epoch: 0.018450291206439335\n",
      "Epoch 5......Step: 154/303....... Average Loss for Epoch: 0.01841138219823698\n",
      "Epoch 5......Step: 155/303....... Average Loss for Epoch: 0.018377868741029695\n",
      "Epoch 5......Step: 156/303....... Average Loss for Epoch: 0.0183682647617295\n",
      "Epoch 5......Step: 157/303....... Average Loss for Epoch: 0.018349150871964776\n",
      "Epoch 5......Step: 158/303....... Average Loss for Epoch: 0.018316374643693997\n",
      "Epoch 5......Step: 159/303....... Average Loss for Epoch: 0.01829011002028326\n",
      "Epoch 5......Step: 160/303....... Average Loss for Epoch: 0.018262604757910594\n",
      "Epoch 5......Step: 161/303....... Average Loss for Epoch: 0.018269088090345355\n",
      "Epoch 5......Step: 162/303....... Average Loss for Epoch: 0.01823225464487885\n",
      "Epoch 5......Step: 163/303....... Average Loss for Epoch: 0.01822109249609014\n",
      "Epoch 5......Step: 164/303....... Average Loss for Epoch: 0.01818710170322802\n",
      "Epoch 5......Step: 165/303....... Average Loss for Epoch: 0.018152587793090126\n",
      "Epoch 5......Step: 166/303....... Average Loss for Epoch: 0.018107986346964377\n",
      "Epoch 5......Step: 167/303....... Average Loss for Epoch: 0.01809007213255781\n",
      "Epoch 5......Step: 168/303....... Average Loss for Epoch: 0.01810342489763917\n",
      "Epoch 5......Step: 169/303....... Average Loss for Epoch: 0.018063501206904472\n",
      "Epoch 5......Step: 170/303....... Average Loss for Epoch: 0.018055850124972708\n",
      "Epoch 5......Step: 171/303....... Average Loss for Epoch: 0.018128285609316407\n",
      "Epoch 5......Step: 172/303....... Average Loss for Epoch: 0.018101807699950283\n",
      "Epoch 5......Step: 173/303....... Average Loss for Epoch: 0.018064112520932808\n",
      "Epoch 5......Step: 174/303....... Average Loss for Epoch: 0.018031420683937854\n",
      "Epoch 5......Step: 175/303....... Average Loss for Epoch: 0.018025119735726288\n",
      "Epoch 5......Step: 176/303....... Average Loss for Epoch: 0.017986353814855895\n",
      "Epoch 5......Step: 177/303....... Average Loss for Epoch: 0.01803396957910667\n",
      "Epoch 5......Step: 178/303....... Average Loss for Epoch: 0.01802736512395773\n",
      "Epoch 5......Step: 179/303....... Average Loss for Epoch: 0.017988763557091437\n",
      "Epoch 5......Step: 180/303....... Average Loss for Epoch: 0.017962051648646594\n",
      "Epoch 5......Step: 181/303....... Average Loss for Epoch: 0.017934551524276234\n",
      "Epoch 5......Step: 182/303....... Average Loss for Epoch: 0.01795116571484359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5......Step: 183/303....... Average Loss for Epoch: 0.01790670931970324\n",
      "Epoch 5......Step: 184/303....... Average Loss for Epoch: 0.017898888524581234\n",
      "Epoch 5......Step: 185/303....... Average Loss for Epoch: 0.01789138509917098\n",
      "Epoch 5......Step: 186/303....... Average Loss for Epoch: 0.01791075101843284\n",
      "Epoch 5......Step: 187/303....... Average Loss for Epoch: 0.017923112346088503\n",
      "Epoch 5......Step: 188/303....... Average Loss for Epoch: 0.017938620693228664\n",
      "Epoch 5......Step: 189/303....... Average Loss for Epoch: 0.0179279939414411\n",
      "Epoch 5......Step: 190/303....... Average Loss for Epoch: 0.01795061652695662\n",
      "Epoch 5......Step: 191/303....... Average Loss for Epoch: 0.017920966859887408\n",
      "Epoch 5......Step: 192/303....... Average Loss for Epoch: 0.017933262735217188\n",
      "Epoch 5......Step: 193/303....... Average Loss for Epoch: 0.0180342843167368\n",
      "Epoch 5......Step: 194/303....... Average Loss for Epoch: 0.018027344248113558\n",
      "Epoch 5......Step: 195/303....... Average Loss for Epoch: 0.018020926559200654\n",
      "Epoch 5......Step: 196/303....... Average Loss for Epoch: 0.018005104326852123\n",
      "Epoch 5......Step: 197/303....... Average Loss for Epoch: 0.018032143667794123\n",
      "Epoch 5......Step: 198/303....... Average Loss for Epoch: 0.018056691078600858\n",
      "Epoch 5......Step: 199/303....... Average Loss for Epoch: 0.018051996855205626\n",
      "Epoch 5......Step: 200/303....... Average Loss for Epoch: 0.018061185199767352\n",
      "Epoch 5......Step: 201/303....... Average Loss for Epoch: 0.018044520896947504\n",
      "Epoch 5......Step: 202/303....... Average Loss for Epoch: 0.01801460729174242\n",
      "Epoch 5......Step: 203/303....... Average Loss for Epoch: 0.01800476491286103\n",
      "Epoch 5......Step: 204/303....... Average Loss for Epoch: 0.017974063507555163\n",
      "Epoch 5......Step: 205/303....... Average Loss for Epoch: 0.017946536284757824\n",
      "Epoch 5......Step: 206/303....... Average Loss for Epoch: 0.017960892592384977\n",
      "Epoch 5......Step: 207/303....... Average Loss for Epoch: 0.017926892126204023\n",
      "Epoch 5......Step: 208/303....... Average Loss for Epoch: 0.01800056079026455\n",
      "Epoch 5......Step: 209/303....... Average Loss for Epoch: 0.018002363149094525\n",
      "Epoch 5......Step: 210/303....... Average Loss for Epoch: 0.01797483559875261\n",
      "Epoch 5......Step: 211/303....... Average Loss for Epoch: 0.017971310773378865\n",
      "Epoch 5......Step: 212/303....... Average Loss for Epoch: 0.017975446618741977\n",
      "Epoch 5......Step: 213/303....... Average Loss for Epoch: 0.017956851142078217\n",
      "Epoch 5......Step: 214/303....... Average Loss for Epoch: 0.017978200519231158\n",
      "Epoch 5......Step: 215/303....... Average Loss for Epoch: 0.017962843145048896\n",
      "Epoch 5......Step: 216/303....... Average Loss for Epoch: 0.017957287288650318\n",
      "Epoch 5......Step: 217/303....... Average Loss for Epoch: 0.01799006397891704\n",
      "Epoch 5......Step: 218/303....... Average Loss for Epoch: 0.017965300593935293\n",
      "Epoch 5......Step: 219/303....... Average Loss for Epoch: 0.01795210260642718\n",
      "Epoch 5......Step: 220/303....... Average Loss for Epoch: 0.017977841054512694\n",
      "Epoch 5......Step: 221/303....... Average Loss for Epoch: 0.017965030308593723\n",
      "Epoch 5......Step: 222/303....... Average Loss for Epoch: 0.01796253870014806\n",
      "Epoch 5......Step: 223/303....... Average Loss for Epoch: 0.017980920029762346\n",
      "Epoch 5......Step: 224/303....... Average Loss for Epoch: 0.017945971897071495\n",
      "Epoch 5......Step: 225/303....... Average Loss for Epoch: 0.01795382677680916\n",
      "Epoch 5......Step: 226/303....... Average Loss for Epoch: 0.017923719047741815\n",
      "Epoch 5......Step: 227/303....... Average Loss for Epoch: 0.017901965642618714\n",
      "Epoch 5......Step: 228/303....... Average Loss for Epoch: 0.017876756124263794\n",
      "Epoch 5......Step: 229/303....... Average Loss for Epoch: 0.01787509401552542\n",
      "Epoch 5......Step: 230/303....... Average Loss for Epoch: 0.017856301517104326\n",
      "Epoch 5......Step: 231/303....... Average Loss for Epoch: 0.01787678992932106\n",
      "Epoch 5......Step: 232/303....... Average Loss for Epoch: 0.01787569162291314\n",
      "Epoch 5......Step: 233/303....... Average Loss for Epoch: 0.017908282757860654\n",
      "Epoch 5......Step: 234/303....... Average Loss for Epoch: 0.017917547657544542\n",
      "Epoch 5......Step: 235/303....... Average Loss for Epoch: 0.01791400602285532\n",
      "Epoch 5......Step: 236/303....... Average Loss for Epoch: 0.017887090644562394\n",
      "Epoch 5......Step: 237/303....... Average Loss for Epoch: 0.017867059521534272\n",
      "Epoch 5......Step: 238/303....... Average Loss for Epoch: 0.017848041515108666\n",
      "Epoch 5......Step: 239/303....... Average Loss for Epoch: 0.01784704694578962\n",
      "Epoch 5......Step: 240/303....... Average Loss for Epoch: 0.01783141706061239\n",
      "Epoch 5......Step: 241/303....... Average Loss for Epoch: 0.01784381624002056\n",
      "Epoch 5......Step: 242/303....... Average Loss for Epoch: 0.017819603077667064\n",
      "Epoch 5......Step: 243/303....... Average Loss for Epoch: 0.017799361628085496\n",
      "Epoch 5......Step: 244/303....... Average Loss for Epoch: 0.017783199169779898\n",
      "Epoch 5......Step: 245/303....... Average Loss for Epoch: 0.017770240497680344\n",
      "Epoch 5......Step: 246/303....... Average Loss for Epoch: 0.017765699715421693\n",
      "Epoch 5......Step: 247/303....... Average Loss for Epoch: 0.017780955258741793\n",
      "Epoch 5......Step: 248/303....... Average Loss for Epoch: 0.01777052818121569\n",
      "Epoch 5......Step: 249/303....... Average Loss for Epoch: 0.01777109263995924\n",
      "Epoch 5......Step: 250/303....... Average Loss for Epoch: 0.017766905274242163\n",
      "Epoch 5......Step: 251/303....... Average Loss for Epoch: 0.017781911258292624\n",
      "Epoch 5......Step: 252/303....... Average Loss for Epoch: 0.01777141999512438\n",
      "Epoch 5......Step: 253/303....... Average Loss for Epoch: 0.01781717171016418\n",
      "Epoch 5......Step: 254/303....... Average Loss for Epoch: 0.017834369995169283\n",
      "Epoch 5......Step: 255/303....... Average Loss for Epoch: 0.017816404107154585\n",
      "Epoch 5......Step: 256/303....... Average Loss for Epoch: 0.017819031396356877\n",
      "Epoch 5......Step: 257/303....... Average Loss for Epoch: 0.01782047167304425\n",
      "Epoch 5......Step: 258/303....... Average Loss for Epoch: 0.017803053165850943\n",
      "Epoch 5......Step: 259/303....... Average Loss for Epoch: 0.01778225739518878\n",
      "Epoch 5......Step: 260/303....... Average Loss for Epoch: 0.017759583034337712\n",
      "Epoch 5......Step: 261/303....... Average Loss for Epoch: 0.017754688362994184\n",
      "Epoch 5......Step: 262/303....... Average Loss for Epoch: 0.017737249730026904\n",
      "Epoch 5......Step: 263/303....... Average Loss for Epoch: 0.01774657770509729\n",
      "Epoch 5......Step: 264/303....... Average Loss for Epoch: 0.017791019632678592\n",
      "Epoch 5......Step: 265/303....... Average Loss for Epoch: 0.017809863149557473\n",
      "Epoch 5......Step: 266/303....... Average Loss for Epoch: 0.017787729190396413\n",
      "Epoch 5......Step: 267/303....... Average Loss for Epoch: 0.017772469394020596\n",
      "Epoch 5......Step: 268/303....... Average Loss for Epoch: 0.01777677884700694\n",
      "Epoch 5......Step: 269/303....... Average Loss for Epoch: 0.017796598142878494\n",
      "Epoch 5......Step: 270/303....... Average Loss for Epoch: 0.017844245572470958\n",
      "Epoch 5......Step: 271/303....... Average Loss for Epoch: 0.017861393720213996\n",
      "Epoch 5......Step: 272/303....... Average Loss for Epoch: 0.017851415538804278\n",
      "Epoch 5......Step: 273/303....... Average Loss for Epoch: 0.017854446541831825\n",
      "Epoch 5......Step: 274/303....... Average Loss for Epoch: 0.017852548207326308\n",
      "Epoch 5......Step: 275/303....... Average Loss for Epoch: 0.017863138666884465\n",
      "Epoch 5......Step: 276/303....... Average Loss for Epoch: 0.017849577759541033\n",
      "Epoch 5......Step: 277/303....... Average Loss for Epoch: 0.017848585399425847\n",
      "Epoch 5......Step: 278/303....... Average Loss for Epoch: 0.01784694174693214\n",
      "Epoch 5......Step: 279/303....... Average Loss for Epoch: 0.0178347748360433\n",
      "Epoch 5......Step: 280/303....... Average Loss for Epoch: 0.0178553725526269\n",
      "Epoch 5......Step: 281/303....... Average Loss for Epoch: 0.01787999556714742\n",
      "Epoch 5......Step: 282/303....... Average Loss for Epoch: 0.017865073233050234\n",
      "Epoch 5......Step: 283/303....... Average Loss for Epoch: 0.017881338823547213\n",
      "Epoch 5......Step: 284/303....... Average Loss for Epoch: 0.017904856681666324\n",
      "Epoch 5......Step: 285/303....... Average Loss for Epoch: 0.01790187796741201\n",
      "Epoch 5......Step: 286/303....... Average Loss for Epoch: 0.017886066656247094\n",
      "Epoch 5......Step: 287/303....... Average Loss for Epoch: 0.01791979625288929\n",
      "Epoch 5......Step: 288/303....... Average Loss for Epoch: 0.0179051196642427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5......Step: 289/303....... Average Loss for Epoch: 0.017886357765425655\n",
      "Epoch 5......Step: 290/303....... Average Loss for Epoch: 0.017893231541303727\n",
      "Epoch 5......Step: 291/303....... Average Loss for Epoch: 0.017876548942216894\n",
      "Epoch 5......Step: 292/303....... Average Loss for Epoch: 0.017858852348481752\n",
      "Epoch 5......Step: 293/303....... Average Loss for Epoch: 0.017840575210673816\n",
      "Epoch 5......Step: 294/303....... Average Loss for Epoch: 0.01784877605153387\n",
      "Epoch 5......Step: 295/303....... Average Loss for Epoch: 0.01782179157549547\n",
      "Epoch 5......Step: 296/303....... Average Loss for Epoch: 0.017852422828484024\n",
      "Epoch 5......Step: 297/303....... Average Loss for Epoch: 0.017830203713767655\n",
      "Epoch 5......Step: 298/303....... Average Loss for Epoch: 0.017811548521104677\n",
      "Epoch 5......Step: 299/303....... Average Loss for Epoch: 0.01779443801172003\n",
      "Epoch 5......Step: 300/303....... Average Loss for Epoch: 0.01778216617181897\n",
      "Epoch 5......Step: 301/303....... Average Loss for Epoch: 0.017801691855356544\n",
      "Epoch 5......Step: 302/303....... Average Loss for Epoch: 0.01777733066458477\n",
      "Epoch 5......Step: 303/303....... Average Loss for Epoch: 0.017762665593806078\n",
      "Epoch 5/5 Done, Total Loss: 0.017762665593806078\n",
      "Time Elapsed for Epoch: 8.019867999999995 seconds\n",
      "Total Training Time: 39.018735299999996 seconds\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "gru_model = train(train_loader, lr, model_type=\"GRU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Time: 0.0597800000000035\n",
      "sMAPE: 6.287702921710172%\n"
     ]
    }
   ],
   "source": [
    "gru_outputs, targets, gru_sMAPE = evaluate(gru_model, test_x, test_y, label_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800\n"
     ]
    }
   ],
   "source": [
    "print(len(gru_outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1149.1997378 ,  618.90084829,  938.33357972, ...,  289.60656023,\n",
      "        389.43621908,  578.39126877])]\n"
     ]
    }
   ],
   "source": [
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(gru_outputs[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(targets[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
