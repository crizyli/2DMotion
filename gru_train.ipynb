{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lh</th>\n",
       "      <th>rh</th>\n",
       "      <th>ls</th>\n",
       "      <th>rs</th>\n",
       "      <th>w</th>\n",
       "      <th>ll</th>\n",
       "      <th>rl</th>\n",
       "      <th>1x</th>\n",
       "      <th>1y</th>\n",
       "      <th>2x</th>\n",
       "      <th>...</th>\n",
       "      <th>21x</th>\n",
       "      <th>21y</th>\n",
       "      <th>22x</th>\n",
       "      <th>22y</th>\n",
       "      <th>23x</th>\n",
       "      <th>23y</th>\n",
       "      <th>24x</th>\n",
       "      <th>24y</th>\n",
       "      <th>25x</th>\n",
       "      <th>25y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2104</td>\n",
       "      <td>6495</td>\n",
       "      <td>289</td>\n",
       "      <td>0</td>\n",
       "      <td>370</td>\n",
       "      <td>1001.090</td>\n",
       "      <td>224.108</td>\n",
       "      <td>998.361</td>\n",
       "      <td>...</td>\n",
       "      <td>1113.03</td>\n",
       "      <td>1015.90</td>\n",
       "      <td>1051.21</td>\n",
       "      <td>977.727</td>\n",
       "      <td>833.350</td>\n",
       "      <td>998.114</td>\n",
       "      <td>827.522</td>\n",
       "      <td>977.748</td>\n",
       "      <td>892.342</td>\n",
       "      <td>965.911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2595</td>\n",
       "      <td>6472</td>\n",
       "      <td>288</td>\n",
       "      <td>0</td>\n",
       "      <td>371</td>\n",
       "      <td>998.301</td>\n",
       "      <td>233.068</td>\n",
       "      <td>998.183</td>\n",
       "      <td>...</td>\n",
       "      <td>1110.15</td>\n",
       "      <td>1015.89</td>\n",
       "      <td>1051.20</td>\n",
       "      <td>980.579</td>\n",
       "      <td>821.632</td>\n",
       "      <td>998.192</td>\n",
       "      <td>815.856</td>\n",
       "      <td>980.623</td>\n",
       "      <td>895.222</td>\n",
       "      <td>968.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>6502</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>370</td>\n",
       "      <td>998.301</td>\n",
       "      <td>233.068</td>\n",
       "      <td>998.183</td>\n",
       "      <td>...</td>\n",
       "      <td>1110.15</td>\n",
       "      <td>1015.89</td>\n",
       "      <td>1051.20</td>\n",
       "      <td>980.579</td>\n",
       "      <td>821.632</td>\n",
       "      <td>998.192</td>\n",
       "      <td>815.856</td>\n",
       "      <td>980.623</td>\n",
       "      <td>895.222</td>\n",
       "      <td>968.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2152</td>\n",
       "      <td>6620</td>\n",
       "      <td>285</td>\n",
       "      <td>0</td>\n",
       "      <td>370</td>\n",
       "      <td>998.290</td>\n",
       "      <td>235.914</td>\n",
       "      <td>998.241</td>\n",
       "      <td>...</td>\n",
       "      <td>1110.13</td>\n",
       "      <td>1015.91</td>\n",
       "      <td>1048.35</td>\n",
       "      <td>977.770</td>\n",
       "      <td>818.732</td>\n",
       "      <td>998.183</td>\n",
       "      <td>815.843</td>\n",
       "      <td>980.596</td>\n",
       "      <td>895.207</td>\n",
       "      <td>968.786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2373</td>\n",
       "      <td>5716</td>\n",
       "      <td>282</td>\n",
       "      <td>0</td>\n",
       "      <td>368</td>\n",
       "      <td>998.338</td>\n",
       "      <td>235.914</td>\n",
       "      <td>998.328</td>\n",
       "      <td>...</td>\n",
       "      <td>1110.14</td>\n",
       "      <td>1015.91</td>\n",
       "      <td>1048.41</td>\n",
       "      <td>977.806</td>\n",
       "      <td>818.758</td>\n",
       "      <td>998.214</td>\n",
       "      <td>815.848</td>\n",
       "      <td>980.633</td>\n",
       "      <td>895.233</td>\n",
       "      <td>965.924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lh  rh    ls    rs    w  ll   rl        1x       1y       2x  ...      21x  \\\n",
       "0   0   0  2104  6495  289   0  370  1001.090  224.108  998.361  ...  1113.03   \n",
       "1   0   0  2595  6472  288   0  371   998.301  233.068  998.183  ...  1110.15   \n",
       "2   0   0  1987  6502  286   0  370   998.301  233.068  998.183  ...  1110.15   \n",
       "3   0   0  2152  6620  285   0  370   998.290  235.914  998.241  ...  1110.13   \n",
       "4   0   0  2373  5716  282   0  368   998.338  235.914  998.328  ...  1110.14   \n",
       "\n",
       "       21y      22x      22y      23x      23y      24x      24y      25x  \\\n",
       "0  1015.90  1051.21  977.727  833.350  998.114  827.522  977.748  892.342   \n",
       "1  1015.89  1051.20  980.579  821.632  998.192  815.856  980.623  895.222   \n",
       "2  1015.89  1051.20  980.579  821.632  998.192  815.856  980.623  895.222   \n",
       "3  1015.91  1048.35  977.770  818.732  998.183  815.843  980.596  895.207   \n",
       "4  1015.91  1048.41  977.806  818.758  998.214  815.848  980.633  895.233   \n",
       "\n",
       "       25y  \n",
       "0  965.911  \n",
       "1  968.837  \n",
       "2  968.837  \n",
       "3  968.786  \n",
       "4  965.924  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('action1_processed/action1.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MinMaxScaler' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-ca28275ebf5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Obtaining the Scale for the labels(usage data) so that output can be re-scaled to actual value during evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mlabel_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m57\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'MinMaxScaler' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "train_x = []\n",
    "\n",
    "# Store csv file in a Pandas DataFrame\n",
    "df = pd.read_csv('action1_processed/action1.csv')\n",
    "\n",
    "# Scaling the input data\n",
    "sc = MinMaxScaler()\n",
    "label_sc = MinMaxScaler()\n",
    "data = sc.fit_transform(df.values)\n",
    "\n",
    "# Obtaining the Scale for the labels(usage data) so that output can be re-scaled to actual value during evaluation\n",
    "label_sc.fit(df.iloc[:,7:57].values.reshape(-1,1))\n",
    "print(label_sc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3367, 10, 57)\n",
      "(3367, 50)\n",
      "(3367, 10, 57)\n",
      "(3367, 50)\n"
     ]
    }
   ],
   "source": [
    "# Define lookback period and split inputs/labels\n",
    "lookback = 10\n",
    "inputs = np.zeros((len(data)-lookback,lookback,df.shape[1]))\n",
    "labels = np.zeros((len(data)-lookback,50))\n",
    "print(inputs.shape)\n",
    "print(labels.shape)\n",
    "for i in range(lookback, len(data)):\n",
    "    inputs[i-lookback] = data[i-lookback:i]\n",
    "    labels[i-lookback] = data[i][7:57]\n",
    "inputs = inputs.reshape(-1,lookback,df.shape[1])\n",
    "labels = labels.reshape(-1,50)\n",
    "print(inputs.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train/test portions and combining all data from different files into a single array\n",
    "test_portion = int(0.1*len(inputs))\n",
    "train_x = inputs[:-test_portion]\n",
    "train_y = labels[:-test_portion]\n",
    "test_x = inputs[-test_portion:]\n",
    "test_y = labels[-test_portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10, 57])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "input_dim = next(iter(train_loader))[0].shape\n",
    "print(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, learn_rate, hidden_dim=256, EPOCHS=5, model_type=\"GRU\"):\n",
    "    \n",
    "    # Setting common hyperparameters\n",
    "    input_dim = next(iter(train_loader))[0].shape[2]\n",
    "    output_dim = 50\n",
    "    n_layers = 2\n",
    "    # Instantiating the model\n",
    "    model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Defining loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "    \n",
    "    model.train()\n",
    "    print(\"Starting Training of {} model\".format(model_type))\n",
    "    epoch_times = []\n",
    "    # Start training loop\n",
    "    for epoch in range(1,EPOCHS+1):\n",
    "        start_time = time.clock()\n",
    "        h = model.init_hidden(batch_size)\n",
    "        avg_loss = 0.\n",
    "        counter = 0\n",
    "        for x, label in train_loader:\n",
    "            counter += 1\n",
    "            h = h.data\n",
    "            model.zero_grad()\n",
    "            \n",
    "            out, h = model(x.to(device).float(), h)\n",
    "            loss = criterion(out, label.to(device).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "            #if counter%20 == 0:\n",
    "            print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, counter, len(train_loader), avg_loss/counter))\n",
    "        current_time = time.clock()\n",
    "        print(\"Epoch {}/{} Done, Total Loss: {}\".format(epoch, EPOCHS, avg_loss/len(train_loader)))\n",
    "        print(\"Time Elapsed for Epoch: {} seconds\".format(str(current_time-start_time)))\n",
    "        epoch_times.append(current_time-start_time)\n",
    "    print(\"Total Training Time: {} seconds\".format(str(sum(epoch_times))))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_x, test_y, label_sc):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    start_time = time.clock()\n",
    "    \n",
    "    inp = torch.from_numpy(np.array(test_x))\n",
    "    labs = torch.from_numpy(np.array(test_y))\n",
    "    h = model.init_hidden(inp.shape[0])\n",
    "    out, h = model(inp.to(device).float(), h)\n",
    "    outputs.append(label_sc.inverse_transform(out.cpu().detach().numpy()).reshape(-1))\n",
    "    targets.append(label_sc.inverse_transform(labs.numpy()).reshape(-1))\n",
    "    \n",
    "    print(\"Evaluation Time: {}\".format(str(time.clock()-start_time)))\n",
    "    sMAPE = 0\n",
    "    for i in range(len(outputs)):\n",
    "        sMAPE += np.mean(abs(outputs[i]-targets[i])/(targets[i]+outputs[i])/2)/len(outputs)\n",
    "    print(\"sMAPE: {}%\".format(sMAPE*100))\n",
    "    return outputs, targets, sMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training of GRU model\n",
      "Epoch 1......Step: 1/303....... Average Loss for Epoch: 0.3210621178150177\n",
      "Epoch 1......Step: 2/303....... Average Loss for Epoch: 0.29572588205337524\n",
      "Epoch 1......Step: 3/303....... Average Loss for Epoch: 0.27231544256210327\n",
      "Epoch 1......Step: 4/303....... Average Loss for Epoch: 0.2419430911540985\n",
      "Epoch 1......Step: 5/303....... Average Loss for Epoch: 0.21058094948530198\n",
      "Epoch 1......Step: 6/303....... Average Loss for Epoch: 0.18632695575555167\n",
      "Epoch 1......Step: 7/303....... Average Loss for Epoch: 0.17063652724027634\n",
      "Epoch 1......Step: 8/303....... Average Loss for Epoch: 0.15722214058041573\n",
      "Epoch 1......Step: 9/303....... Average Loss for Epoch: 0.14417381915781233\n",
      "Epoch 1......Step: 10/303....... Average Loss for Epoch: 0.13290211409330369\n",
      "Epoch 1......Step: 11/303....... Average Loss for Epoch: 0.12411066855896603\n",
      "Epoch 1......Step: 12/303....... Average Loss for Epoch: 0.11727562795082729\n",
      "Epoch 1......Step: 13/303....... Average Loss for Epoch: 0.1117857124369878\n",
      "Epoch 1......Step: 14/303....... Average Loss for Epoch: 0.10846014241022724\n",
      "Epoch 1......Step: 15/303....... Average Loss for Epoch: 0.10439485559860866\n",
      "Epoch 1......Step: 16/303....... Average Loss for Epoch: 0.10003344947472215\n",
      "Epoch 1......Step: 17/303....... Average Loss for Epoch: 0.09636865709634389\n",
      "Epoch 1......Step: 18/303....... Average Loss for Epoch: 0.09325966383847925\n",
      "Epoch 1......Step: 19/303....... Average Loss for Epoch: 0.0902160239454947\n",
      "Epoch 1......Step: 20/303....... Average Loss for Epoch: 0.08765319995582103\n",
      "Epoch 1......Step: 21/303....... Average Loss for Epoch: 0.08534272316665877\n",
      "Epoch 1......Step: 22/303....... Average Loss for Epoch: 0.08297591419382529\n",
      "Epoch 1......Step: 23/303....... Average Loss for Epoch: 0.08115186169743538\n",
      "Epoch 1......Step: 24/303....... Average Loss for Epoch: 0.07893349082830052\n",
      "Epoch 1......Step: 25/303....... Average Loss for Epoch: 0.07710810951888561\n",
      "Epoch 1......Step: 26/303....... Average Loss for Epoch: 0.07545184623450041\n",
      "Epoch 1......Step: 27/303....... Average Loss for Epoch: 0.07397791170687587\n",
      "Epoch 1......Step: 28/303....... Average Loss for Epoch: 0.07256566234199065\n",
      "Epoch 1......Step: 29/303....... Average Loss for Epoch: 0.07108451406760462\n",
      "Epoch 1......Step: 30/303....... Average Loss for Epoch: 0.06984158251434565\n",
      "Epoch 1......Step: 31/303....... Average Loss for Epoch: 0.06885092783599131\n",
      "Epoch 1......Step: 32/303....... Average Loss for Epoch: 0.06808319775154814\n",
      "Epoch 1......Step: 33/303....... Average Loss for Epoch: 0.06698606078597633\n",
      "Epoch 1......Step: 34/303....... Average Loss for Epoch: 0.06595296549665577\n",
      "Epoch 1......Step: 35/303....... Average Loss for Epoch: 0.06492284775844642\n",
      "Epoch 1......Step: 36/303....... Average Loss for Epoch: 0.06401822038201822\n",
      "Epoch 1......Step: 37/303....... Average Loss for Epoch: 0.06318504408606\n",
      "Epoch 1......Step: 38/303....... Average Loss for Epoch: 0.062310947350373395\n",
      "Epoch 1......Step: 39/303....... Average Loss for Epoch: 0.061367178526826396\n",
      "Epoch 1......Step: 40/303....... Average Loss for Epoch: 0.060803437465801836\n",
      "Epoch 1......Step: 41/303....... Average Loss for Epoch: 0.060073234913189236\n",
      "Epoch 1......Step: 42/303....... Average Loss for Epoch: 0.05914785486779043\n",
      "Epoch 1......Step: 43/303....... Average Loss for Epoch: 0.05851340488812258\n",
      "Epoch 1......Step: 44/303....... Average Loss for Epoch: 0.05775621783157641\n",
      "Epoch 1......Step: 45/303....... Average Loss for Epoch: 0.05719307309223546\n",
      "Epoch 1......Step: 46/303....... Average Loss for Epoch: 0.056550910777371864\n",
      "Epoch 1......Step: 47/303....... Average Loss for Epoch: 0.055916549598282954\n",
      "Epoch 1......Step: 48/303....... Average Loss for Epoch: 0.05524179704176883\n",
      "Epoch 1......Step: 49/303....... Average Loss for Epoch: 0.05466037676954756\n",
      "Epoch 1......Step: 50/303....... Average Loss for Epoch: 0.05403285149484873\n",
      "Epoch 1......Step: 51/303....... Average Loss for Epoch: 0.05347593807998825\n",
      "Epoch 1......Step: 52/303....... Average Loss for Epoch: 0.05316564526695471\n",
      "Epoch 1......Step: 53/303....... Average Loss for Epoch: 0.05304061045061867\n",
      "Epoch 1......Step: 54/303....... Average Loss for Epoch: 0.052635832586222224\n",
      "Epoch 1......Step: 55/303....... Average Loss for Epoch: 0.05205888402732936\n",
      "Epoch 1......Step: 56/303....... Average Loss for Epoch: 0.051535833427416425\n",
      "Epoch 1......Step: 57/303....... Average Loss for Epoch: 0.05107096327762855\n",
      "Epoch 1......Step: 58/303....... Average Loss for Epoch: 0.050603428438048936\n",
      "Epoch 1......Step: 59/303....... Average Loss for Epoch: 0.05018043922165693\n",
      "Epoch 1......Step: 60/303....... Average Loss for Epoch: 0.04967552799110611\n",
      "Epoch 1......Step: 61/303....... Average Loss for Epoch: 0.04914061190774206\n",
      "Epoch 1......Step: 62/303....... Average Loss for Epoch: 0.04877674540564898\n",
      "Epoch 1......Step: 63/303....... Average Loss for Epoch: 0.048416597886927544\n",
      "Epoch 1......Step: 64/303....... Average Loss for Epoch: 0.04802056844346225\n",
      "Epoch 1......Step: 65/303....... Average Loss for Epoch: 0.04778758126955766\n",
      "Epoch 1......Step: 66/303....... Average Loss for Epoch: 0.047502961066184624\n",
      "Epoch 1......Step: 67/303....... Average Loss for Epoch: 0.04717803729781464\n",
      "Epoch 1......Step: 68/303....... Average Loss for Epoch: 0.046796085586880934\n",
      "Epoch 1......Step: 69/303....... Average Loss for Epoch: 0.0464645832831013\n",
      "Epoch 1......Step: 70/303....... Average Loss for Epoch: 0.04609769353909152\n",
      "Epoch 1......Step: 71/303....... Average Loss for Epoch: 0.045800927134466846\n",
      "Epoch 1......Step: 72/303....... Average Loss for Epoch: 0.04546915656990475\n",
      "Epoch 1......Step: 73/303....... Average Loss for Epoch: 0.04513112149418217\n",
      "Epoch 1......Step: 74/303....... Average Loss for Epoch: 0.04485893773066031\n",
      "Epoch 1......Step: 75/303....... Average Loss for Epoch: 0.04452534635861715\n",
      "Epoch 1......Step: 76/303....... Average Loss for Epoch: 0.04422194760684904\n",
      "Epoch 1......Step: 77/303....... Average Loss for Epoch: 0.04400171811220708\n",
      "Epoch 1......Step: 78/303....... Average Loss for Epoch: 0.04383676475248276\n",
      "Epoch 1......Step: 79/303....... Average Loss for Epoch: 0.043657248698269265\n",
      "Epoch 1......Step: 80/303....... Average Loss for Epoch: 0.04351351226214319\n",
      "Epoch 1......Step: 81/303....... Average Loss for Epoch: 0.04321874218222536\n",
      "Epoch 1......Step: 82/303....... Average Loss for Epoch: 0.04296109055327933\n",
      "Epoch 1......Step: 83/303....... Average Loss for Epoch: 0.042760904945702435\n",
      "Epoch 1......Step: 84/303....... Average Loss for Epoch: 0.0425026632091474\n",
      "Epoch 1......Step: 85/303....... Average Loss for Epoch: 0.04228062940871014\n",
      "Epoch 1......Step: 86/303....... Average Loss for Epoch: 0.042031711205666844\n",
      "Epoch 1......Step: 87/303....... Average Loss for Epoch: 0.041937011904243766\n",
      "Epoch 1......Step: 88/303....... Average Loss for Epoch: 0.0416666318831796\n",
      "Epoch 1......Step: 89/303....... Average Loss for Epoch: 0.04149912666068988\n",
      "Epoch 1......Step: 90/303....... Average Loss for Epoch: 0.04132830477837059\n",
      "Epoch 1......Step: 91/303....... Average Loss for Epoch: 0.04109959337082538\n",
      "Epoch 1......Step: 92/303....... Average Loss for Epoch: 0.04099803891680811\n",
      "Epoch 1......Step: 93/303....... Average Loss for Epoch: 0.040777383332130726\n",
      "Epoch 1......Step: 94/303....... Average Loss for Epoch: 0.04056441242945321\n",
      "Epoch 1......Step: 95/303....... Average Loss for Epoch: 0.04036705654702689\n",
      "Epoch 1......Step: 96/303....... Average Loss for Epoch: 0.04018331905050824\n",
      "Epoch 1......Step: 97/303....... Average Loss for Epoch: 0.03995147710378023\n",
      "Epoch 1......Step: 98/303....... Average Loss for Epoch: 0.03972263993429286\n",
      "Epoch 1......Step: 99/303....... Average Loss for Epoch: 0.039502774656872554\n",
      "Epoch 1......Step: 100/303....... Average Loss for Epoch: 0.03933517847210169\n",
      "Epoch 1......Step: 101/303....... Average Loss for Epoch: 0.039221918579227855\n",
      "Epoch 1......Step: 102/303....... Average Loss for Epoch: 0.0390424362073342\n",
      "Epoch 1......Step: 103/303....... Average Loss for Epoch: 0.03888827341708165\n",
      "Epoch 1......Step: 104/303....... Average Loss for Epoch: 0.03884141254596985\n",
      "Epoch 1......Step: 105/303....... Average Loss for Epoch: 0.03866616494598843\n",
      "Epoch 1......Step: 106/303....... Average Loss for Epoch: 0.03849872899294462\n",
      "Epoch 1......Step: 107/303....... Average Loss for Epoch: 0.03838536528017476\n",
      "Epoch 1......Step: 108/303....... Average Loss for Epoch: 0.038184543071245705\n",
      "Epoch 1......Step: 109/303....... Average Loss for Epoch: 0.03819768226474797\n",
      "Epoch 1......Step: 110/303....... Average Loss for Epoch: 0.038087681951847946\n",
      "Epoch 1......Step: 111/303....... Average Loss for Epoch: 0.0380005233560328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1......Step: 112/303....... Average Loss for Epoch: 0.03796737017442605\n",
      "Epoch 1......Step: 113/303....... Average Loss for Epoch: 0.03793691073964655\n",
      "Epoch 1......Step: 114/303....... Average Loss for Epoch: 0.037859176318242885\n",
      "Epoch 1......Step: 115/303....... Average Loss for Epoch: 0.037776681894193524\n",
      "Epoch 1......Step: 116/303....... Average Loss for Epoch: 0.037659983744780565\n",
      "Epoch 1......Step: 117/303....... Average Loss for Epoch: 0.03755390546961218\n",
      "Epoch 1......Step: 118/303....... Average Loss for Epoch: 0.03743825931796583\n",
      "Epoch 1......Step: 119/303....... Average Loss for Epoch: 0.03741287821982087\n",
      "Epoch 1......Step: 120/303....... Average Loss for Epoch: 0.03734133656447133\n",
      "Epoch 1......Step: 121/303....... Average Loss for Epoch: 0.037203159814408006\n",
      "Epoch 1......Step: 122/303....... Average Loss for Epoch: 0.03705973466705592\n",
      "Epoch 1......Step: 123/303....... Average Loss for Epoch: 0.036924633144484305\n",
      "Epoch 1......Step: 124/303....... Average Loss for Epoch: 0.03681149823410857\n",
      "Epoch 1......Step: 125/303....... Average Loss for Epoch: 0.03669202211499214\n",
      "Epoch 1......Step: 126/303....... Average Loss for Epoch: 0.03672191873192787\n",
      "Epoch 1......Step: 127/303....... Average Loss for Epoch: 0.0367597673880303\n",
      "Epoch 1......Step: 128/303....... Average Loss for Epoch: 0.0367017780081369\n",
      "Epoch 1......Step: 129/303....... Average Loss for Epoch: 0.036552082703903664\n",
      "Epoch 1......Step: 130/303....... Average Loss for Epoch: 0.03651773506918779\n",
      "Epoch 1......Step: 131/303....... Average Loss for Epoch: 0.03652958772394038\n",
      "Epoch 1......Step: 132/303....... Average Loss for Epoch: 0.03646978551070347\n",
      "Epoch 1......Step: 133/303....... Average Loss for Epoch: 0.036399215809944874\n",
      "Epoch 1......Step: 134/303....... Average Loss for Epoch: 0.03631452126289481\n",
      "Epoch 1......Step: 135/303....... Average Loss for Epoch: 0.036219084083482074\n",
      "Epoch 1......Step: 136/303....... Average Loss for Epoch: 0.036087634849964696\n",
      "Epoch 1......Step: 137/303....... Average Loss for Epoch: 0.03599254113968706\n",
      "Epoch 1......Step: 138/303....... Average Loss for Epoch: 0.035854833443527634\n",
      "Epoch 1......Step: 139/303....... Average Loss for Epoch: 0.03578881158245553\n",
      "Epoch 1......Step: 140/303....... Average Loss for Epoch: 0.035662432108074425\n",
      "Epoch 1......Step: 141/303....... Average Loss for Epoch: 0.03562390566506284\n",
      "Epoch 1......Step: 142/303....... Average Loss for Epoch: 0.03551521297859054\n",
      "Epoch 1......Step: 143/303....... Average Loss for Epoch: 0.0354623969748512\n",
      "Epoch 1......Step: 144/303....... Average Loss for Epoch: 0.03538690903224051\n",
      "Epoch 1......Step: 145/303....... Average Loss for Epoch: 0.035282423103163985\n",
      "Epoch 1......Step: 146/303....... Average Loss for Epoch: 0.0351723064221952\n",
      "Epoch 1......Step: 147/303....... Average Loss for Epoch: 0.03506823925643551\n",
      "Epoch 1......Step: 148/303....... Average Loss for Epoch: 0.03497654767203573\n",
      "Epoch 1......Step: 149/303....... Average Loss for Epoch: 0.03491626811157537\n",
      "Epoch 1......Step: 150/303....... Average Loss for Epoch: 0.03482852216809988\n",
      "Epoch 1......Step: 151/303....... Average Loss for Epoch: 0.0347910031478926\n",
      "Epoch 1......Step: 152/303....... Average Loss for Epoch: 0.034696953150590785\n",
      "Epoch 1......Step: 153/303....... Average Loss for Epoch: 0.03458159122299525\n",
      "Epoch 1......Step: 154/303....... Average Loss for Epoch: 0.03449600741460726\n",
      "Epoch 1......Step: 155/303....... Average Loss for Epoch: 0.034407516821257526\n",
      "Epoch 1......Step: 156/303....... Average Loss for Epoch: 0.034345251496117085\n",
      "Epoch 1......Step: 157/303....... Average Loss for Epoch: 0.03425953033006495\n",
      "Epoch 1......Step: 158/303....... Average Loss for Epoch: 0.03425110505210071\n",
      "Epoch 1......Step: 159/303....... Average Loss for Epoch: 0.03416828453868815\n",
      "Epoch 1......Step: 160/303....... Average Loss for Epoch: 0.03409177822759375\n",
      "Epoch 1......Step: 161/303....... Average Loss for Epoch: 0.03406954082365362\n",
      "Epoch 1......Step: 162/303....... Average Loss for Epoch: 0.03400913716788277\n",
      "Epoch 1......Step: 163/303....... Average Loss for Epoch: 0.03392780189362406\n",
      "Epoch 1......Step: 164/303....... Average Loss for Epoch: 0.03384894942410472\n",
      "Epoch 1......Step: 165/303....... Average Loss for Epoch: 0.03375086724532373\n",
      "Epoch 1......Step: 166/303....... Average Loss for Epoch: 0.03365797830005008\n",
      "Epoch 1......Step: 167/303....... Average Loss for Epoch: 0.03356881832291266\n",
      "Epoch 1......Step: 168/303....... Average Loss for Epoch: 0.033545743618603976\n",
      "Epoch 1......Step: 169/303....... Average Loss for Epoch: 0.03346464990158758\n",
      "Epoch 1......Step: 170/303....... Average Loss for Epoch: 0.03341281284742496\n",
      "Epoch 1......Step: 171/303....... Average Loss for Epoch: 0.03335014023758166\n",
      "Epoch 1......Step: 172/303....... Average Loss for Epoch: 0.03327731679865094\n",
      "Epoch 1......Step: 173/303....... Average Loss for Epoch: 0.03318920828445109\n",
      "Epoch 1......Step: 174/303....... Average Loss for Epoch: 0.03310538817668098\n",
      "Epoch 1......Step: 175/303....... Average Loss for Epoch: 0.0330289962887764\n",
      "Epoch 1......Step: 176/303....... Average Loss for Epoch: 0.03297967509239574\n",
      "Epoch 1......Step: 177/303....... Average Loss for Epoch: 0.03294004492545869\n",
      "Epoch 1......Step: 178/303....... Average Loss for Epoch: 0.032868548861464085\n",
      "Epoch 1......Step: 179/303....... Average Loss for Epoch: 0.032800912503257146\n",
      "Epoch 1......Step: 180/303....... Average Loss for Epoch: 0.03275936899913682\n",
      "Epoch 1......Step: 181/303....... Average Loss for Epoch: 0.03271556389389446\n",
      "Epoch 1......Step: 182/303....... Average Loss for Epoch: 0.032660826979266415\n",
      "Epoch 1......Step: 183/303....... Average Loss for Epoch: 0.032578411578236384\n",
      "Epoch 1......Step: 184/303....... Average Loss for Epoch: 0.03257749520201722\n",
      "Epoch 1......Step: 185/303....... Average Loss for Epoch: 0.032523423030569744\n",
      "Epoch 1......Step: 186/303....... Average Loss for Epoch: 0.03244867853780267\n",
      "Epoch 1......Step: 187/303....... Average Loss for Epoch: 0.0323852918603841\n",
      "Epoch 1......Step: 188/303....... Average Loss for Epoch: 0.03233617294500483\n",
      "Epoch 1......Step: 189/303....... Average Loss for Epoch: 0.03229331579946336\n",
      "Epoch 1......Step: 190/303....... Average Loss for Epoch: 0.0322288768758115\n",
      "Epoch 1......Step: 191/303....... Average Loss for Epoch: 0.03219613877578555\n",
      "Epoch 1......Step: 192/303....... Average Loss for Epoch: 0.032195124755768724\n",
      "Epoch 1......Step: 193/303....... Average Loss for Epoch: 0.0321297858180623\n",
      "Epoch 1......Step: 194/303....... Average Loss for Epoch: 0.03205645766070823\n",
      "Epoch 1......Step: 195/303....... Average Loss for Epoch: 0.03202871851240977\n",
      "Epoch 1......Step: 196/303....... Average Loss for Epoch: 0.0320232639716444\n",
      "Epoch 1......Step: 197/303....... Average Loss for Epoch: 0.03204058111659464\n",
      "Epoch 1......Step: 198/303....... Average Loss for Epoch: 0.032017261664749996\n",
      "Epoch 1......Step: 199/303....... Average Loss for Epoch: 0.03194489870126822\n",
      "Epoch 1......Step: 200/303....... Average Loss for Epoch: 0.03189438813365996\n",
      "Epoch 1......Step: 201/303....... Average Loss for Epoch: 0.031877192636182654\n",
      "Epoch 1......Step: 202/303....... Average Loss for Epoch: 0.03183217949722663\n",
      "Epoch 1......Step: 203/303....... Average Loss for Epoch: 0.03181129447161564\n",
      "Epoch 1......Step: 204/303....... Average Loss for Epoch: 0.03176083657707946\n",
      "Epoch 1......Step: 205/303....... Average Loss for Epoch: 0.03174402650173117\n",
      "Epoch 1......Step: 206/303....... Average Loss for Epoch: 0.031735844919186774\n",
      "Epoch 1......Step: 207/303....... Average Loss for Epoch: 0.03167489979074197\n",
      "Epoch 1......Step: 208/303....... Average Loss for Epoch: 0.03164027104727351\n",
      "Epoch 1......Step: 209/303....... Average Loss for Epoch: 0.03159131300435112\n",
      "Epoch 1......Step: 210/303....... Average Loss for Epoch: 0.031562987706136136\n",
      "Epoch 1......Step: 211/303....... Average Loss for Epoch: 0.03158179337767911\n",
      "Epoch 1......Step: 212/303....... Average Loss for Epoch: 0.031557520218896414\n",
      "Epoch 1......Step: 213/303....... Average Loss for Epoch: 0.03151055524221888\n",
      "Epoch 1......Step: 214/303....... Average Loss for Epoch: 0.031457604962253125\n",
      "Epoch 1......Step: 215/303....... Average Loss for Epoch: 0.03144184230545233\n",
      "Epoch 1......Step: 216/303....... Average Loss for Epoch: 0.031394312580771466\n",
      "Epoch 1......Step: 217/303....... Average Loss for Epoch: 0.0313647402029845\n",
      "Epoch 1......Step: 218/303....... Average Loss for Epoch: 0.03136916862749452\n",
      "Epoch 1......Step: 219/303....... Average Loss for Epoch: 0.0313133905008095\n",
      "Epoch 1......Step: 220/303....... Average Loss for Epoch: 0.031292687305672605\n",
      "Epoch 1......Step: 221/303....... Average Loss for Epoch: 0.031246822504368842\n",
      "Epoch 1......Step: 222/303....... Average Loss for Epoch: 0.03130007567941337\n",
      "Epoch 1......Step: 223/303....... Average Loss for Epoch: 0.031229256134430124\n",
      "Epoch 1......Step: 224/303....... Average Loss for Epoch: 0.031165585574594194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1......Step: 225/303....... Average Loss for Epoch: 0.031116430962251293\n",
      "Epoch 1......Step: 226/303....... Average Loss for Epoch: 0.031080866055315837\n",
      "Epoch 1......Step: 227/303....... Average Loss for Epoch: 0.031041914330475394\n",
      "Epoch 1......Step: 228/303....... Average Loss for Epoch: 0.031034384572290276\n",
      "Epoch 1......Step: 229/303....... Average Loss for Epoch: 0.031001056381806277\n",
      "Epoch 1......Step: 230/303....... Average Loss for Epoch: 0.03100292808578714\n",
      "Epoch 1......Step: 231/303....... Average Loss for Epoch: 0.030946125155435752\n",
      "Epoch 1......Step: 232/303....... Average Loss for Epoch: 0.030918106994720113\n",
      "Epoch 1......Step: 233/303....... Average Loss for Epoch: 0.030867200756501487\n",
      "Epoch 1......Step: 234/303....... Average Loss for Epoch: 0.03088160369815862\n",
      "Epoch 1......Step: 235/303....... Average Loss for Epoch: 0.0308400085829991\n",
      "Epoch 1......Step: 236/303....... Average Loss for Epoch: 0.030843486303021594\n",
      "Epoch 1......Step: 237/303....... Average Loss for Epoch: 0.03083843152718579\n",
      "Epoch 1......Step: 238/303....... Average Loss for Epoch: 0.03082183171298449\n",
      "Epoch 1......Step: 239/303....... Average Loss for Epoch: 0.030850833246803933\n",
      "Epoch 1......Step: 240/303....... Average Loss for Epoch: 0.030831619650901605\n",
      "Epoch 1......Step: 241/303....... Average Loss for Epoch: 0.03080116772320879\n",
      "Epoch 1......Step: 242/303....... Average Loss for Epoch: 0.03080585059009431\n",
      "Epoch 1......Step: 243/303....... Average Loss for Epoch: 0.030793604027065966\n",
      "Epoch 1......Step: 244/303....... Average Loss for Epoch: 0.030767041880667943\n",
      "Epoch 1......Step: 245/303....... Average Loss for Epoch: 0.030743345857730933\n",
      "Epoch 1......Step: 246/303....... Average Loss for Epoch: 0.03070770739009831\n",
      "Epoch 1......Step: 247/303....... Average Loss for Epoch: 0.03065886586406815\n",
      "Epoch 1......Step: 248/303....... Average Loss for Epoch: 0.030607116051889476\n",
      "Epoch 1......Step: 249/303....... Average Loss for Epoch: 0.03057101194982428\n",
      "Epoch 1......Step: 250/303....... Average Loss for Epoch: 0.030513043936342\n",
      "Epoch 1......Step: 251/303....... Average Loss for Epoch: 0.030499763723715132\n",
      "Epoch 1......Step: 252/303....... Average Loss for Epoch: 0.030454264129585927\n",
      "Epoch 1......Step: 253/303....... Average Loss for Epoch: 0.030454901967330177\n",
      "Epoch 1......Step: 254/303....... Average Loss for Epoch: 0.030433325621292112\n",
      "Epoch 1......Step: 255/303....... Average Loss for Epoch: 0.030401120294688964\n",
      "Epoch 1......Step: 256/303....... Average Loss for Epoch: 0.030400732397538377\n",
      "Epoch 1......Step: 257/303....... Average Loss for Epoch: 0.03041999144588347\n",
      "Epoch 1......Step: 258/303....... Average Loss for Epoch: 0.030369721176174952\n",
      "Epoch 1......Step: 259/303....... Average Loss for Epoch: 0.030327097181964333\n",
      "Epoch 1......Step: 260/303....... Average Loss for Epoch: 0.030278977698001724\n",
      "Epoch 1......Step: 261/303....... Average Loss for Epoch: 0.03023369500763229\n",
      "Epoch 1......Step: 262/303....... Average Loss for Epoch: 0.03025999061322508\n",
      "Epoch 1......Step: 263/303....... Average Loss for Epoch: 0.030208908924443414\n",
      "Epoch 1......Step: 264/303....... Average Loss for Epoch: 0.03016383563182458\n",
      "Epoch 1......Step: 265/303....... Average Loss for Epoch: 0.03014888630231034\n",
      "Epoch 1......Step: 266/303....... Average Loss for Epoch: 0.03014822780063614\n",
      "Epoch 1......Step: 267/303....... Average Loss for Epoch: 0.030150287268397292\n",
      "Epoch 1......Step: 268/303....... Average Loss for Epoch: 0.030121383948752016\n",
      "Epoch 1......Step: 269/303....... Average Loss for Epoch: 0.030089693272063945\n",
      "Epoch 1......Step: 270/303....... Average Loss for Epoch: 0.0300623362366524\n",
      "Epoch 1......Step: 271/303....... Average Loss for Epoch: 0.030055060243832008\n",
      "Epoch 1......Step: 272/303....... Average Loss for Epoch: 0.030035880027914092\n",
      "Epoch 1......Step: 273/303....... Average Loss for Epoch: 0.03000220072015629\n",
      "Epoch 1......Step: 274/303....... Average Loss for Epoch: 0.029950767289847136\n",
      "Epoch 1......Step: 275/303....... Average Loss for Epoch: 0.029919950924813747\n",
      "Epoch 1......Step: 276/303....... Average Loss for Epoch: 0.029896341881755252\n",
      "Epoch 1......Step: 277/303....... Average Loss for Epoch: 0.02988754069291405\n",
      "Epoch 1......Step: 278/303....... Average Loss for Epoch: 0.02985415407020947\n",
      "Epoch 1......Step: 279/303....... Average Loss for Epoch: 0.02984375320708773\n",
      "Epoch 1......Step: 280/303....... Average Loss for Epoch: 0.02978757650125772\n",
      "Epoch 1......Step: 281/303....... Average Loss for Epoch: 0.029747611445331702\n",
      "Epoch 1......Step: 282/303....... Average Loss for Epoch: 0.029805055122611158\n",
      "Epoch 1......Step: 283/303....... Average Loss for Epoch: 0.029766199830590416\n",
      "Epoch 1......Step: 284/303....... Average Loss for Epoch: 0.029751279276572694\n",
      "Epoch 1......Step: 285/303....... Average Loss for Epoch: 0.029735897752668775\n",
      "Epoch 1......Step: 286/303....... Average Loss for Epoch: 0.029691065104769453\n",
      "Epoch 1......Step: 287/303....... Average Loss for Epoch: 0.02965087735967042\n",
      "Epoch 1......Step: 288/303....... Average Loss for Epoch: 0.02961127485549595\n",
      "Epoch 1......Step: 289/303....... Average Loss for Epoch: 0.02958842550298025\n",
      "Epoch 1......Step: 290/303....... Average Loss for Epoch: 0.029541701576190776\n",
      "Epoch 1......Step: 291/303....... Average Loss for Epoch: 0.02953219263657569\n",
      "Epoch 1......Step: 292/303....... Average Loss for Epoch: 0.029497753953152937\n",
      "Epoch 1......Step: 293/303....... Average Loss for Epoch: 0.02948017135173489\n",
      "Epoch 1......Step: 294/303....... Average Loss for Epoch: 0.02943784040620639\n",
      "Epoch 1......Step: 295/303....... Average Loss for Epoch: 0.029401825523098647\n",
      "Epoch 1......Step: 296/303....... Average Loss for Epoch: 0.02937678771559149\n",
      "Epoch 1......Step: 297/303....... Average Loss for Epoch: 0.029331249548655368\n",
      "Epoch 1......Step: 298/303....... Average Loss for Epoch: 0.02930432784639849\n",
      "Epoch 1......Step: 299/303....... Average Loss for Epoch: 0.029258781256010898\n",
      "Epoch 1......Step: 300/303....... Average Loss for Epoch: 0.029248729965959987\n",
      "Epoch 1......Step: 301/303....... Average Loss for Epoch: 0.02921099964826309\n",
      "Epoch 1......Step: 302/303....... Average Loss for Epoch: 0.02917323119333939\n",
      "Epoch 1......Step: 303/303....... Average Loss for Epoch: 0.029134415836986338\n",
      "Epoch 1/5 Done, Total Loss: 0.029134415836986338\n",
      "Time Elapsed for Epoch: 6.234309700000011 seconds\n",
      "Epoch 2......Step: 1/303....... Average Loss for Epoch: 0.02199065126478672\n",
      "Epoch 2......Step: 2/303....... Average Loss for Epoch: 0.019252534955739975\n",
      "Epoch 2......Step: 3/303....... Average Loss for Epoch: 0.017037305670479935\n",
      "Epoch 2......Step: 4/303....... Average Loss for Epoch: 0.017431417712941766\n",
      "Epoch 2......Step: 5/303....... Average Loss for Epoch: 0.01902041081339121\n",
      "Epoch 2......Step: 6/303....... Average Loss for Epoch: 0.018799887814869482\n",
      "Epoch 2......Step: 7/303....... Average Loss for Epoch: 0.018751080946198533\n",
      "Epoch 2......Step: 8/303....... Average Loss for Epoch: 0.02040269214194268\n",
      "Epoch 2......Step: 9/303....... Average Loss for Epoch: 0.020094275371068053\n",
      "Epoch 2......Step: 10/303....... Average Loss for Epoch: 0.0197892795316875\n",
      "Epoch 2......Step: 11/303....... Average Loss for Epoch: 0.020319944332269104\n",
      "Epoch 2......Step: 12/303....... Average Loss for Epoch: 0.020027630884821217\n",
      "Epoch 2......Step: 13/303....... Average Loss for Epoch: 0.020349362220328588\n",
      "Epoch 2......Step: 14/303....... Average Loss for Epoch: 0.020481803254889592\n",
      "Epoch 2......Step: 15/303....... Average Loss for Epoch: 0.020570882471899192\n",
      "Epoch 2......Step: 16/303....... Average Loss for Epoch: 0.02041057328460738\n",
      "Epoch 2......Step: 17/303....... Average Loss for Epoch: 0.02024581310722758\n",
      "Epoch 2......Step: 18/303....... Average Loss for Epoch: 0.02022313652560115\n",
      "Epoch 2......Step: 19/303....... Average Loss for Epoch: 0.020748909219707314\n",
      "Epoch 2......Step: 20/303....... Average Loss for Epoch: 0.021268788399174808\n",
      "Epoch 2......Step: 21/303....... Average Loss for Epoch: 0.021354798626686846\n",
      "Epoch 2......Step: 22/303....... Average Loss for Epoch: 0.02122240018268878\n",
      "Epoch 2......Step: 23/303....... Average Loss for Epoch: 0.021613532555815967\n",
      "Epoch 2......Step: 24/303....... Average Loss for Epoch: 0.021360813562447827\n",
      "Epoch 2......Step: 25/303....... Average Loss for Epoch: 0.021184049397706985\n",
      "Epoch 2......Step: 26/303....... Average Loss for Epoch: 0.021038564208608408\n",
      "Epoch 2......Step: 27/303....... Average Loss for Epoch: 0.021024336141568643\n",
      "Epoch 2......Step: 28/303....... Average Loss for Epoch: 0.021275998758418218\n",
      "Epoch 2......Step: 29/303....... Average Loss for Epoch: 0.021184270751887356\n",
      "Epoch 2......Step: 30/303....... Average Loss for Epoch: 0.021083166201909382\n",
      "Epoch 2......Step: 31/303....... Average Loss for Epoch: 0.021273111864443747\n",
      "Epoch 2......Step: 32/303....... Average Loss for Epoch: 0.021475992281921208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2......Step: 33/303....... Average Loss for Epoch: 0.021691700330737865\n",
      "Epoch 2......Step: 34/303....... Average Loss for Epoch: 0.021985942881335232\n",
      "Epoch 2......Step: 35/303....... Average Loss for Epoch: 0.021799215807446412\n",
      "Epoch 2......Step: 36/303....... Average Loss for Epoch: 0.02168430773437851\n",
      "Epoch 2......Step: 37/303....... Average Loss for Epoch: 0.021670731766199745\n",
      "Epoch 2......Step: 38/303....... Average Loss for Epoch: 0.021540042233506317\n",
      "Epoch 2......Step: 39/303....... Average Loss for Epoch: 0.021602720953524113\n",
      "Epoch 2......Step: 40/303....... Average Loss for Epoch: 0.021467681298963724\n",
      "Epoch 2......Step: 41/303....... Average Loss for Epoch: 0.02135517364140691\n",
      "Epoch 2......Step: 42/303....... Average Loss for Epoch: 0.021271865393611648\n",
      "Epoch 2......Step: 43/303....... Average Loss for Epoch: 0.021105221104483273\n",
      "Epoch 2......Step: 44/303....... Average Loss for Epoch: 0.02119040120901032\n",
      "Epoch 2......Step: 45/303....... Average Loss for Epoch: 0.021177705998222034\n",
      "Epoch 2......Step: 46/303....... Average Loss for Epoch: 0.02110340373347635\n",
      "Epoch 2......Step: 47/303....... Average Loss for Epoch: 0.021097310759285663\n",
      "Epoch 2......Step: 48/303....... Average Loss for Epoch: 0.021141681509713333\n",
      "Epoch 2......Step: 49/303....... Average Loss for Epoch: 0.02101677637167123\n",
      "Epoch 2......Step: 50/303....... Average Loss for Epoch: 0.02091463305056095\n",
      "Epoch 2......Step: 51/303....... Average Loss for Epoch: 0.02082875221237248\n",
      "Epoch 2......Step: 52/303....... Average Loss for Epoch: 0.020831585467721406\n",
      "Epoch 2......Step: 53/303....... Average Loss for Epoch: 0.020864797791220108\n",
      "Epoch 2......Step: 54/303....... Average Loss for Epoch: 0.020749115091921004\n",
      "Epoch 2......Step: 55/303....... Average Loss for Epoch: 0.02065536781129512\n",
      "Epoch 2......Step: 56/303....... Average Loss for Epoch: 0.020786185798767422\n",
      "Epoch 2......Step: 57/303....... Average Loss for Epoch: 0.0207404026827007\n",
      "Epoch 2......Step: 58/303....... Average Loss for Epoch: 0.02096680518047049\n",
      "Epoch 2......Step: 59/303....... Average Loss for Epoch: 0.021022276589804788\n",
      "Epoch 2......Step: 60/303....... Average Loss for Epoch: 0.02122020471530656\n",
      "Epoch 2......Step: 61/303....... Average Loss for Epoch: 0.021229791424435672\n",
      "Epoch 2......Step: 62/303....... Average Loss for Epoch: 0.021440993528813124\n",
      "Epoch 2......Step: 63/303....... Average Loss for Epoch: 0.021432674696875945\n",
      "Epoch 2......Step: 64/303....... Average Loss for Epoch: 0.02139415506098885\n",
      "Epoch 2......Step: 65/303....... Average Loss for Epoch: 0.02131099054733148\n",
      "Epoch 2......Step: 66/303....... Average Loss for Epoch: 0.021281590473584154\n",
      "Epoch 2......Step: 67/303....... Average Loss for Epoch: 0.02134723743118012\n",
      "Epoch 2......Step: 68/303....... Average Loss for Epoch: 0.02135807159356773\n",
      "Epoch 2......Step: 69/303....... Average Loss for Epoch: 0.021391193814359714\n",
      "Epoch 2......Step: 70/303....... Average Loss for Epoch: 0.021358486650777716\n",
      "Epoch 2......Step: 71/303....... Average Loss for Epoch: 0.021292436697428495\n",
      "Epoch 2......Step: 72/303....... Average Loss for Epoch: 0.02122938761021942\n",
      "Epoch 2......Step: 73/303....... Average Loss for Epoch: 0.021266699462414603\n",
      "Epoch 2......Step: 74/303....... Average Loss for Epoch: 0.02140284310774626\n",
      "Epoch 2......Step: 75/303....... Average Loss for Epoch: 0.021490956954658032\n",
      "Epoch 2......Step: 76/303....... Average Loss for Epoch: 0.021384888157052428\n",
      "Epoch 2......Step: 77/303....... Average Loss for Epoch: 0.021297422370740345\n",
      "Epoch 2......Step: 78/303....... Average Loss for Epoch: 0.02124454088222522\n",
      "Epoch 2......Step: 79/303....... Average Loss for Epoch: 0.02123832829956767\n",
      "Epoch 2......Step: 80/303....... Average Loss for Epoch: 0.02126082458999008\n",
      "Epoch 2......Step: 81/303....... Average Loss for Epoch: 0.02121977317204446\n",
      "Epoch 2......Step: 82/303....... Average Loss for Epoch: 0.021216749645224433\n",
      "Epoch 2......Step: 83/303....... Average Loss for Epoch: 0.02120808381811682\n",
      "Epoch 2......Step: 84/303....... Average Loss for Epoch: 0.02128413539113743\n",
      "Epoch 2......Step: 85/303....... Average Loss for Epoch: 0.02124837507658145\n",
      "Epoch 2......Step: 86/303....... Average Loss for Epoch: 0.021178751469178254\n",
      "Epoch 2......Step: 87/303....... Average Loss for Epoch: 0.021130749613217925\n",
      "Epoch 2......Step: 88/303....... Average Loss for Epoch: 0.021141548577526755\n",
      "Epoch 2......Step: 89/303....... Average Loss for Epoch: 0.021204019258363863\n",
      "Epoch 2......Step: 90/303....... Average Loss for Epoch: 0.021168790095382268\n",
      "Epoch 2......Step: 91/303....... Average Loss for Epoch: 0.021075195499828885\n",
      "Epoch 2......Step: 92/303....... Average Loss for Epoch: 0.021067747481815193\n",
      "Epoch 2......Step: 93/303....... Average Loss for Epoch: 0.020979589392100612\n",
      "Epoch 2......Step: 94/303....... Average Loss for Epoch: 0.021035122407719174\n",
      "Epoch 2......Step: 95/303....... Average Loss for Epoch: 0.0211311487579032\n",
      "Epoch 2......Step: 96/303....... Average Loss for Epoch: 0.021133358648512512\n",
      "Epoch 2......Step: 97/303....... Average Loss for Epoch: 0.02105360755639285\n",
      "Epoch 2......Step: 98/303....... Average Loss for Epoch: 0.020989858165231287\n",
      "Epoch 2......Step: 99/303....... Average Loss for Epoch: 0.020978123608111132\n",
      "Epoch 2......Step: 100/303....... Average Loss for Epoch: 0.021011002529412508\n",
      "Epoch 2......Step: 101/303....... Average Loss for Epoch: 0.020958497862119486\n",
      "Epoch 2......Step: 102/303....... Average Loss for Epoch: 0.021004813595437537\n",
      "Epoch 2......Step: 103/303....... Average Loss for Epoch: 0.02092139120891835\n",
      "Epoch 2......Step: 104/303....... Average Loss for Epoch: 0.02102069861183946\n",
      "Epoch 2......Step: 105/303....... Average Loss for Epoch: 0.021012948063157854\n",
      "Epoch 2......Step: 106/303....... Average Loss for Epoch: 0.020969750096072565\n",
      "Epoch 2......Step: 107/303....... Average Loss for Epoch: 0.020962025239088824\n",
      "Epoch 2......Step: 108/303....... Average Loss for Epoch: 0.02100063330942282\n",
      "Epoch 2......Step: 109/303....... Average Loss for Epoch: 0.020987900401200722\n",
      "Epoch 2......Step: 110/303....... Average Loss for Epoch: 0.020948401635343377\n",
      "Epoch 2......Step: 111/303....... Average Loss for Epoch: 0.02088765259664338\n",
      "Epoch 2......Step: 112/303....... Average Loss for Epoch: 0.020876963157206774\n",
      "Epoch 2......Step: 113/303....... Average Loss for Epoch: 0.020946925802700286\n",
      "Epoch 2......Step: 114/303....... Average Loss for Epoch: 0.02101471023470686\n",
      "Epoch 2......Step: 115/303....... Average Loss for Epoch: 0.0209814857205619\n",
      "Epoch 2......Step: 116/303....... Average Loss for Epoch: 0.020948699193781818\n",
      "Epoch 2......Step: 117/303....... Average Loss for Epoch: 0.02092698937616287\n",
      "Epoch 2......Step: 118/303....... Average Loss for Epoch: 0.020952786808296785\n",
      "Epoch 2......Step: 119/303....... Average Loss for Epoch: 0.02098947124821799\n",
      "Epoch 2......Step: 120/303....... Average Loss for Epoch: 0.020983252814039587\n",
      "Epoch 2......Step: 121/303....... Average Loss for Epoch: 0.020953520716837613\n",
      "Epoch 2......Step: 122/303....... Average Loss for Epoch: 0.020949809995220333\n",
      "Epoch 2......Step: 123/303....... Average Loss for Epoch: 0.020914109968921034\n",
      "Epoch 2......Step: 124/303....... Average Loss for Epoch: 0.020872341228588935\n",
      "Epoch 2......Step: 125/303....... Average Loss for Epoch: 0.020855847403407096\n",
      "Epoch 2......Step: 126/303....... Average Loss for Epoch: 0.02084749849838397\n",
      "Epoch 2......Step: 127/303....... Average Loss for Epoch: 0.020831153735401124\n",
      "Epoch 2......Step: 128/303....... Average Loss for Epoch: 0.020793927367776632\n",
      "Epoch 2......Step: 129/303....... Average Loss for Epoch: 0.020808108150959015\n",
      "Epoch 2......Step: 130/303....... Average Loss for Epoch: 0.02083591138227628\n",
      "Epoch 2......Step: 131/303....... Average Loss for Epoch: 0.020822617971828876\n",
      "Epoch 2......Step: 132/303....... Average Loss for Epoch: 0.020854329676226233\n",
      "Epoch 2......Step: 133/303....... Average Loss for Epoch: 0.020850414200160736\n",
      "Epoch 2......Step: 134/303....... Average Loss for Epoch: 0.02094642676190654\n",
      "Epoch 2......Step: 135/303....... Average Loss for Epoch: 0.021015225364654153\n",
      "Epoch 2......Step: 136/303....... Average Loss for Epoch: 0.020962083431453827\n",
      "Epoch 2......Step: 137/303....... Average Loss for Epoch: 0.021001269639789186\n",
      "Epoch 2......Step: 138/303....... Average Loss for Epoch: 0.02102094694999033\n",
      "Epoch 2......Step: 139/303....... Average Loss for Epoch: 0.02101292020956175\n",
      "Epoch 2......Step: 140/303....... Average Loss for Epoch: 0.02100403074042073\n",
      "Epoch 2......Step: 141/303....... Average Loss for Epoch: 0.020968887829452963\n",
      "Epoch 2......Step: 142/303....... Average Loss for Epoch: 0.020967841141817854\n",
      "Epoch 2......Step: 143/303....... Average Loss for Epoch: 0.02092934453899002\n",
      "Epoch 2......Step: 144/303....... Average Loss for Epoch: 0.020908015734878264\n",
      "Epoch 2......Step: 145/303....... Average Loss for Epoch: 0.02085787268548176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2......Step: 146/303....... Average Loss for Epoch: 0.020853873406064836\n",
      "Epoch 2......Step: 147/303....... Average Loss for Epoch: 0.020825257170058432\n",
      "Epoch 2......Step: 148/303....... Average Loss for Epoch: 0.020844314423565928\n",
      "Epoch 2......Step: 149/303....... Average Loss for Epoch: 0.020905930161376127\n",
      "Epoch 2......Step: 150/303....... Average Loss for Epoch: 0.020910152072707813\n",
      "Epoch 2......Step: 151/303....... Average Loss for Epoch: 0.020891852948247202\n",
      "Epoch 2......Step: 152/303....... Average Loss for Epoch: 0.020893864787036653\n",
      "Epoch 2......Step: 153/303....... Average Loss for Epoch: 0.020860510750436316\n",
      "Epoch 2......Step: 154/303....... Average Loss for Epoch: 0.020906536005340613\n",
      "Epoch 2......Step: 155/303....... Average Loss for Epoch: 0.02089363594930018\n",
      "Epoch 2......Step: 156/303....... Average Loss for Epoch: 0.02090646303855838\n",
      "Epoch 2......Step: 157/303....... Average Loss for Epoch: 0.020958482305620127\n",
      "Epoch 2......Step: 158/303....... Average Loss for Epoch: 0.020992760086738612\n",
      "Epoch 2......Step: 159/303....... Average Loss for Epoch: 0.0210139089550987\n",
      "Epoch 2......Step: 160/303....... Average Loss for Epoch: 0.020999825350008904\n",
      "Epoch 2......Step: 161/303....... Average Loss for Epoch: 0.02105229619821036\n",
      "Epoch 2......Step: 162/303....... Average Loss for Epoch: 0.021094506215534093\n",
      "Epoch 2......Step: 163/303....... Average Loss for Epoch: 0.02105416980303504\n",
      "Epoch 2......Step: 164/303....... Average Loss for Epoch: 0.021022789452860995\n",
      "Epoch 2......Step: 165/303....... Average Loss for Epoch: 0.021054105539665077\n",
      "Epoch 2......Step: 166/303....... Average Loss for Epoch: 0.021049211474965853\n",
      "Epoch 2......Step: 167/303....... Average Loss for Epoch: 0.021182538060371032\n",
      "Epoch 2......Step: 168/303....... Average Loss for Epoch: 0.021208498638034576\n",
      "Epoch 2......Step: 169/303....... Average Loss for Epoch: 0.021209136819874748\n",
      "Epoch 2......Step: 170/303....... Average Loss for Epoch: 0.021200295296661995\n",
      "Epoch 2......Step: 171/303....... Average Loss for Epoch: 0.02118480608564371\n",
      "Epoch 2......Step: 172/303....... Average Loss for Epoch: 0.02116608197345983\n",
      "Epoch 2......Step: 173/303....... Average Loss for Epoch: 0.021180028644944893\n",
      "Epoch 2......Step: 174/303....... Average Loss for Epoch: 0.02119424417829034\n",
      "Epoch 2......Step: 175/303....... Average Loss for Epoch: 0.021224824339151384\n",
      "Epoch 2......Step: 176/303....... Average Loss for Epoch: 0.0212108132281256\n",
      "Epoch 2......Step: 177/303....... Average Loss for Epoch: 0.02125717830018135\n",
      "Epoch 2......Step: 178/303....... Average Loss for Epoch: 0.021283688264365278\n",
      "Epoch 2......Step: 179/303....... Average Loss for Epoch: 0.02126111603191112\n",
      "Epoch 2......Step: 180/303....... Average Loss for Epoch: 0.021254688408225773\n",
      "Epoch 2......Step: 181/303....... Average Loss for Epoch: 0.021278543572728806\n",
      "Epoch 2......Step: 182/303....... Average Loss for Epoch: 0.02126051144735826\n",
      "Epoch 2......Step: 183/303....... Average Loss for Epoch: 0.021233438010342786\n",
      "Epoch 2......Step: 184/303....... Average Loss for Epoch: 0.021280609304085374\n",
      "Epoch 2......Step: 185/303....... Average Loss for Epoch: 0.021272960909315058\n",
      "Epoch 2......Step: 186/303....... Average Loss for Epoch: 0.021229768201949134\n",
      "Epoch 2......Step: 187/303....... Average Loss for Epoch: 0.02124274897583347\n",
      "Epoch 2......Step: 188/303....... Average Loss for Epoch: 0.021205710207528258\n",
      "Epoch 2......Step: 189/303....... Average Loss for Epoch: 0.02117798253696747\n",
      "Epoch 2......Step: 190/303....... Average Loss for Epoch: 0.021198960619145318\n",
      "Epoch 2......Step: 191/303....... Average Loss for Epoch: 0.0211988122772952\n",
      "Epoch 2......Step: 192/303....... Average Loss for Epoch: 0.021171112302302692\n",
      "Epoch 2......Step: 193/303....... Average Loss for Epoch: 0.021147064555803112\n",
      "Epoch 2......Step: 194/303....... Average Loss for Epoch: 0.021121346305326087\n",
      "Epoch 2......Step: 195/303....... Average Loss for Epoch: 0.021086411966154208\n",
      "Epoch 2......Step: 196/303....... Average Loss for Epoch: 0.02108330959549212\n",
      "Epoch 2......Step: 197/303....... Average Loss for Epoch: 0.021065790125825986\n",
      "Epoch 2......Step: 198/303....... Average Loss for Epoch: 0.021084668561628068\n",
      "Epoch 2......Step: 199/303....... Average Loss for Epoch: 0.021054336562490644\n",
      "Epoch 2......Step: 200/303....... Average Loss for Epoch: 0.021019490049220622\n",
      "Epoch 2......Step: 201/303....... Average Loss for Epoch: 0.021002960150403465\n",
      "Epoch 2......Step: 202/303....... Average Loss for Epoch: 0.020983778949974493\n",
      "Epoch 2......Step: 203/303....... Average Loss for Epoch: 0.020935404775067796\n",
      "Epoch 2......Step: 204/303....... Average Loss for Epoch: 0.020910166528112457\n",
      "Epoch 2......Step: 205/303....... Average Loss for Epoch: 0.020923423217382373\n",
      "Epoch 2......Step: 206/303....... Average Loss for Epoch: 0.020920649817376173\n",
      "Epoch 2......Step: 207/303....... Average Loss for Epoch: 0.020899020283874394\n",
      "Epoch 2......Step: 208/303....... Average Loss for Epoch: 0.020879703954471134\n",
      "Epoch 2......Step: 209/303....... Average Loss for Epoch: 0.020915310219393676\n",
      "Epoch 2......Step: 210/303....... Average Loss for Epoch: 0.020899709708811273\n",
      "Epoch 2......Step: 211/303....... Average Loss for Epoch: 0.020866461910342717\n",
      "Epoch 2......Step: 212/303....... Average Loss for Epoch: 0.020844051667120098\n",
      "Epoch 2......Step: 213/303....... Average Loss for Epoch: 0.02082858191794353\n",
      "Epoch 2......Step: 214/303....... Average Loss for Epoch: 0.020816416279456327\n",
      "Epoch 2......Step: 215/303....... Average Loss for Epoch: 0.020803406280140543\n",
      "Epoch 2......Step: 216/303....... Average Loss for Epoch: 0.020871213447578526\n",
      "Epoch 2......Step: 217/303....... Average Loss for Epoch: 0.020874617192121695\n",
      "Epoch 2......Step: 218/303....... Average Loss for Epoch: 0.020849724761594874\n",
      "Epoch 2......Step: 219/303....... Average Loss for Epoch: 0.020822372427848103\n",
      "Epoch 2......Step: 220/303....... Average Loss for Epoch: 0.020883700052614917\n",
      "Epoch 2......Step: 221/303....... Average Loss for Epoch: 0.020890699653165642\n",
      "Epoch 2......Step: 222/303....... Average Loss for Epoch: 0.020880757390546636\n",
      "Epoch 2......Step: 223/303....... Average Loss for Epoch: 0.02089066078026065\n",
      "Epoch 2......Step: 224/303....... Average Loss for Epoch: 0.020865099587743834\n",
      "Epoch 2......Step: 225/303....... Average Loss for Epoch: 0.020843763276934622\n",
      "Epoch 2......Step: 226/303....... Average Loss for Epoch: 0.02080993006928964\n",
      "Epoch 2......Step: 227/303....... Average Loss for Epoch: 0.02080415181470994\n",
      "Epoch 2......Step: 228/303....... Average Loss for Epoch: 0.020814125667113745\n",
      "Epoch 2......Step: 229/303....... Average Loss for Epoch: 0.020780429544992842\n",
      "Epoch 2......Step: 230/303....... Average Loss for Epoch: 0.020769855290975258\n",
      "Epoch 2......Step: 231/303....... Average Loss for Epoch: 0.020746120001749815\n",
      "Epoch 2......Step: 232/303....... Average Loss for Epoch: 0.020725958615316654\n",
      "Epoch 2......Step: 233/303....... Average Loss for Epoch: 0.02071511283788686\n",
      "Epoch 2......Step: 234/303....... Average Loss for Epoch: 0.020718027865434557\n",
      "Epoch 2......Step: 235/303....... Average Loss for Epoch: 0.020710121455820317\n",
      "Epoch 2......Step: 236/303....... Average Loss for Epoch: 0.02067950175743613\n",
      "Epoch 2......Step: 237/303....... Average Loss for Epoch: 0.020666004425771378\n",
      "Epoch 2......Step: 238/303....... Average Loss for Epoch: 0.02062097705603272\n",
      "Epoch 2......Step: 239/303....... Average Loss for Epoch: 0.020643718419869325\n",
      "Epoch 2......Step: 240/303....... Average Loss for Epoch: 0.020621112264537562\n",
      "Epoch 2......Step: 241/303....... Average Loss for Epoch: 0.020639029833878473\n",
      "Epoch 2......Step: 242/303....... Average Loss for Epoch: 0.02065876033541954\n",
      "Epoch 2......Step: 243/303....... Average Loss for Epoch: 0.020645057173146877\n",
      "Epoch 2......Step: 244/303....... Average Loss for Epoch: 0.020689182819371096\n",
      "Epoch 2......Step: 245/303....... Average Loss for Epoch: 0.020716432288137018\n",
      "Epoch 2......Step: 246/303....... Average Loss for Epoch: 0.02073883317880393\n",
      "Epoch 2......Step: 247/303....... Average Loss for Epoch: 0.020769214977770442\n",
      "Epoch 2......Step: 248/303....... Average Loss for Epoch: 0.02076366629766961\n",
      "Epoch 2......Step: 249/303....... Average Loss for Epoch: 0.02077043201983813\n",
      "Epoch 2......Step: 250/303....... Average Loss for Epoch: 0.02081497259810567\n",
      "Epoch 2......Step: 251/303....... Average Loss for Epoch: 0.020815447881906868\n",
      "Epoch 2......Step: 252/303....... Average Loss for Epoch: 0.020808235331926318\n",
      "Epoch 2......Step: 253/303....... Average Loss for Epoch: 0.020787872863646317\n",
      "Epoch 2......Step: 254/303....... Average Loss for Epoch: 0.020778531578963432\n",
      "Epoch 2......Step: 255/303....... Average Loss for Epoch: 0.02075596073386716\n",
      "Epoch 2......Step: 256/303....... Average Loss for Epoch: 0.020826929772738367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2......Step: 257/303....... Average Loss for Epoch: 0.020827429412577866\n",
      "Epoch 2......Step: 258/303....... Average Loss for Epoch: 0.02081209068121605\n",
      "Epoch 2......Step: 259/303....... Average Loss for Epoch: 0.020805822879536272\n",
      "Epoch 2......Step: 260/303....... Average Loss for Epoch: 0.020791851148868983\n",
      "Epoch 2......Step: 261/303....... Average Loss for Epoch: 0.02077550831397146\n",
      "Epoch 2......Step: 262/303....... Average Loss for Epoch: 0.02075210065076142\n",
      "Epoch 2......Step: 263/303....... Average Loss for Epoch: 0.02073359565221538\n",
      "Epoch 2......Step: 264/303....... Average Loss for Epoch: 0.020710674784795352\n",
      "Epoch 2......Step: 265/303....... Average Loss for Epoch: 0.020711730385445198\n",
      "Epoch 2......Step: 266/303....... Average Loss for Epoch: 0.020736520850848882\n",
      "Epoch 2......Step: 267/303....... Average Loss for Epoch: 0.02071386290372311\n",
      "Epoch 2......Step: 268/303....... Average Loss for Epoch: 0.020691557398149325\n",
      "Epoch 2......Step: 269/303....... Average Loss for Epoch: 0.020723213971381292\n",
      "Epoch 2......Step: 270/303....... Average Loss for Epoch: 0.02071872045872388\n",
      "Epoch 2......Step: 271/303....... Average Loss for Epoch: 0.02073255726359647\n",
      "Epoch 2......Step: 272/303....... Average Loss for Epoch: 0.020742096383508074\n",
      "Epoch 2......Step: 273/303....... Average Loss for Epoch: 0.020730286863915648\n",
      "Epoch 2......Step: 274/303....... Average Loss for Epoch: 0.020699367776893785\n",
      "Epoch 2......Step: 275/303....... Average Loss for Epoch: 0.02070090667090633\n",
      "Epoch 2......Step: 276/303....... Average Loss for Epoch: 0.020730372072885864\n",
      "Epoch 2......Step: 277/303....... Average Loss for Epoch: 0.02076171017806668\n",
      "Epoch 2......Step: 278/303....... Average Loss for Epoch: 0.02076869086101115\n",
      "Epoch 2......Step: 279/303....... Average Loss for Epoch: 0.020763693384051752\n",
      "Epoch 2......Step: 280/303....... Average Loss for Epoch: 0.020794086404410855\n",
      "Epoch 2......Step: 281/303....... Average Loss for Epoch: 0.020771329284536243\n",
      "Epoch 2......Step: 282/303....... Average Loss for Epoch: 0.020772870705984797\n",
      "Epoch 2......Step: 283/303....... Average Loss for Epoch: 0.020834473132850213\n",
      "Epoch 2......Step: 284/303....... Average Loss for Epoch: 0.020850845727361213\n",
      "Epoch 2......Step: 285/303....... Average Loss for Epoch: 0.02084453742166883\n",
      "Epoch 2......Step: 286/303....... Average Loss for Epoch: 0.020828224637161392\n",
      "Epoch 2......Step: 287/303....... Average Loss for Epoch: 0.020806587359605142\n",
      "Epoch 2......Step: 288/303....... Average Loss for Epoch: 0.020785154166838363\n",
      "Epoch 2......Step: 289/303....... Average Loss for Epoch: 0.020819617654470837\n",
      "Epoch 2......Step: 290/303....... Average Loss for Epoch: 0.020808681441021378\n",
      "Epoch 2......Step: 291/303....... Average Loss for Epoch: 0.02077893500257911\n",
      "Epoch 2......Step: 292/303....... Average Loss for Epoch: 0.020761799370257617\n",
      "Epoch 2......Step: 293/303....... Average Loss for Epoch: 0.020752417727821718\n",
      "Epoch 2......Step: 294/303....... Average Loss for Epoch: 0.02072280343799364\n",
      "Epoch 2......Step: 295/303....... Average Loss for Epoch: 0.02071637273213621\n",
      "Epoch 2......Step: 296/303....... Average Loss for Epoch: 0.02071286437756105\n",
      "Epoch 2......Step: 297/303....... Average Loss for Epoch: 0.020730822274981926\n",
      "Epoch 2......Step: 298/303....... Average Loss for Epoch: 0.020752649935549938\n",
      "Epoch 2......Step: 299/303....... Average Loss for Epoch: 0.02072590588900357\n",
      "Epoch 2......Step: 300/303....... Average Loss for Epoch: 0.020713444172094267\n",
      "Epoch 2......Step: 301/303....... Average Loss for Epoch: 0.020693016705718943\n",
      "Epoch 2......Step: 302/303....... Average Loss for Epoch: 0.020701220715480136\n",
      "Epoch 2......Step: 303/303....... Average Loss for Epoch: 0.020680162158146158\n",
      "Epoch 2/5 Done, Total Loss: 0.020680162158146158\n",
      "Time Elapsed for Epoch: 6.2689968999999905 seconds\n",
      "Epoch 3......Step: 1/303....... Average Loss for Epoch: 0.024786952883005142\n",
      "Epoch 3......Step: 2/303....... Average Loss for Epoch: 0.022258177399635315\n",
      "Epoch 3......Step: 3/303....... Average Loss for Epoch: 0.020605037609736126\n",
      "Epoch 3......Step: 4/303....... Average Loss for Epoch: 0.02286385837942362\n",
      "Epoch 3......Step: 5/303....... Average Loss for Epoch: 0.02111494857817888\n",
      "Epoch 3......Step: 6/303....... Average Loss for Epoch: 0.021700425228724878\n",
      "Epoch 3......Step: 7/303....... Average Loss for Epoch: 0.02079505074237074\n",
      "Epoch 3......Step: 8/303....... Average Loss for Epoch: 0.020168045768514276\n",
      "Epoch 3......Step: 9/303....... Average Loss for Epoch: 0.019871143624186516\n",
      "Epoch 3......Step: 10/303....... Average Loss for Epoch: 0.01924556475132704\n",
      "Epoch 3......Step: 11/303....... Average Loss for Epoch: 0.019882760603319515\n",
      "Epoch 3......Step: 12/303....... Average Loss for Epoch: 0.019310737106328208\n",
      "Epoch 3......Step: 13/303....... Average Loss for Epoch: 0.01886537284232103\n",
      "Epoch 3......Step: 14/303....... Average Loss for Epoch: 0.018691832864923135\n",
      "Epoch 3......Step: 15/303....... Average Loss for Epoch: 0.018394776992499827\n",
      "Epoch 3......Step: 16/303....... Average Loss for Epoch: 0.018090771918650717\n",
      "Epoch 3......Step: 17/303....... Average Loss for Epoch: 0.01834201719611883\n",
      "Epoch 3......Step: 18/303....... Average Loss for Epoch: 0.018613182473927736\n",
      "Epoch 3......Step: 19/303....... Average Loss for Epoch: 0.0184146751110491\n",
      "Epoch 3......Step: 20/303....... Average Loss for Epoch: 0.01848703809082508\n",
      "Epoch 3......Step: 21/303....... Average Loss for Epoch: 0.018190741139863218\n",
      "Epoch 3......Step: 22/303....... Average Loss for Epoch: 0.017903651026162235\n",
      "Epoch 3......Step: 23/303....... Average Loss for Epoch: 0.017720936068698116\n",
      "Epoch 3......Step: 24/303....... Average Loss for Epoch: 0.018007716823679704\n",
      "Epoch 3......Step: 25/303....... Average Loss for Epoch: 0.018021854497492315\n",
      "Epoch 3......Step: 26/303....... Average Loss for Epoch: 0.018239386988660462\n",
      "Epoch 3......Step: 27/303....... Average Loss for Epoch: 0.018273033443148488\n",
      "Epoch 3......Step: 28/303....... Average Loss for Epoch: 0.01839289185591042\n",
      "Epoch 3......Step: 29/303....... Average Loss for Epoch: 0.018744947998945057\n",
      "Epoch 3......Step: 30/303....... Average Loss for Epoch: 0.018681602211048205\n",
      "Epoch 3......Step: 31/303....... Average Loss for Epoch: 0.01857321270771565\n",
      "Epoch 3......Step: 32/303....... Average Loss for Epoch: 0.018803373211994767\n",
      "Epoch 3......Step: 33/303....... Average Loss for Epoch: 0.018604864789681\n",
      "Epoch 3......Step: 34/303....... Average Loss for Epoch: 0.018718589480747196\n",
      "Epoch 3......Step: 35/303....... Average Loss for Epoch: 0.01886950529047421\n",
      "Epoch 3......Step: 36/303....... Average Loss for Epoch: 0.018742482436613903\n",
      "Epoch 3......Step: 37/303....... Average Loss for Epoch: 0.018601040104152384\n",
      "Epoch 3......Step: 38/303....... Average Loss for Epoch: 0.018838722577416582\n",
      "Epoch 3......Step: 39/303....... Average Loss for Epoch: 0.018945191939098712\n",
      "Epoch 3......Step: 40/303....... Average Loss for Epoch: 0.019009328191168608\n",
      "Epoch 3......Step: 41/303....... Average Loss for Epoch: 0.018930378869721074\n",
      "Epoch 3......Step: 42/303....... Average Loss for Epoch: 0.019088976994334234\n",
      "Epoch 3......Step: 43/303....... Average Loss for Epoch: 0.01924873379513968\n",
      "Epoch 3......Step: 44/303....... Average Loss for Epoch: 0.019237634239040992\n",
      "Epoch 3......Step: 45/303....... Average Loss for Epoch: 0.01942591412613789\n",
      "Epoch 3......Step: 46/303....... Average Loss for Epoch: 0.019345903343966474\n",
      "Epoch 3......Step: 47/303....... Average Loss for Epoch: 0.019437168011481456\n",
      "Epoch 3......Step: 48/303....... Average Loss for Epoch: 0.01963053789222613\n",
      "Epoch 3......Step: 49/303....... Average Loss for Epoch: 0.019597717028643404\n",
      "Epoch 3......Step: 50/303....... Average Loss for Epoch: 0.01957923723384738\n",
      "Epoch 3......Step: 51/303....... Average Loss for Epoch: 0.019485887085251948\n",
      "Epoch 3......Step: 52/303....... Average Loss for Epoch: 0.019389868445264604\n",
      "Epoch 3......Step: 53/303....... Average Loss for Epoch: 0.019476015524903557\n",
      "Epoch 3......Step: 54/303....... Average Loss for Epoch: 0.0195508331094903\n",
      "Epoch 3......Step: 55/303....... Average Loss for Epoch: 0.0195939561013471\n",
      "Epoch 3......Step: 56/303....... Average Loss for Epoch: 0.019538312368760153\n",
      "Epoch 3......Step: 57/303....... Average Loss for Epoch: 0.01952019580558204\n",
      "Epoch 3......Step: 58/303....... Average Loss for Epoch: 0.0194709086630108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3......Step: 59/303....... Average Loss for Epoch: 0.0195551495525544\n",
      "Epoch 3......Step: 60/303....... Average Loss for Epoch: 0.019474586565047504\n",
      "Epoch 3......Step: 61/303....... Average Loss for Epoch: 0.019636533513176638\n",
      "Epoch 3......Step: 62/303....... Average Loss for Epoch: 0.019826195322938504\n",
      "Epoch 3......Step: 63/303....... Average Loss for Epoch: 0.019709434286351243\n",
      "Epoch 3......Step: 64/303....... Average Loss for Epoch: 0.019679651988553815\n",
      "Epoch 3......Step: 65/303....... Average Loss for Epoch: 0.019591134729293677\n",
      "Epoch 3......Step: 66/303....... Average Loss for Epoch: 0.019493489159327564\n",
      "Epoch 3......Step: 67/303....... Average Loss for Epoch: 0.019390446926230816\n",
      "Epoch 3......Step: 68/303....... Average Loss for Epoch: 0.01933779819485019\n",
      "Epoch 3......Step: 69/303....... Average Loss for Epoch: 0.019236018881201744\n",
      "Epoch 3......Step: 70/303....... Average Loss for Epoch: 0.019190957210958003\n",
      "Epoch 3......Step: 71/303....... Average Loss for Epoch: 0.019227172208713814\n",
      "Epoch 3......Step: 72/303....... Average Loss for Epoch: 0.019291856662473746\n",
      "Epoch 3......Step: 73/303....... Average Loss for Epoch: 0.01926762023813104\n",
      "Epoch 3......Step: 74/303....... Average Loss for Epoch: 0.019279574824346078\n",
      "Epoch 3......Step: 75/303....... Average Loss for Epoch: 0.01939867779612541\n",
      "Epoch 3......Step: 76/303....... Average Loss for Epoch: 0.01951202071320854\n",
      "Epoch 3......Step: 77/303....... Average Loss for Epoch: 0.019463488832116127\n",
      "Epoch 3......Step: 78/303....... Average Loss for Epoch: 0.01951516469797263\n",
      "Epoch 3......Step: 79/303....... Average Loss for Epoch: 0.019512969458216352\n",
      "Epoch 3......Step: 80/303....... Average Loss for Epoch: 0.019425396563019604\n",
      "Epoch 3......Step: 81/303....... Average Loss for Epoch: 0.01949822919926158\n",
      "Epoch 3......Step: 82/303....... Average Loss for Epoch: 0.019415895772598137\n",
      "Epoch 3......Step: 83/303....... Average Loss for Epoch: 0.019325467095856207\n",
      "Epoch 3......Step: 84/303....... Average Loss for Epoch: 0.01932414053451447\n",
      "Epoch 3......Step: 85/303....... Average Loss for Epoch: 0.01930088212384897\n",
      "Epoch 3......Step: 86/303....... Average Loss for Epoch: 0.019227383008529973\n",
      "Epoch 3......Step: 87/303....... Average Loss for Epoch: 0.019218820093692034\n",
      "Epoch 3......Step: 88/303....... Average Loss for Epoch: 0.019169549362479964\n",
      "Epoch 3......Step: 89/303....... Average Loss for Epoch: 0.019114256616723672\n",
      "Epoch 3......Step: 90/303....... Average Loss for Epoch: 0.019183784826762147\n",
      "Epoch 3......Step: 91/303....... Average Loss for Epoch: 0.019394406271013586\n",
      "Epoch 3......Step: 92/303....... Average Loss for Epoch: 0.01948817375966388\n",
      "Epoch 3......Step: 93/303....... Average Loss for Epoch: 0.019448325678866396\n",
      "Epoch 3......Step: 94/303....... Average Loss for Epoch: 0.019432627041130624\n",
      "Epoch 3......Step: 95/303....... Average Loss for Epoch: 0.01938245187660581\n",
      "Epoch 3......Step: 96/303....... Average Loss for Epoch: 0.01929542302968912\n",
      "Epoch 3......Step: 97/303....... Average Loss for Epoch: 0.019262574622695594\n",
      "Epoch 3......Step: 98/303....... Average Loss for Epoch: 0.019194869581153805\n",
      "Epoch 3......Step: 99/303....... Average Loss for Epoch: 0.019263323349407828\n",
      "Epoch 3......Step: 100/303....... Average Loss for Epoch: 0.019245198806747795\n",
      "Epoch 3......Step: 101/303....... Average Loss for Epoch: 0.019241299839819422\n",
      "Epoch 3......Step: 102/303....... Average Loss for Epoch: 0.01921002699208318\n",
      "Epoch 3......Step: 103/303....... Average Loss for Epoch: 0.01929443889270419\n",
      "Epoch 3......Step: 104/303....... Average Loss for Epoch: 0.019261143394172765\n",
      "Epoch 3......Step: 105/303....... Average Loss for Epoch: 0.01937451973734867\n",
      "Epoch 3......Step: 106/303....... Average Loss for Epoch: 0.0193520163115606\n",
      "Epoch 3......Step: 107/303....... Average Loss for Epoch: 0.019339018222814964\n",
      "Epoch 3......Step: 108/303....... Average Loss for Epoch: 0.01929668415579255\n",
      "Epoch 3......Step: 109/303....... Average Loss for Epoch: 0.01924542078765434\n",
      "Epoch 3......Step: 110/303....... Average Loss for Epoch: 0.019408333242278208\n",
      "Epoch 3......Step: 111/303....... Average Loss for Epoch: 0.019438706178990035\n",
      "Epoch 3......Step: 112/303....... Average Loss for Epoch: 0.019433321589271406\n",
      "Epoch 3......Step: 113/303....... Average Loss for Epoch: 0.019446881858846254\n",
      "Epoch 3......Step: 114/303....... Average Loss for Epoch: 0.019398796233234174\n",
      "Epoch 3......Step: 115/303....... Average Loss for Epoch: 0.0194124740993847\n",
      "Epoch 3......Step: 116/303....... Average Loss for Epoch: 0.019362780891744227\n",
      "Epoch 3......Step: 117/303....... Average Loss for Epoch: 0.019310767635002606\n",
      "Epoch 3......Step: 118/303....... Average Loss for Epoch: 0.01931209021674122\n",
      "Epoch 3......Step: 119/303....... Average Loss for Epoch: 0.019299969429291096\n",
      "Epoch 3......Step: 120/303....... Average Loss for Epoch: 0.019395671295933427\n",
      "Epoch 3......Step: 121/303....... Average Loss for Epoch: 0.019410739606631196\n",
      "Epoch 3......Step: 122/303....... Average Loss for Epoch: 0.019417004667405712\n",
      "Epoch 3......Step: 123/303....... Average Loss for Epoch: 0.019372577688134298\n",
      "Epoch 3......Step: 124/303....... Average Loss for Epoch: 0.01940079221892501\n",
      "Epoch 3......Step: 125/303....... Average Loss for Epoch: 0.019342026330530643\n",
      "Epoch 3......Step: 126/303....... Average Loss for Epoch: 0.019300832368788264\n",
      "Epoch 3......Step: 127/303....... Average Loss for Epoch: 0.019265885362181608\n",
      "Epoch 3......Step: 128/303....... Average Loss for Epoch: 0.019271408040367533\n",
      "Epoch 3......Step: 129/303....... Average Loss for Epoch: 0.01926965056377095\n",
      "Epoch 3......Step: 130/303....... Average Loss for Epoch: 0.019231346507485095\n",
      "Epoch 3......Step: 131/303....... Average Loss for Epoch: 0.019208908408083987\n",
      "Epoch 3......Step: 132/303....... Average Loss for Epoch: 0.019242800518193028\n",
      "Epoch 3......Step: 133/303....... Average Loss for Epoch: 0.01919924284338503\n",
      "Epoch 3......Step: 134/303....... Average Loss for Epoch: 0.01915727703095372\n",
      "Epoch 3......Step: 135/303....... Average Loss for Epoch: 0.01912491809990671\n",
      "Epoch 3......Step: 136/303....... Average Loss for Epoch: 0.019088726305841085\n",
      "Epoch 3......Step: 137/303....... Average Loss for Epoch: 0.01908180172670714\n",
      "Epoch 3......Step: 138/303....... Average Loss for Epoch: 0.019061021203094202\n",
      "Epoch 3......Step: 139/303....... Average Loss for Epoch: 0.019040983284173682\n",
      "Epoch 3......Step: 140/303....... Average Loss for Epoch: 0.019145528366789222\n",
      "Epoch 3......Step: 141/303....... Average Loss for Epoch: 0.019186842611971053\n",
      "Epoch 3......Step: 142/303....... Average Loss for Epoch: 0.01924124822466516\n",
      "Epoch 3......Step: 143/303....... Average Loss for Epoch: 0.019263597680711664\n",
      "Epoch 3......Step: 144/303....... Average Loss for Epoch: 0.019267559407227155\n",
      "Epoch 3......Step: 145/303....... Average Loss for Epoch: 0.019303205011990565\n",
      "Epoch 3......Step: 146/303....... Average Loss for Epoch: 0.01937410665982186\n",
      "Epoch 3......Step: 147/303....... Average Loss for Epoch: 0.01936104947536373\n",
      "Epoch 3......Step: 148/303....... Average Loss for Epoch: 0.019338630372658372\n",
      "Epoch 3......Step: 149/303....... Average Loss for Epoch: 0.01931822408090702\n",
      "Epoch 3......Step: 150/303....... Average Loss for Epoch: 0.019342511637757222\n",
      "Epoch 3......Step: 151/303....... Average Loss for Epoch: 0.019344517313536036\n",
      "Epoch 3......Step: 152/303....... Average Loss for Epoch: 0.019358712585495885\n",
      "Epoch 3......Step: 153/303....... Average Loss for Epoch: 0.01938388534985921\n",
      "Epoch 3......Step: 154/303....... Average Loss for Epoch: 0.01935062842298444\n",
      "Epoch 3......Step: 155/303....... Average Loss for Epoch: 0.019339103089465248\n",
      "Epoch 3......Step: 156/303....... Average Loss for Epoch: 0.01935039112607065\n",
      "Epoch 3......Step: 157/303....... Average Loss for Epoch: 0.019321846805370538\n",
      "Epoch 3......Step: 158/303....... Average Loss for Epoch: 0.019296878801446548\n",
      "Epoch 3......Step: 159/303....... Average Loss for Epoch: 0.019298307334055315\n",
      "Epoch 3......Step: 160/303....... Average Loss for Epoch: 0.019303000677609816\n",
      "Epoch 3......Step: 161/303....... Average Loss for Epoch: 0.01934375508069437\n",
      "Epoch 3......Step: 162/303....... Average Loss for Epoch: 0.019400400609743816\n",
      "Epoch 3......Step: 163/303....... Average Loss for Epoch: 0.019381419458409394\n",
      "Epoch 3......Step: 164/303....... Average Loss for Epoch: 0.01934379265961669\n",
      "Epoch 3......Step: 165/303....... Average Loss for Epoch: 0.01936095667946519\n",
      "Epoch 3......Step: 166/303....... Average Loss for Epoch: 0.019467594836984414\n",
      "Epoch 3......Step: 167/303....... Average Loss for Epoch: 0.019489053106995044\n",
      "Epoch 3......Step: 168/303....... Average Loss for Epoch: 0.0194801736700659\n",
      "Epoch 3......Step: 169/303....... Average Loss for Epoch: 0.019454567285375837\n",
      "Epoch 3......Step: 170/303....... Average Loss for Epoch: 0.019429622919243925\n",
      "Epoch 3......Step: 171/303....... Average Loss for Epoch: 0.019400745603032628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3......Step: 172/303....... Average Loss for Epoch: 0.019375220991584452\n",
      "Epoch 3......Step: 173/303....... Average Loss for Epoch: 0.019401045613033924\n",
      "Epoch 3......Step: 174/303....... Average Loss for Epoch: 0.019410589410141968\n",
      "Epoch 3......Step: 175/303....... Average Loss for Epoch: 0.019382013545504638\n",
      "Epoch 3......Step: 176/303....... Average Loss for Epoch: 0.019380062284075062\n",
      "Epoch 3......Step: 177/303....... Average Loss for Epoch: 0.019404413594238164\n",
      "Epoch 3......Step: 178/303....... Average Loss for Epoch: 0.019376351063798987\n",
      "Epoch 3......Step: 179/303....... Average Loss for Epoch: 0.01943354397575116\n",
      "Epoch 3......Step: 180/303....... Average Loss for Epoch: 0.01944294413034287\n",
      "Epoch 3......Step: 181/303....... Average Loss for Epoch: 0.019444065791096806\n",
      "Epoch 3......Step: 182/303....... Average Loss for Epoch: 0.019449154431880503\n",
      "Epoch 3......Step: 183/303....... Average Loss for Epoch: 0.019426229109611014\n",
      "Epoch 3......Step: 184/303....... Average Loss for Epoch: 0.01942442908235218\n",
      "Epoch 3......Step: 185/303....... Average Loss for Epoch: 0.019416181209522324\n",
      "Epoch 3......Step: 186/303....... Average Loss for Epoch: 0.019562061024849774\n",
      "Epoch 3......Step: 187/303....... Average Loss for Epoch: 0.019588291963791464\n",
      "Epoch 3......Step: 188/303....... Average Loss for Epoch: 0.019581071546974967\n",
      "Epoch 3......Step: 189/303....... Average Loss for Epoch: 0.019591123930045536\n",
      "Epoch 3......Step: 190/303....... Average Loss for Epoch: 0.019609991578679337\n",
      "Epoch 3......Step: 191/303....... Average Loss for Epoch: 0.019606681784410128\n",
      "Epoch 3......Step: 192/303....... Average Loss for Epoch: 0.019579982331682306\n",
      "Epoch 3......Step: 193/303....... Average Loss for Epoch: 0.0195600620852156\n",
      "Epoch 3......Step: 194/303....... Average Loss for Epoch: 0.0195579046489114\n",
      "Epoch 3......Step: 195/303....... Average Loss for Epoch: 0.019575929617843567\n",
      "Epoch 3......Step: 196/303....... Average Loss for Epoch: 0.019558248903640375\n",
      "Epoch 3......Step: 197/303....... Average Loss for Epoch: 0.019530355533216207\n",
      "Epoch 3......Step: 198/303....... Average Loss for Epoch: 0.019541721442958924\n",
      "Epoch 3......Step: 199/303....... Average Loss for Epoch: 0.01951638794499426\n",
      "Epoch 3......Step: 200/303....... Average Loss for Epoch: 0.019538282547146082\n",
      "Epoch 3......Step: 201/303....... Average Loss for Epoch: 0.019569732498989176\n",
      "Epoch 3......Step: 202/303....... Average Loss for Epoch: 0.0195445626476171\n",
      "Epoch 3......Step: 203/303....... Average Loss for Epoch: 0.019574762418352324\n",
      "Epoch 3......Step: 204/303....... Average Loss for Epoch: 0.01955235846724142\n",
      "Epoch 3......Step: 205/303....... Average Loss for Epoch: 0.01956002149367478\n",
      "Epoch 3......Step: 206/303....... Average Loss for Epoch: 0.019593009973867138\n",
      "Epoch 3......Step: 207/303....... Average Loss for Epoch: 0.01958720524151976\n",
      "Epoch 3......Step: 208/303....... Average Loss for Epoch: 0.019579458395198274\n",
      "Epoch 3......Step: 209/303....... Average Loss for Epoch: 0.019560817669203694\n",
      "Epoch 3......Step: 210/303....... Average Loss for Epoch: 0.01956086548577462\n",
      "Epoch 3......Step: 211/303....... Average Loss for Epoch: 0.019539534162853565\n",
      "Epoch 3......Step: 212/303....... Average Loss for Epoch: 0.01953755832224522\n",
      "Epoch 3......Step: 213/303....... Average Loss for Epoch: 0.019534474308711823\n",
      "Epoch 3......Step: 214/303....... Average Loss for Epoch: 0.019492913146337894\n",
      "Epoch 3......Step: 215/303....... Average Loss for Epoch: 0.019486884205320548\n",
      "Epoch 3......Step: 216/303....... Average Loss for Epoch: 0.019472017923059564\n",
      "Epoch 3......Step: 217/303....... Average Loss for Epoch: 0.01945220490634304\n",
      "Epoch 3......Step: 218/303....... Average Loss for Epoch: 0.01942165189533742\n",
      "Epoch 3......Step: 219/303....... Average Loss for Epoch: 0.019408651380948553\n",
      "Epoch 3......Step: 220/303....... Average Loss for Epoch: 0.01940973398106342\n",
      "Epoch 3......Step: 221/303....... Average Loss for Epoch: 0.01940336103268743\n",
      "Epoch 3......Step: 222/303....... Average Loss for Epoch: 0.01940090700497364\n",
      "Epoch 3......Step: 223/303....... Average Loss for Epoch: 0.01936971761637071\n",
      "Epoch 3......Step: 224/303....... Average Loss for Epoch: 0.019434258287739276\n",
      "Epoch 3......Step: 225/303....... Average Loss for Epoch: 0.019401045226388507\n",
      "Epoch 3......Step: 226/303....... Average Loss for Epoch: 0.019385540956812622\n",
      "Epoch 3......Step: 227/303....... Average Loss for Epoch: 0.01938974103246229\n",
      "Epoch 3......Step: 228/303....... Average Loss for Epoch: 0.019391348601825405\n",
      "Epoch 3......Step: 229/303....... Average Loss for Epoch: 0.019386411193091275\n",
      "Epoch 3......Step: 230/303....... Average Loss for Epoch: 0.019390756936500903\n",
      "Epoch 3......Step: 231/303....... Average Loss for Epoch: 0.019372400698691478\n",
      "Epoch 3......Step: 232/303....... Average Loss for Epoch: 0.01940237934267598\n",
      "Epoch 3......Step: 233/303....... Average Loss for Epoch: 0.019442950783081576\n",
      "Epoch 3......Step: 234/303....... Average Loss for Epoch: 0.01944895389959471\n",
      "Epoch 3......Step: 235/303....... Average Loss for Epoch: 0.019432982631978835\n",
      "Epoch 3......Step: 236/303....... Average Loss for Epoch: 0.01944204597039369\n",
      "Epoch 3......Step: 237/303....... Average Loss for Epoch: 0.01949732227620319\n",
      "Epoch 3......Step: 238/303....... Average Loss for Epoch: 0.019520579473342466\n",
      "Epoch 3......Step: 239/303....... Average Loss for Epoch: 0.019533233315718972\n",
      "Epoch 3......Step: 240/303....... Average Loss for Epoch: 0.01951647748161728\n",
      "Epoch 3......Step: 241/303....... Average Loss for Epoch: 0.01949818836548269\n",
      "Epoch 3......Step: 242/303....... Average Loss for Epoch: 0.019478836377759365\n",
      "Epoch 3......Step: 243/303....... Average Loss for Epoch: 0.019461951456166835\n",
      "Epoch 3......Step: 244/303....... Average Loss for Epoch: 0.019438072462703605\n",
      "Epoch 3......Step: 245/303....... Average Loss for Epoch: 0.019436281619175357\n",
      "Epoch 3......Step: 246/303....... Average Loss for Epoch: 0.019417709159869248\n",
      "Epoch 3......Step: 247/303....... Average Loss for Epoch: 0.01939383039047361\n",
      "Epoch 3......Step: 248/303....... Average Loss for Epoch: 0.019378452479178385\n",
      "Epoch 3......Step: 249/303....... Average Loss for Epoch: 0.0193698557026415\n",
      "Epoch 3......Step: 250/303....... Average Loss for Epoch: 0.01935502114892006\n",
      "Epoch 3......Step: 251/303....... Average Loss for Epoch: 0.01932431129642217\n",
      "Epoch 3......Step: 252/303....... Average Loss for Epoch: 0.019305658545197238\n",
      "Epoch 3......Step: 253/303....... Average Loss for Epoch: 0.01930663505340871\n",
      "Epoch 3......Step: 254/303....... Average Loss for Epoch: 0.019360014776457247\n",
      "Epoch 3......Step: 255/303....... Average Loss for Epoch: 0.0193383419849709\n",
      "Epoch 3......Step: 256/303....... Average Loss for Epoch: 0.019318731854582438\n",
      "Epoch 3......Step: 257/303....... Average Loss for Epoch: 0.019320218722383105\n",
      "Epoch 3......Step: 258/303....... Average Loss for Epoch: 0.019323549366938746\n",
      "Epoch 3......Step: 259/303....... Average Loss for Epoch: 0.019327468619930007\n",
      "Epoch 3......Step: 260/303....... Average Loss for Epoch: 0.019301917411100405\n",
      "Epoch 3......Step: 261/303....... Average Loss for Epoch: 0.019299765513546164\n",
      "Epoch 3......Step: 262/303....... Average Loss for Epoch: 0.019306714091990286\n",
      "Epoch 3......Step: 263/303....... Average Loss for Epoch: 0.019295955905442908\n",
      "Epoch 3......Step: 264/303....... Average Loss for Epoch: 0.019278627432261903\n",
      "Epoch 3......Step: 265/303....... Average Loss for Epoch: 0.01926297549650354\n",
      "Epoch 3......Step: 266/303....... Average Loss for Epoch: 0.01925092469200604\n",
      "Epoch 3......Step: 267/303....... Average Loss for Epoch: 0.019234855272127\n",
      "Epoch 3......Step: 268/303....... Average Loss for Epoch: 0.019235672818413423\n",
      "Epoch 3......Step: 269/303....... Average Loss for Epoch: 0.019242017477972356\n",
      "Epoch 3......Step: 270/303....... Average Loss for Epoch: 0.019228559325414674\n",
      "Epoch 3......Step: 271/303....... Average Loss for Epoch: 0.019219800280029044\n",
      "Epoch 3......Step: 272/303....... Average Loss for Epoch: 0.019209528376129183\n",
      "Epoch 3......Step: 273/303....... Average Loss for Epoch: 0.01918936654884409\n",
      "Epoch 3......Step: 274/303....... Average Loss for Epoch: 0.01917378380418803\n",
      "Epoch 3......Step: 275/303....... Average Loss for Epoch: 0.019156028560616753\n",
      "Epoch 3......Step: 276/303....... Average Loss for Epoch: 0.019170886620987152\n",
      "Epoch 3......Step: 277/303....... Average Loss for Epoch: 0.019188641062820002\n",
      "Epoch 3......Step: 278/303....... Average Loss for Epoch: 0.019186684982382136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3......Step: 279/303....... Average Loss for Epoch: 0.019230461475776516\n",
      "Epoch 3......Step: 280/303....... Average Loss for Epoch: 0.019219586619042925\n",
      "Epoch 3......Step: 281/303....... Average Loss for Epoch: 0.01922651258996585\n",
      "Epoch 3......Step: 282/303....... Average Loss for Epoch: 0.019201241156205216\n",
      "Epoch 3......Step: 283/303....... Average Loss for Epoch: 0.01920393987989889\n",
      "Epoch 3......Step: 284/303....... Average Loss for Epoch: 0.019220452139180312\n",
      "Epoch 3......Step: 285/303....... Average Loss for Epoch: 0.019208950405580957\n",
      "Epoch 3......Step: 286/303....... Average Loss for Epoch: 0.019192530881040372\n",
      "Epoch 3......Step: 287/303....... Average Loss for Epoch: 0.019195382833766397\n",
      "Epoch 3......Step: 288/303....... Average Loss for Epoch: 0.01917017782236346\n",
      "Epoch 3......Step: 289/303....... Average Loss for Epoch: 0.019171881576465075\n",
      "Epoch 3......Step: 290/303....... Average Loss for Epoch: 0.01915932892468469\n",
      "Epoch 3......Step: 291/303....... Average Loss for Epoch: 0.019170505042696736\n",
      "Epoch 3......Step: 292/303....... Average Loss for Epoch: 0.019194114793806453\n",
      "Epoch 3......Step: 293/303....... Average Loss for Epoch: 0.01920034603530433\n",
      "Epoch 3......Step: 294/303....... Average Loss for Epoch: 0.01920188645369747\n",
      "Epoch 3......Step: 295/303....... Average Loss for Epoch: 0.01920517488556393\n",
      "Epoch 3......Step: 296/303....... Average Loss for Epoch: 0.019202087678619334\n",
      "Epoch 3......Step: 297/303....... Average Loss for Epoch: 0.01918612957088534\n",
      "Epoch 3......Step: 298/303....... Average Loss for Epoch: 0.019211469858809806\n",
      "Epoch 3......Step: 299/303....... Average Loss for Epoch: 0.019197653736109318\n",
      "Epoch 3......Step: 300/303....... Average Loss for Epoch: 0.019204275558392207\n",
      "Epoch 3......Step: 301/303....... Average Loss for Epoch: 0.01921994793553685\n",
      "Epoch 3......Step: 302/303....... Average Loss for Epoch: 0.019278128834947054\n",
      "Epoch 3......Step: 303/303....... Average Loss for Epoch: 0.01927802604761454\n",
      "Epoch 3/5 Done, Total Loss: 0.01927802604761454\n",
      "Time Elapsed for Epoch: 6.097093000000001 seconds\n",
      "Epoch 4......Step: 1/303....... Average Loss for Epoch: 0.021243954077363014\n",
      "Epoch 4......Step: 2/303....... Average Loss for Epoch: 0.019990811124444008\n",
      "Epoch 4......Step: 3/303....... Average Loss for Epoch: 0.019145294403036434\n",
      "Epoch 4......Step: 4/303....... Average Loss for Epoch: 0.018801040481776\n",
      "Epoch 4......Step: 5/303....... Average Loss for Epoch: 0.017918866686522962\n",
      "Epoch 4......Step: 6/303....... Average Loss for Epoch: 0.017760744784027338\n",
      "Epoch 4......Step: 7/303....... Average Loss for Epoch: 0.01765948547316449\n",
      "Epoch 4......Step: 8/303....... Average Loss for Epoch: 0.017339538550004363\n",
      "Epoch 4......Step: 9/303....... Average Loss for Epoch: 0.01851183817618423\n",
      "Epoch 4......Step: 10/303....... Average Loss for Epoch: 0.018372645787894725\n",
      "Epoch 4......Step: 11/303....... Average Loss for Epoch: 0.018931758505376903\n",
      "Epoch 4......Step: 12/303....... Average Loss for Epoch: 0.01831464438388745\n",
      "Epoch 4......Step: 13/303....... Average Loss for Epoch: 0.019729334861040115\n",
      "Epoch 4......Step: 14/303....... Average Loss for Epoch: 0.019999545466687\n",
      "Epoch 4......Step: 15/303....... Average Loss for Epoch: 0.019706694036722185\n",
      "Epoch 4......Step: 16/303....... Average Loss for Epoch: 0.01990156329702586\n",
      "Epoch 4......Step: 17/303....... Average Loss for Epoch: 0.019998795955496675\n",
      "Epoch 4......Step: 18/303....... Average Loss for Epoch: 0.019793262601726584\n",
      "Epoch 4......Step: 19/303....... Average Loss for Epoch: 0.019471915046635428\n",
      "Epoch 4......Step: 20/303....... Average Loss for Epoch: 0.01925071501173079\n",
      "Epoch 4......Step: 21/303....... Average Loss for Epoch: 0.019041309576658977\n",
      "Epoch 4......Step: 22/303....... Average Loss for Epoch: 0.018665885298766872\n",
      "Epoch 4......Step: 23/303....... Average Loss for Epoch: 0.018534965894144516\n",
      "Epoch 4......Step: 24/303....... Average Loss for Epoch: 0.018517885046700638\n",
      "Epoch 4......Step: 25/303....... Average Loss for Epoch: 0.018543079122900964\n",
      "Epoch 4......Step: 26/303....... Average Loss for Epoch: 0.018465057373619996\n",
      "Epoch 4......Step: 27/303....... Average Loss for Epoch: 0.0182403186681094\n",
      "Epoch 4......Step: 28/303....... Average Loss for Epoch: 0.018302141888333217\n",
      "Epoch 4......Step: 29/303....... Average Loss for Epoch: 0.018475863440283413\n",
      "Epoch 4......Step: 30/303....... Average Loss for Epoch: 0.018375507680078347\n",
      "Epoch 4......Step: 31/303....... Average Loss for Epoch: 0.01847247322720866\n",
      "Epoch 4......Step: 32/303....... Average Loss for Epoch: 0.018362888979027048\n",
      "Epoch 4......Step: 33/303....... Average Loss for Epoch: 0.018132304603403263\n",
      "Epoch 4......Step: 34/303....... Average Loss for Epoch: 0.018018428400597152\n",
      "Epoch 4......Step: 35/303....... Average Loss for Epoch: 0.018228551905070032\n",
      "Epoch 4......Step: 36/303....... Average Loss for Epoch: 0.018367695053004555\n",
      "Epoch 4......Step: 37/303....... Average Loss for Epoch: 0.01862135121749865\n",
      "Epoch 4......Step: 38/303....... Average Loss for Epoch: 0.018648765343976647\n",
      "Epoch 4......Step: 39/303....... Average Loss for Epoch: 0.018768184221325777\n",
      "Epoch 4......Step: 40/303....... Average Loss for Epoch: 0.018730614986270665\n",
      "Epoch 4......Step: 41/303....... Average Loss for Epoch: 0.018699025962410902\n",
      "Epoch 4......Step: 42/303....... Average Loss for Epoch: 0.018707899288052603\n",
      "Epoch 4......Step: 43/303....... Average Loss for Epoch: 0.018550539844084616\n",
      "Epoch 4......Step: 44/303....... Average Loss for Epoch: 0.018596121681515466\n",
      "Epoch 4......Step: 45/303....... Average Loss for Epoch: 0.018477733992040158\n",
      "Epoch 4......Step: 46/303....... Average Loss for Epoch: 0.018414736138251814\n",
      "Epoch 4......Step: 47/303....... Average Loss for Epoch: 0.01837125549370304\n",
      "Epoch 4......Step: 48/303....... Average Loss for Epoch: 0.01830468645008902\n",
      "Epoch 4......Step: 49/303....... Average Loss for Epoch: 0.018320009834608252\n",
      "Epoch 4......Step: 50/303....... Average Loss for Epoch: 0.018303347676992418\n",
      "Epoch 4......Step: 51/303....... Average Loss for Epoch: 0.018260345228162465\n",
      "Epoch 4......Step: 52/303....... Average Loss for Epoch: 0.018302465323358774\n",
      "Epoch 4......Step: 53/303....... Average Loss for Epoch: 0.018305305953858035\n",
      "Epoch 4......Step: 54/303....... Average Loss for Epoch: 0.018220271173588657\n",
      "Epoch 4......Step: 55/303....... Average Loss for Epoch: 0.018480002219704063\n",
      "Epoch 4......Step: 56/303....... Average Loss for Epoch: 0.018372962700336108\n",
      "Epoch 4......Step: 57/303....... Average Loss for Epoch: 0.01847719023690412\n",
      "Epoch 4......Step: 58/303....... Average Loss for Epoch: 0.01839705033163572\n",
      "Epoch 4......Step: 59/303....... Average Loss for Epoch: 0.018271969647099404\n",
      "Epoch 4......Step: 60/303....... Average Loss for Epoch: 0.018236216980343063\n",
      "Epoch 4......Step: 61/303....... Average Loss for Epoch: 0.018240026747960537\n",
      "Epoch 4......Step: 62/303....... Average Loss for Epoch: 0.018181161023676395\n",
      "Epoch 4......Step: 63/303....... Average Loss for Epoch: 0.018168015671627864\n",
      "Epoch 4......Step: 64/303....... Average Loss for Epoch: 0.018198680510977283\n",
      "Epoch 4......Step: 65/303....... Average Loss for Epoch: 0.01827597016325364\n",
      "Epoch 4......Step: 66/303....... Average Loss for Epoch: 0.01843627028618798\n",
      "Epoch 4......Step: 67/303....... Average Loss for Epoch: 0.018454108943245305\n",
      "Epoch 4......Step: 68/303....... Average Loss for Epoch: 0.018419228932436776\n",
      "Epoch 4......Step: 69/303....... Average Loss for Epoch: 0.018433504859390465\n",
      "Epoch 4......Step: 70/303....... Average Loss for Epoch: 0.01846692892057555\n",
      "Epoch 4......Step: 71/303....... Average Loss for Epoch: 0.018430880047905614\n",
      "Epoch 4......Step: 72/303....... Average Loss for Epoch: 0.01837852903796981\n",
      "Epoch 4......Step: 73/303....... Average Loss for Epoch: 0.018418263325033938\n",
      "Epoch 4......Step: 74/303....... Average Loss for Epoch: 0.018468153272830957\n",
      "Epoch 4......Step: 75/303....... Average Loss for Epoch: 0.018408119132121405\n",
      "Epoch 4......Step: 76/303....... Average Loss for Epoch: 0.01834434079692552\n",
      "Epoch 4......Step: 77/303....... Average Loss for Epoch: 0.018363689132905626\n",
      "Epoch 4......Step: 78/303....... Average Loss for Epoch: 0.018338515256077815\n",
      "Epoch 4......Step: 79/303....... Average Loss for Epoch: 0.018271258713889727\n",
      "Epoch 4......Step: 80/303....... Average Loss for Epoch: 0.01826823737937957\n",
      "Epoch 4......Step: 81/303....... Average Loss for Epoch: 0.0182398591466524\n",
      "Epoch 4......Step: 82/303....... Average Loss for Epoch: 0.018297795933194277\n",
      "Epoch 4......Step: 83/303....... Average Loss for Epoch: 0.018300141510834176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4......Step: 84/303....... Average Loss for Epoch: 0.018223719272230352\n",
      "Epoch 4......Step: 85/303....... Average Loss for Epoch: 0.018183379247784615\n",
      "Epoch 4......Step: 86/303....... Average Loss for Epoch: 0.018122757286872973\n",
      "Epoch 4......Step: 87/303....... Average Loss for Epoch: 0.018103883740888244\n",
      "Epoch 4......Step: 88/303....... Average Loss for Epoch: 0.018120683073489505\n",
      "Epoch 4......Step: 89/303....... Average Loss for Epoch: 0.01807262711854798\n",
      "Epoch 4......Step: 90/303....... Average Loss for Epoch: 0.01808177066139049\n",
      "Epoch 4......Step: 91/303....... Average Loss for Epoch: 0.018037858071153635\n",
      "Epoch 4......Step: 92/303....... Average Loss for Epoch: 0.01802524016238749\n",
      "Epoch 4......Step: 93/303....... Average Loss for Epoch: 0.017972497289539667\n",
      "Epoch 4......Step: 94/303....... Average Loss for Epoch: 0.01793250270148224\n",
      "Epoch 4......Step: 95/303....... Average Loss for Epoch: 0.01793548689272843\n",
      "Epoch 4......Step: 96/303....... Average Loss for Epoch: 0.017974970406309392\n",
      "Epoch 4......Step: 97/303....... Average Loss for Epoch: 0.017925541761532885\n",
      "Epoch 4......Step: 98/303....... Average Loss for Epoch: 0.017945681289978782\n",
      "Epoch 4......Step: 99/303....... Average Loss for Epoch: 0.017972367872117145\n",
      "Epoch 4......Step: 100/303....... Average Loss for Epoch: 0.017966089555993676\n",
      "Epoch 4......Step: 101/303....... Average Loss for Epoch: 0.017952400868260623\n",
      "Epoch 4......Step: 102/303....... Average Loss for Epoch: 0.018014124074183843\n",
      "Epoch 4......Step: 103/303....... Average Loss for Epoch: 0.018063886516085526\n",
      "Epoch 4......Step: 104/303....... Average Loss for Epoch: 0.01803015073080762\n",
      "Epoch 4......Step: 105/303....... Average Loss for Epoch: 0.018119188211858272\n",
      "Epoch 4......Step: 106/303....... Average Loss for Epoch: 0.018069793724999676\n",
      "Epoch 4......Step: 107/303....... Average Loss for Epoch: 0.018054124655497966\n",
      "Epoch 4......Step: 108/303....... Average Loss for Epoch: 0.018051376850861642\n",
      "Epoch 4......Step: 109/303....... Average Loss for Epoch: 0.018139974419198453\n",
      "Epoch 4......Step: 110/303....... Average Loss for Epoch: 0.018109810758720746\n",
      "Epoch 4......Step: 111/303....... Average Loss for Epoch: 0.018116979012349703\n",
      "Epoch 4......Step: 112/303....... Average Loss for Epoch: 0.01814033686449485\n",
      "Epoch 4......Step: 113/303....... Average Loss for Epoch: 0.018136363311678963\n",
      "Epoch 4......Step: 114/303....... Average Loss for Epoch: 0.018143164745548314\n",
      "Epoch 4......Step: 115/303....... Average Loss for Epoch: 0.018145337409299352\n",
      "Epoch 4......Step: 116/303....... Average Loss for Epoch: 0.018158582199750275\n",
      "Epoch 4......Step: 117/303....... Average Loss for Epoch: 0.018183167291502666\n",
      "Epoch 4......Step: 118/303....... Average Loss for Epoch: 0.018135083910330373\n",
      "Epoch 4......Step: 119/303....... Average Loss for Epoch: 0.018143170137553156\n",
      "Epoch 4......Step: 120/303....... Average Loss for Epoch: 0.01811456026043743\n",
      "Epoch 4......Step: 121/303....... Average Loss for Epoch: 0.01807972995856823\n",
      "Epoch 4......Step: 122/303....... Average Loss for Epoch: 0.018045727087215323\n",
      "Epoch 4......Step: 123/303....... Average Loss for Epoch: 0.01801555998229641\n",
      "Epoch 4......Step: 124/303....... Average Loss for Epoch: 0.018011101559104938\n",
      "Epoch 4......Step: 125/303....... Average Loss for Epoch: 0.017984745137393475\n",
      "Epoch 4......Step: 126/303....... Average Loss for Epoch: 0.017972168357421953\n",
      "Epoch 4......Step: 127/303....... Average Loss for Epoch: 0.017938562265531286\n",
      "Epoch 4......Step: 128/303....... Average Loss for Epoch: 0.017930375601281412\n",
      "Epoch 4......Step: 129/303....... Average Loss for Epoch: 0.01790672825837089\n",
      "Epoch 4......Step: 130/303....... Average Loss for Epoch: 0.017904126035192838\n",
      "Epoch 4......Step: 131/303....... Average Loss for Epoch: 0.01794090149710879\n",
      "Epoch 4......Step: 132/303....... Average Loss for Epoch: 0.0179367653926778\n",
      "Epoch 4......Step: 133/303....... Average Loss for Epoch: 0.017949385253390425\n",
      "Epoch 4......Step: 134/303....... Average Loss for Epoch: 0.01795502968215898\n",
      "Epoch 4......Step: 135/303....... Average Loss for Epoch: 0.017962649195558494\n",
      "Epoch 4......Step: 136/303....... Average Loss for Epoch: 0.017956205623169595\n",
      "Epoch 4......Step: 137/303....... Average Loss for Epoch: 0.017947319385180942\n",
      "Epoch 4......Step: 138/303....... Average Loss for Epoch: 0.0179720481088304\n",
      "Epoch 4......Step: 139/303....... Average Loss for Epoch: 0.017960084421332363\n",
      "Epoch 4......Step: 140/303....... Average Loss for Epoch: 0.017927674616553955\n",
      "Epoch 4......Step: 141/303....... Average Loss for Epoch: 0.018035510687012198\n",
      "Epoch 4......Step: 142/303....... Average Loss for Epoch: 0.018059531856380717\n",
      "Epoch 4......Step: 143/303....... Average Loss for Epoch: 0.018105489260458445\n",
      "Epoch 4......Step: 144/303....... Average Loss for Epoch: 0.018081490125041455\n",
      "Epoch 4......Step: 145/303....... Average Loss for Epoch: 0.018124148988261306\n",
      "Epoch 4......Step: 146/303....... Average Loss for Epoch: 0.01809934874333135\n",
      "Epoch 4......Step: 147/303....... Average Loss for Epoch: 0.018063907796631053\n",
      "Epoch 4......Step: 148/303....... Average Loss for Epoch: 0.018032920622342342\n",
      "Epoch 4......Step: 149/303....... Average Loss for Epoch: 0.01815634663373032\n",
      "Epoch 4......Step: 150/303....... Average Loss for Epoch: 0.018100634813308716\n",
      "Epoch 4......Step: 151/303....... Average Loss for Epoch: 0.018105103307409793\n",
      "Epoch 4......Step: 152/303....... Average Loss for Epoch: 0.018087601938628052\n",
      "Epoch 4......Step: 153/303....... Average Loss for Epoch: 0.01809705003755155\n",
      "Epoch 4......Step: 154/303....... Average Loss for Epoch: 0.018062608038353457\n",
      "Epoch 4......Step: 155/303....... Average Loss for Epoch: 0.018082065399615994\n",
      "Epoch 4......Step: 156/303....... Average Loss for Epoch: 0.01806055741886107\n",
      "Epoch 4......Step: 157/303....... Average Loss for Epoch: 0.018030202306901954\n",
      "Epoch 4......Step: 158/303....... Average Loss for Epoch: 0.01805365529453641\n",
      "Epoch 4......Step: 159/303....... Average Loss for Epoch: 0.018089585187815647\n",
      "Epoch 4......Step: 160/303....... Average Loss for Epoch: 0.018073658092180266\n",
      "Epoch 4......Step: 161/303....... Average Loss for Epoch: 0.018035151405688028\n",
      "Epoch 4......Step: 162/303....... Average Loss for Epoch: 0.018022377521121576\n",
      "Epoch 4......Step: 163/303....... Average Loss for Epoch: 0.01801045014425838\n",
      "Epoch 4......Step: 164/303....... Average Loss for Epoch: 0.018019291930036936\n",
      "Epoch 4......Step: 165/303....... Average Loss for Epoch: 0.018039603977266585\n",
      "Epoch 4......Step: 166/303....... Average Loss for Epoch: 0.018016282087528562\n",
      "Epoch 4......Step: 167/303....... Average Loss for Epoch: 0.01799985028044906\n",
      "Epoch 4......Step: 168/303....... Average Loss for Epoch: 0.018002626674604557\n",
      "Epoch 4......Step: 169/303....... Average Loss for Epoch: 0.017983274170632897\n",
      "Epoch 4......Step: 170/303....... Average Loss for Epoch: 0.017968403454869986\n",
      "Epoch 4......Step: 171/303....... Average Loss for Epoch: 0.01793337164021898\n",
      "Epoch 4......Step: 172/303....... Average Loss for Epoch: 0.017884010730614495\n",
      "Epoch 4......Step: 173/303....... Average Loss for Epoch: 0.017952357521119146\n",
      "Epoch 4......Step: 174/303....... Average Loss for Epoch: 0.017994574795680486\n",
      "Epoch 4......Step: 175/303....... Average Loss for Epoch: 0.018042349378977505\n",
      "Epoch 4......Step: 176/303....... Average Loss for Epoch: 0.018024490648795934\n",
      "Epoch 4......Step: 177/303....... Average Loss for Epoch: 0.017991875214133896\n",
      "Epoch 4......Step: 178/303....... Average Loss for Epoch: 0.017982675845661523\n",
      "Epoch 4......Step: 179/303....... Average Loss for Epoch: 0.017998471393057422\n",
      "Epoch 4......Step: 180/303....... Average Loss for Epoch: 0.01798407011665404\n",
      "Epoch 4......Step: 181/303....... Average Loss for Epoch: 0.017970155970867496\n",
      "Epoch 4......Step: 182/303....... Average Loss for Epoch: 0.017969790073418684\n",
      "Epoch 4......Step: 183/303....... Average Loss for Epoch: 0.017942037357047934\n",
      "Epoch 4......Step: 184/303....... Average Loss for Epoch: 0.017938566439704078\n",
      "Epoch 4......Step: 185/303....... Average Loss for Epoch: 0.01793455321945854\n",
      "Epoch 4......Step: 186/303....... Average Loss for Epoch: 0.017933865763767754\n",
      "Epoch 4......Step: 187/303....... Average Loss for Epoch: 0.017913879019850715\n",
      "Epoch 4......Step: 188/303....... Average Loss for Epoch: 0.017895017606899776\n",
      "Epoch 4......Step: 189/303....... Average Loss for Epoch: 0.017952783314126825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4......Step: 190/303....... Average Loss for Epoch: 0.017923783824632042\n",
      "Epoch 4......Step: 191/303....... Average Loss for Epoch: 0.01793059951464855\n",
      "Epoch 4......Step: 192/303....... Average Loss for Epoch: 0.017925314935079466\n",
      "Epoch 4......Step: 193/303....... Average Loss for Epoch: 0.017978849429926724\n",
      "Epoch 4......Step: 194/303....... Average Loss for Epoch: 0.017957459271107753\n",
      "Epoch 4......Step: 195/303....... Average Loss for Epoch: 0.017958475964573714\n",
      "Epoch 4......Step: 196/303....... Average Loss for Epoch: 0.0179576040910823\n",
      "Epoch 4......Step: 197/303....... Average Loss for Epoch: 0.01802511407336608\n",
      "Epoch 4......Step: 198/303....... Average Loss for Epoch: 0.018032448545023047\n",
      "Epoch 4......Step: 199/303....... Average Loss for Epoch: 0.01804053937857175\n",
      "Epoch 4......Step: 200/303....... Average Loss for Epoch: 0.018043512124568225\n",
      "Epoch 4......Step: 201/303....... Average Loss for Epoch: 0.018028953075594274\n",
      "Epoch 4......Step: 202/303....... Average Loss for Epoch: 0.01806499389009458\n",
      "Epoch 4......Step: 203/303....... Average Loss for Epoch: 0.01807154924723462\n",
      "Epoch 4......Step: 204/303....... Average Loss for Epoch: 0.018061056500300765\n",
      "Epoch 4......Step: 205/303....... Average Loss for Epoch: 0.018084224745086055\n",
      "Epoch 4......Step: 206/303....... Average Loss for Epoch: 0.018137551360374805\n",
      "Epoch 4......Step: 207/303....... Average Loss for Epoch: 0.01810521048894107\n",
      "Epoch 4......Step: 208/303....... Average Loss for Epoch: 0.018097754509653896\n",
      "Epoch 4......Step: 209/303....... Average Loss for Epoch: 0.01813411255006157\n",
      "Epoch 4......Step: 210/303....... Average Loss for Epoch: 0.01821805127408533\n",
      "Epoch 4......Step: 211/303....... Average Loss for Epoch: 0.0181825981470999\n",
      "Epoch 4......Step: 212/303....... Average Loss for Epoch: 0.018182062890978075\n",
      "Epoch 4......Step: 213/303....... Average Loss for Epoch: 0.018193966376774468\n",
      "Epoch 4......Step: 214/303....... Average Loss for Epoch: 0.01816574737394803\n",
      "Epoch 4......Step: 215/303....... Average Loss for Epoch: 0.018202825445075368\n",
      "Epoch 4......Step: 216/303....... Average Loss for Epoch: 0.018179335644365184\n",
      "Epoch 4......Step: 217/303....... Average Loss for Epoch: 0.01814983743951068\n",
      "Epoch 4......Step: 218/303....... Average Loss for Epoch: 0.01813817877373701\n",
      "Epoch 4......Step: 219/303....... Average Loss for Epoch: 0.01811543807863645\n",
      "Epoch 4......Step: 220/303....... Average Loss for Epoch: 0.018087069275365634\n",
      "Epoch 4......Step: 221/303....... Average Loss for Epoch: 0.018101421340760603\n",
      "Epoch 4......Step: 222/303....... Average Loss for Epoch: 0.018072820063009188\n",
      "Epoch 4......Step: 223/303....... Average Loss for Epoch: 0.018127629988630524\n",
      "Epoch 4......Step: 224/303....... Average Loss for Epoch: 0.018170564772195315\n",
      "Epoch 4......Step: 225/303....... Average Loss for Epoch: 0.018138593745728335\n",
      "Epoch 4......Step: 226/303....... Average Loss for Epoch: 0.01813800809740097\n",
      "Epoch 4......Step: 227/303....... Average Loss for Epoch: 0.018192051286247072\n",
      "Epoch 4......Step: 228/303....... Average Loss for Epoch: 0.018195551816014607\n",
      "Epoch 4......Step: 229/303....... Average Loss for Epoch: 0.018169144489782085\n",
      "Epoch 4......Step: 230/303....... Average Loss for Epoch: 0.018211205817921006\n",
      "Epoch 4......Step: 231/303....... Average Loss for Epoch: 0.0182270138560758\n",
      "Epoch 4......Step: 232/303....... Average Loss for Epoch: 0.018246457346008514\n",
      "Epoch 4......Step: 233/303....... Average Loss for Epoch: 0.01827648240081614\n",
      "Epoch 4......Step: 234/303....... Average Loss for Epoch: 0.018256605352060154\n",
      "Epoch 4......Step: 235/303....... Average Loss for Epoch: 0.01823683961274776\n",
      "Epoch 4......Step: 236/303....... Average Loss for Epoch: 0.018278867014194445\n",
      "Epoch 4......Step: 237/303....... Average Loss for Epoch: 0.018270377109007998\n",
      "Epoch 4......Step: 238/303....... Average Loss for Epoch: 0.01836702730819708\n",
      "Epoch 4......Step: 239/303....... Average Loss for Epoch: 0.018352232214889515\n",
      "Epoch 4......Step: 240/303....... Average Loss for Epoch: 0.018331919998551407\n",
      "Epoch 4......Step: 241/303....... Average Loss for Epoch: 0.018339651972982894\n",
      "Epoch 4......Step: 242/303....... Average Loss for Epoch: 0.018335491103266387\n",
      "Epoch 4......Step: 243/303....... Average Loss for Epoch: 0.01833130387826957\n",
      "Epoch 4......Step: 244/303....... Average Loss for Epoch: 0.018321594582168293\n",
      "Epoch 4......Step: 245/303....... Average Loss for Epoch: 0.018308450494493755\n",
      "Epoch 4......Step: 246/303....... Average Loss for Epoch: 0.018308374614310945\n",
      "Epoch 4......Step: 247/303....... Average Loss for Epoch: 0.01828443580627562\n",
      "Epoch 4......Step: 248/303....... Average Loss for Epoch: 0.018313669025026742\n",
      "Epoch 4......Step: 249/303....... Average Loss for Epoch: 0.01835862788405404\n",
      "Epoch 4......Step: 250/303....... Average Loss for Epoch: 0.018330727592110633\n",
      "Epoch 4......Step: 251/303....... Average Loss for Epoch: 0.01838494049184826\n",
      "Epoch 4......Step: 252/303....... Average Loss for Epoch: 0.018366333104610916\n",
      "Epoch 4......Step: 253/303....... Average Loss for Epoch: 0.018344851015614193\n",
      "Epoch 4......Step: 254/303....... Average Loss for Epoch: 0.018331734096910073\n",
      "Epoch 4......Step: 255/303....... Average Loss for Epoch: 0.01832025210720067\n",
      "Epoch 4......Step: 256/303....... Average Loss for Epoch: 0.01828842187387636\n",
      "Epoch 4......Step: 257/303....... Average Loss for Epoch: 0.018338135802386336\n",
      "Epoch 4......Step: 258/303....... Average Loss for Epoch: 0.018316420083937718\n",
      "Epoch 4......Step: 259/303....... Average Loss for Epoch: 0.018326373339332208\n",
      "Epoch 4......Step: 260/303....... Average Loss for Epoch: 0.018368669878691436\n",
      "Epoch 4......Step: 261/303....... Average Loss for Epoch: 0.018379968987114127\n",
      "Epoch 4......Step: 262/303....... Average Loss for Epoch: 0.018374965895848874\n",
      "Epoch 4......Step: 263/303....... Average Loss for Epoch: 0.018377291208607614\n",
      "Epoch 4......Step: 264/303....... Average Loss for Epoch: 0.018367011798545718\n",
      "Epoch 4......Step: 265/303....... Average Loss for Epoch: 0.01835146889371692\n",
      "Epoch 4......Step: 266/303....... Average Loss for Epoch: 0.018332457831619603\n",
      "Epoch 4......Step: 267/303....... Average Loss for Epoch: 0.018309182867631037\n",
      "Epoch 4......Step: 268/303....... Average Loss for Epoch: 0.018289099596857802\n",
      "Epoch 4......Step: 269/303....... Average Loss for Epoch: 0.018290910139319844\n",
      "Epoch 4......Step: 270/303....... Average Loss for Epoch: 0.018274205719569215\n",
      "Epoch 4......Step: 271/303....... Average Loss for Epoch: 0.01826803609247687\n",
      "Epoch 4......Step: 272/303....... Average Loss for Epoch: 0.01828692477155367\n",
      "Epoch 4......Step: 273/303....... Average Loss for Epoch: 0.018307200039399195\n",
      "Epoch 4......Step: 274/303....... Average Loss for Epoch: 0.018307213015500864\n",
      "Epoch 4......Step: 275/303....... Average Loss for Epoch: 0.018324572088366204\n",
      "Epoch 4......Step: 276/303....... Average Loss for Epoch: 0.018319120660991124\n",
      "Epoch 4......Step: 277/303....... Average Loss for Epoch: 0.018302578065504022\n",
      "Epoch 4......Step: 278/303....... Average Loss for Epoch: 0.01828410296211783\n",
      "Epoch 4......Step: 279/303....... Average Loss for Epoch: 0.01829970387681838\n",
      "Epoch 4......Step: 280/303....... Average Loss for Epoch: 0.01828154222374516\n",
      "Epoch 4......Step: 281/303....... Average Loss for Epoch: 0.018254767205181707\n",
      "Epoch 4......Step: 282/303....... Average Loss for Epoch: 0.018270157452619878\n",
      "Epoch 4......Step: 283/303....... Average Loss for Epoch: 0.018311106586703773\n",
      "Epoch 4......Step: 284/303....... Average Loss for Epoch: 0.01831797032590798\n",
      "Epoch 4......Step: 285/303....... Average Loss for Epoch: 0.018313434808269927\n",
      "Epoch 4......Step: 286/303....... Average Loss for Epoch: 0.018298554169079224\n",
      "Epoch 4......Step: 287/303....... Average Loss for Epoch: 0.01830584001613826\n",
      "Epoch 4......Step: 288/303....... Average Loss for Epoch: 0.01829226842770974\n",
      "Epoch 4......Step: 289/303....... Average Loss for Epoch: 0.01828381631871408\n",
      "Epoch 4......Step: 290/303....... Average Loss for Epoch: 0.018278481278183132\n",
      "Epoch 4......Step: 291/303....... Average Loss for Epoch: 0.0182796660810709\n",
      "Epoch 4......Step: 292/303....... Average Loss for Epoch: 0.018317308882889273\n",
      "Epoch 4......Step: 293/303....... Average Loss for Epoch: 0.018301968955444396\n",
      "Epoch 4......Step: 294/303....... Average Loss for Epoch: 0.018297654925053624\n",
      "Epoch 4......Step: 295/303....... Average Loss for Epoch: 0.01832239788215039\n",
      "Epoch 4......Step: 296/303....... Average Loss for Epoch: 0.018298355267836235\n",
      "Epoch 4......Step: 297/303....... Average Loss for Epoch: 0.018289526726937656\n",
      "Epoch 4......Step: 298/303....... Average Loss for Epoch: 0.01829305061470442\n",
      "Epoch 4......Step: 299/303....... Average Loss for Epoch: 0.01833234661037607\n",
      "Epoch 4......Step: 300/303....... Average Loss for Epoch: 0.018379651218031843\n",
      "Epoch 4......Step: 301/303....... Average Loss for Epoch: 0.018374721315438762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4......Step: 302/303....... Average Loss for Epoch: 0.018358087749991394\n",
      "Epoch 4......Step: 303/303....... Average Loss for Epoch: 0.018355583092781774\n",
      "Epoch 4/5 Done, Total Loss: 0.018355583092781774\n",
      "Time Elapsed for Epoch: 8.683923499999992 seconds\n",
      "Epoch 5......Step: 1/303....... Average Loss for Epoch: 0.02025875449180603\n",
      "Epoch 5......Step: 2/303....... Average Loss for Epoch: 0.022283140569925308\n",
      "Epoch 5......Step: 3/303....... Average Loss for Epoch: 0.02046171762049198\n",
      "Epoch 5......Step: 4/303....... Average Loss for Epoch: 0.018884537275880575\n",
      "Epoch 5......Step: 5/303....... Average Loss for Epoch: 0.018158570490777494\n",
      "Epoch 5......Step: 6/303....... Average Loss for Epoch: 0.017541580833494663\n",
      "Epoch 5......Step: 7/303....... Average Loss for Epoch: 0.016931559225278243\n",
      "Epoch 5......Step: 8/303....... Average Loss for Epoch: 0.016737263300456107\n",
      "Epoch 5......Step: 9/303....... Average Loss for Epoch: 0.01616494616286622\n",
      "Epoch 5......Step: 10/303....... Average Loss for Epoch: 0.01610006708651781\n",
      "Epoch 5......Step: 11/303....... Average Loss for Epoch: 0.01689132082868706\n",
      "Epoch 5......Step: 12/303....... Average Loss for Epoch: 0.016728333818415802\n",
      "Epoch 5......Step: 13/303....... Average Loss for Epoch: 0.01667845105895629\n",
      "Epoch 5......Step: 14/303....... Average Loss for Epoch: 0.016588764797363962\n",
      "Epoch 5......Step: 15/303....... Average Loss for Epoch: 0.016375489408771197\n",
      "Epoch 5......Step: 16/303....... Average Loss for Epoch: 0.01647611625958234\n",
      "Epoch 5......Step: 17/303....... Average Loss for Epoch: 0.016350022188442594\n",
      "Epoch 5......Step: 18/303....... Average Loss for Epoch: 0.016601032215274043\n",
      "Epoch 5......Step: 19/303....... Average Loss for Epoch: 0.016665910860817684\n",
      "Epoch 5......Step: 20/303....... Average Loss for Epoch: 0.016372880898416042\n",
      "Epoch 5......Step: 21/303....... Average Loss for Epoch: 0.016577963229446185\n",
      "Epoch 5......Step: 22/303....... Average Loss for Epoch: 0.016700610433789818\n",
      "Epoch 5......Step: 23/303....... Average Loss for Epoch: 0.01684284436961879\n",
      "Epoch 5......Step: 24/303....... Average Loss for Epoch: 0.016781656925256055\n",
      "Epoch 5......Step: 25/303....... Average Loss for Epoch: 0.016879183277487754\n",
      "Epoch 5......Step: 26/303....... Average Loss for Epoch: 0.016808818130252454\n",
      "Epoch 5......Step: 27/303....... Average Loss for Epoch: 0.01676108843336503\n",
      "Epoch 5......Step: 28/303....... Average Loss for Epoch: 0.01662178786604532\n",
      "Epoch 5......Step: 29/303....... Average Loss for Epoch: 0.016731048420328517\n",
      "Epoch 5......Step: 30/303....... Average Loss for Epoch: 0.016608714777976274\n",
      "Epoch 5......Step: 31/303....... Average Loss for Epoch: 0.01678219218287737\n",
      "Epoch 5......Step: 32/303....... Average Loss for Epoch: 0.016748948401072994\n",
      "Epoch 5......Step: 33/303....... Average Loss for Epoch: 0.01683775976187352\n",
      "Epoch 5......Step: 34/303....... Average Loss for Epoch: 0.016877316020648268\n",
      "Epoch 5......Step: 35/303....... Average Loss for Epoch: 0.016827363068503992\n",
      "Epoch 5......Step: 36/303....... Average Loss for Epoch: 0.016733547957200143\n",
      "Epoch 5......Step: 37/303....... Average Loss for Epoch: 0.016671726228417578\n",
      "Epoch 5......Step: 38/303....... Average Loss for Epoch: 0.016770946891292146\n",
      "Epoch 5......Step: 39/303....... Average Loss for Epoch: 0.01703479327261448\n",
      "Epoch 5......Step: 40/303....... Average Loss for Epoch: 0.017182825645431877\n",
      "Epoch 5......Step: 41/303....... Average Loss for Epoch: 0.017191539995554016\n",
      "Epoch 5......Step: 42/303....... Average Loss for Epoch: 0.017179731368308977\n",
      "Epoch 5......Step: 43/303....... Average Loss for Epoch: 0.017292147892159083\n",
      "Epoch 5......Step: 44/303....... Average Loss for Epoch: 0.017555393574928694\n",
      "Epoch 5......Step: 45/303....... Average Loss for Epoch: 0.01761259444885784\n",
      "Epoch 5......Step: 46/303....... Average Loss for Epoch: 0.0175527335225564\n",
      "Epoch 5......Step: 47/303....... Average Loss for Epoch: 0.01783304409857126\n",
      "Epoch 5......Step: 48/303....... Average Loss for Epoch: 0.017770036686367046\n",
      "Epoch 5......Step: 49/303....... Average Loss for Epoch: 0.01794049320552422\n",
      "Epoch 5......Step: 50/303....... Average Loss for Epoch: 0.017852750904858113\n",
      "Epoch 5......Step: 51/303....... Average Loss for Epoch: 0.01778147754935073\n",
      "Epoch 5......Step: 52/303....... Average Loss for Epoch: 0.017677358441198103\n",
      "Epoch 5......Step: 53/303....... Average Loss for Epoch: 0.017663525294442223\n",
      "Epoch 5......Step: 54/303....... Average Loss for Epoch: 0.017613416227201622\n",
      "Epoch 5......Step: 55/303....... Average Loss for Epoch: 0.01751993926749988\n",
      "Epoch 5......Step: 56/303....... Average Loss for Epoch: 0.017551104854127125\n",
      "Epoch 5......Step: 57/303....... Average Loss for Epoch: 0.017455973744130972\n",
      "Epoch 5......Step: 58/303....... Average Loss for Epoch: 0.017421173044191354\n",
      "Epoch 5......Step: 59/303....... Average Loss for Epoch: 0.017414365989803258\n",
      "Epoch 5......Step: 60/303....... Average Loss for Epoch: 0.01736158588901162\n",
      "Epoch 5......Step: 61/303....... Average Loss for Epoch: 0.01736594797646413\n",
      "Epoch 5......Step: 62/303....... Average Loss for Epoch: 0.017441205199687712\n",
      "Epoch 5......Step: 63/303....... Average Loss for Epoch: 0.017414594630873394\n",
      "Epoch 5......Step: 64/303....... Average Loss for Epoch: 0.017688577296212316\n",
      "Epoch 5......Step: 65/303....... Average Loss for Epoch: 0.017821938859728666\n",
      "Epoch 5......Step: 66/303....... Average Loss for Epoch: 0.017757179060329992\n",
      "Epoch 5......Step: 67/303....... Average Loss for Epoch: 0.017717045745742854\n",
      "Epoch 5......Step: 68/303....... Average Loss for Epoch: 0.01766954004928908\n",
      "Epoch 5......Step: 69/303....... Average Loss for Epoch: 0.017605139792937298\n",
      "Epoch 5......Step: 70/303....... Average Loss for Epoch: 0.017638721303748234\n",
      "Epoch 5......Step: 71/303....... Average Loss for Epoch: 0.01760847778649817\n",
      "Epoch 5......Step: 72/303....... Average Loss for Epoch: 0.017513406074916322\n",
      "Epoch 5......Step: 73/303....... Average Loss for Epoch: 0.017446531072156886\n",
      "Epoch 5......Step: 74/303....... Average Loss for Epoch: 0.01753219643708419\n",
      "Epoch 5......Step: 75/303....... Average Loss for Epoch: 0.017600537451605003\n",
      "Epoch 5......Step: 76/303....... Average Loss for Epoch: 0.017546999574589887\n",
      "Epoch 5......Step: 77/303....... Average Loss for Epoch: 0.017466211171409528\n",
      "Epoch 5......Step: 78/303....... Average Loss for Epoch: 0.01772182371515112\n",
      "Epoch 5......Step: 79/303....... Average Loss for Epoch: 0.017688649600442453\n",
      "Epoch 5......Step: 80/303....... Average Loss for Epoch: 0.017736186902038754\n",
      "Epoch 5......Step: 81/303....... Average Loss for Epoch: 0.01775347763373528\n",
      "Epoch 5......Step: 82/303....... Average Loss for Epoch: 0.017733110705526862\n",
      "Epoch 5......Step: 83/303....... Average Loss for Epoch: 0.017712886276912976\n",
      "Epoch 5......Step: 84/303....... Average Loss for Epoch: 0.01778338829587613\n",
      "Epoch 5......Step: 85/303....... Average Loss for Epoch: 0.01770067802246879\n",
      "Epoch 5......Step: 86/303....... Average Loss for Epoch: 0.0176338060369152\n",
      "Epoch 5......Step: 87/303....... Average Loss for Epoch: 0.017584697150721633\n",
      "Epoch 5......Step: 88/303....... Average Loss for Epoch: 0.017580545348623258\n",
      "Epoch 5......Step: 89/303....... Average Loss for Epoch: 0.017607702183003508\n",
      "Epoch 5......Step: 90/303....... Average Loss for Epoch: 0.017556656348622506\n",
      "Epoch 5......Step: 91/303....... Average Loss for Epoch: 0.01750911707774951\n",
      "Epoch 5......Step: 92/303....... Average Loss for Epoch: 0.017533679685109983\n",
      "Epoch 5......Step: 93/303....... Average Loss for Epoch: 0.01751909199701522\n",
      "Epoch 5......Step: 94/303....... Average Loss for Epoch: 0.017540657486608054\n",
      "Epoch 5......Step: 95/303....... Average Loss for Epoch: 0.017534535407627883\n",
      "Epoch 5......Step: 96/303....... Average Loss for Epoch: 0.017494403019857902\n",
      "Epoch 5......Step: 97/303....... Average Loss for Epoch: 0.017494854392464627\n",
      "Epoch 5......Step: 98/303....... Average Loss for Epoch: 0.017431089569035232\n",
      "Epoch 5......Step: 99/303....... Average Loss for Epoch: 0.017408749295605555\n",
      "Epoch 5......Step: 100/303....... Average Loss for Epoch: 0.01749279420822859\n",
      "Epoch 5......Step: 101/303....... Average Loss for Epoch: 0.01748440516098301\n",
      "Epoch 5......Step: 102/303....... Average Loss for Epoch: 0.017490172247384108\n",
      "Epoch 5......Step: 103/303....... Average Loss for Epoch: 0.017473850833270157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5......Step: 104/303....... Average Loss for Epoch: 0.017539241942218863\n",
      "Epoch 5......Step: 105/303....... Average Loss for Epoch: 0.01755285294992583\n",
      "Epoch 5......Step: 106/303....... Average Loss for Epoch: 0.017521986078892677\n",
      "Epoch 5......Step: 107/303....... Average Loss for Epoch: 0.017549264540669518\n",
      "Epoch 5......Step: 108/303....... Average Loss for Epoch: 0.01750335399992764\n",
      "Epoch 5......Step: 109/303....... Average Loss for Epoch: 0.017528089728855757\n",
      "Epoch 5......Step: 110/303....... Average Loss for Epoch: 0.017456542480398306\n",
      "Epoch 5......Step: 111/303....... Average Loss for Epoch: 0.017410291476292653\n",
      "Epoch 5......Step: 112/303....... Average Loss for Epoch: 0.017459480208344758\n",
      "Epoch 5......Step: 113/303....... Average Loss for Epoch: 0.01741751421045149\n",
      "Epoch 5......Step: 114/303....... Average Loss for Epoch: 0.017388693146865097\n",
      "Epoch 5......Step: 115/303....... Average Loss for Epoch: 0.017406478580897267\n",
      "Epoch 5......Step: 116/303....... Average Loss for Epoch: 0.017364497400884485\n",
      "Epoch 5......Step: 117/303....... Average Loss for Epoch: 0.01730204823339342\n",
      "Epoch 5......Step: 118/303....... Average Loss for Epoch: 0.017276957852102943\n",
      "Epoch 5......Step: 119/303....... Average Loss for Epoch: 0.017271621121453636\n",
      "Epoch 5......Step: 120/303....... Average Loss for Epoch: 0.01725488741261264\n",
      "Epoch 5......Step: 121/303....... Average Loss for Epoch: 0.01721195883525551\n",
      "Epoch 5......Step: 122/303....... Average Loss for Epoch: 0.01718076625380848\n",
      "Epoch 5......Step: 123/303....... Average Loss for Epoch: 0.017191374356426845\n",
      "Epoch 5......Step: 124/303....... Average Loss for Epoch: 0.01716708935677044\n",
      "Epoch 5......Step: 125/303....... Average Loss for Epoch: 0.01713434672355652\n",
      "Epoch 5......Step: 126/303....... Average Loss for Epoch: 0.017088853133221466\n",
      "Epoch 5......Step: 127/303....... Average Loss for Epoch: 0.017071602748721604\n",
      "Epoch 5......Step: 128/303....... Average Loss for Epoch: 0.017033489180903416\n",
      "Epoch 5......Step: 129/303....... Average Loss for Epoch: 0.017040152037732823\n",
      "Epoch 5......Step: 130/303....... Average Loss for Epoch: 0.017045008641882586\n",
      "Epoch 5......Step: 131/303....... Average Loss for Epoch: 0.017031251744345853\n",
      "Epoch 5......Step: 132/303....... Average Loss for Epoch: 0.017034729956790354\n",
      "Epoch 5......Step: 133/303....... Average Loss for Epoch: 0.017025809230885112\n",
      "Epoch 5......Step: 134/303....... Average Loss for Epoch: 0.01700704475280954\n",
      "Epoch 5......Step: 135/303....... Average Loss for Epoch: 0.016999670269864577\n",
      "Epoch 5......Step: 136/303....... Average Loss for Epoch: 0.01698558741276536\n",
      "Epoch 5......Step: 137/303....... Average Loss for Epoch: 0.016980584024240936\n",
      "Epoch 5......Step: 138/303....... Average Loss for Epoch: 0.016971275446585554\n",
      "Epoch 5......Step: 139/303....... Average Loss for Epoch: 0.016983429713697314\n",
      "Epoch 5......Step: 140/303....... Average Loss for Epoch: 0.01696483923255333\n",
      "Epoch 5......Step: 141/303....... Average Loss for Epoch: 0.01694968120848879\n",
      "Epoch 5......Step: 142/303....... Average Loss for Epoch: 0.01693760140628462\n",
      "Epoch 5......Step: 143/303....... Average Loss for Epoch: 0.016990225853307263\n",
      "Epoch 5......Step: 144/303....... Average Loss for Epoch: 0.01702088920865208\n",
      "Epoch 5......Step: 145/303....... Average Loss for Epoch: 0.01701779240935013\n",
      "Epoch 5......Step: 146/303....... Average Loss for Epoch: 0.016981228854354113\n",
      "Epoch 5......Step: 147/303....... Average Loss for Epoch: 0.017017055176147797\n",
      "Epoch 5......Step: 148/303....... Average Loss for Epoch: 0.017080396099167096\n",
      "Epoch 5......Step: 149/303....... Average Loss for Epoch: 0.017073571006923714\n",
      "Epoch 5......Step: 150/303....... Average Loss for Epoch: 0.017056201739857595\n",
      "Epoch 5......Step: 151/303....... Average Loss for Epoch: 0.017056750065788922\n",
      "Epoch 5......Step: 152/303....... Average Loss for Epoch: 0.017085951440477448\n",
      "Epoch 5......Step: 153/303....... Average Loss for Epoch: 0.01707416851974391\n",
      "Epoch 5......Step: 154/303....... Average Loss for Epoch: 0.017070172342483873\n",
      "Epoch 5......Step: 155/303....... Average Loss for Epoch: 0.017032927226635717\n",
      "Epoch 5......Step: 156/303....... Average Loss for Epoch: 0.017031397372006606\n",
      "Epoch 5......Step: 157/303....... Average Loss for Epoch: 0.017062114741487108\n",
      "Epoch 5......Step: 158/303....... Average Loss for Epoch: 0.017084716100104248\n",
      "Epoch 5......Step: 159/303....... Average Loss for Epoch: 0.01706559013036437\n",
      "Epoch 5......Step: 160/303....... Average Loss for Epoch: 0.01710629011504352\n",
      "Epoch 5......Step: 161/303....... Average Loss for Epoch: 0.017131688593892577\n",
      "Epoch 5......Step: 162/303....... Average Loss for Epoch: 0.01718413349195027\n",
      "Epoch 5......Step: 163/303....... Average Loss for Epoch: 0.017208807307518333\n",
      "Epoch 5......Step: 164/303....... Average Loss for Epoch: 0.01719747247492395\n",
      "Epoch 5......Step: 165/303....... Average Loss for Epoch: 0.017191019451076334\n",
      "Epoch 5......Step: 166/303....... Average Loss for Epoch: 0.017228096263505608\n",
      "Epoch 5......Step: 167/303....... Average Loss for Epoch: 0.01727838242080754\n",
      "Epoch 5......Step: 168/303....... Average Loss for Epoch: 0.017285890832898162\n",
      "Epoch 5......Step: 169/303....... Average Loss for Epoch: 0.017266415539004747\n",
      "Epoch 5......Step: 170/303....... Average Loss for Epoch: 0.017264426855699105\n",
      "Epoch 5......Step: 171/303....... Average Loss for Epoch: 0.01729612404818249\n",
      "Epoch 5......Step: 172/303....... Average Loss for Epoch: 0.017280643329370852\n",
      "Epoch 5......Step: 173/303....... Average Loss for Epoch: 0.017314207713211203\n",
      "Epoch 5......Step: 174/303....... Average Loss for Epoch: 0.01728674576714121\n",
      "Epoch 5......Step: 175/303....... Average Loss for Epoch: 0.017312112780553952\n",
      "Epoch 5......Step: 176/303....... Average Loss for Epoch: 0.017306800434281202\n",
      "Epoch 5......Step: 177/303....... Average Loss for Epoch: 0.017290055420312842\n",
      "Epoch 5......Step: 178/303....... Average Loss for Epoch: 0.017322371478370402\n",
      "Epoch 5......Step: 179/303....... Average Loss for Epoch: 0.017309522677691622\n",
      "Epoch 5......Step: 180/303....... Average Loss for Epoch: 0.017335623818346194\n",
      "Epoch 5......Step: 181/303....... Average Loss for Epoch: 0.017375795731516503\n",
      "Epoch 5......Step: 182/303....... Average Loss for Epoch: 0.017355443162659367\n",
      "Epoch 5......Step: 183/303....... Average Loss for Epoch: 0.017352397640139027\n",
      "Epoch 5......Step: 184/303....... Average Loss for Epoch: 0.017355993805129245\n",
      "Epoch 5......Step: 185/303....... Average Loss for Epoch: 0.01737813623370351\n",
      "Epoch 5......Step: 186/303....... Average Loss for Epoch: 0.017389674450681413\n",
      "Epoch 5......Step: 187/303....... Average Loss for Epoch: 0.017372611441953296\n",
      "Epoch 5......Step: 188/303....... Average Loss for Epoch: 0.01738961137078227\n",
      "Epoch 5......Step: 189/303....... Average Loss for Epoch: 0.017360805765425088\n",
      "Epoch 5......Step: 190/303....... Average Loss for Epoch: 0.017363154035257667\n",
      "Epoch 5......Step: 191/303....... Average Loss for Epoch: 0.017325597859099897\n",
      "Epoch 5......Step: 192/303....... Average Loss for Epoch: 0.017359830167454977\n",
      "Epoch 5......Step: 193/303....... Average Loss for Epoch: 0.017348237388762476\n",
      "Epoch 5......Step: 194/303....... Average Loss for Epoch: 0.017406538479298968\n",
      "Epoch 5......Step: 195/303....... Average Loss for Epoch: 0.017457798338280275\n",
      "Epoch 5......Step: 196/303....... Average Loss for Epoch: 0.017434611455632413\n",
      "Epoch 5......Step: 197/303....... Average Loss for Epoch: 0.01747712577565521\n",
      "Epoch 5......Step: 198/303....... Average Loss for Epoch: 0.01745424953505698\n",
      "Epoch 5......Step: 199/303....... Average Loss for Epoch: 0.017445845880438036\n",
      "Epoch 5......Step: 200/303....... Average Loss for Epoch: 0.017505704131908714\n",
      "Epoch 5......Step: 201/303....... Average Loss for Epoch: 0.01750858173015254\n",
      "Epoch 5......Step: 202/303....... Average Loss for Epoch: 0.017587189742428535\n",
      "Epoch 5......Step: 203/303....... Average Loss for Epoch: 0.01757634110436798\n",
      "Epoch 5......Step: 204/303....... Average Loss for Epoch: 0.017588394150758784\n",
      "Epoch 5......Step: 205/303....... Average Loss for Epoch: 0.017574183193103568\n",
      "Epoch 5......Step: 206/303....... Average Loss for Epoch: 0.01755536338585673\n",
      "Epoch 5......Step: 207/303....... Average Loss for Epoch: 0.017554346391040344\n",
      "Epoch 5......Step: 208/303....... Average Loss for Epoch: 0.017536684628934242\n",
      "Epoch 5......Step: 209/303....... Average Loss for Epoch: 0.017528969598443883\n",
      "Epoch 5......Step: 210/303....... Average Loss for Epoch: 0.01755276450089046\n",
      "Epoch 5......Step: 211/303....... Average Loss for Epoch: 0.01755240149972563\n",
      "Epoch 5......Step: 212/303....... Average Loss for Epoch: 0.017547406660357735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5......Step: 213/303....... Average Loss for Epoch: 0.01754590273428131\n",
      "Epoch 5......Step: 214/303....... Average Loss for Epoch: 0.01752723604614768\n",
      "Epoch 5......Step: 215/303....... Average Loss for Epoch: 0.017515735133269497\n",
      "Epoch 5......Step: 216/303....... Average Loss for Epoch: 0.0175865555173476\n",
      "Epoch 5......Step: 217/303....... Average Loss for Epoch: 0.017561350687308245\n",
      "Epoch 5......Step: 218/303....... Average Loss for Epoch: 0.017569980917310497\n",
      "Epoch 5......Step: 219/303....... Average Loss for Epoch: 0.01765133486422774\n",
      "Epoch 5......Step: 220/303....... Average Loss for Epoch: 0.01765771827406504\n",
      "Epoch 5......Step: 221/303....... Average Loss for Epoch: 0.017647454936512454\n",
      "Epoch 5......Step: 222/303....... Average Loss for Epoch: 0.017652284798657034\n",
      "Epoch 5......Step: 223/303....... Average Loss for Epoch: 0.017634988733203957\n",
      "Epoch 5......Step: 224/303....... Average Loss for Epoch: 0.01764758791042758\n",
      "Epoch 5......Step: 225/303....... Average Loss for Epoch: 0.017621631158722773\n",
      "Epoch 5......Step: 226/303....... Average Loss for Epoch: 0.017647682400667563\n",
      "Epoch 5......Step: 227/303....... Average Loss for Epoch: 0.0176771351665521\n",
      "Epoch 5......Step: 228/303....... Average Loss for Epoch: 0.017671329990486827\n",
      "Epoch 5......Step: 229/303....... Average Loss for Epoch: 0.01766153207398528\n",
      "Epoch 5......Step: 230/303....... Average Loss for Epoch: 0.017684763868379853\n",
      "Epoch 5......Step: 231/303....... Average Loss for Epoch: 0.01767068314749054\n",
      "Epoch 5......Step: 232/303....... Average Loss for Epoch: 0.017664617357036935\n",
      "Epoch 5......Step: 233/303....... Average Loss for Epoch: 0.01763742784030192\n",
      "Epoch 5......Step: 234/303....... Average Loss for Epoch: 0.017615808727235623\n",
      "Epoch 5......Step: 235/303....... Average Loss for Epoch: 0.017599699598677614\n",
      "Epoch 5......Step: 236/303....... Average Loss for Epoch: 0.017593970672243227\n",
      "Epoch 5......Step: 237/303....... Average Loss for Epoch: 0.0175734777042823\n",
      "Epoch 5......Step: 238/303....... Average Loss for Epoch: 0.017550385138411232\n",
      "Epoch 5......Step: 239/303....... Average Loss for Epoch: 0.017514462368445914\n",
      "Epoch 5......Step: 240/303....... Average Loss for Epoch: 0.01754723567670832\n",
      "Epoch 5......Step: 241/303....... Average Loss for Epoch: 0.01753976643549209\n",
      "Epoch 5......Step: 242/303....... Average Loss for Epoch: 0.017546306479691475\n",
      "Epoch 5......Step: 243/303....... Average Loss for Epoch: 0.0175448204831454\n",
      "Epoch 5......Step: 244/303....... Average Loss for Epoch: 0.017578083540877847\n",
      "Epoch 5......Step: 245/303....... Average Loss for Epoch: 0.017562253001545156\n",
      "Epoch 5......Step: 246/303....... Average Loss for Epoch: 0.01754088703982108\n",
      "Epoch 5......Step: 247/303....... Average Loss for Epoch: 0.017557825957262806\n",
      "Epoch 5......Step: 248/303....... Average Loss for Epoch: 0.017568724631752457\n",
      "Epoch 5......Step: 249/303....... Average Loss for Epoch: 0.017546739556404\n",
      "Epoch 5......Step: 250/303....... Average Loss for Epoch: 0.017570143707096576\n",
      "Epoch 5......Step: 251/303....... Average Loss for Epoch: 0.017560644424056627\n",
      "Epoch 5......Step: 252/303....... Average Loss for Epoch: 0.017593528897989364\n",
      "Epoch 5......Step: 253/303....... Average Loss for Epoch: 0.01758279134574615\n",
      "Epoch 5......Step: 254/303....... Average Loss for Epoch: 0.017554776501057186\n",
      "Epoch 5......Step: 255/303....... Average Loss for Epoch: 0.017525701429329667\n",
      "Epoch 5......Step: 256/303....... Average Loss for Epoch: 0.01753151872253511\n",
      "Epoch 5......Step: 257/303....... Average Loss for Epoch: 0.017556764975422086\n",
      "Epoch 5......Step: 258/303....... Average Loss for Epoch: 0.017527567670326823\n",
      "Epoch 5......Step: 259/303....... Average Loss for Epoch: 0.017516793805429833\n",
      "Epoch 5......Step: 260/303....... Average Loss for Epoch: 0.01750854647431809\n",
      "Epoch 5......Step: 261/303....... Average Loss for Epoch: 0.01751039850514853\n",
      "Epoch 5......Step: 262/303....... Average Loss for Epoch: 0.01750097920023303\n",
      "Epoch 5......Step: 263/303....... Average Loss for Epoch: 0.017495245463834515\n",
      "Epoch 5......Step: 264/303....... Average Loss for Epoch: 0.0174702467629686\n",
      "Epoch 5......Step: 265/303....... Average Loss for Epoch: 0.01747700604698006\n",
      "Epoch 5......Step: 266/303....... Average Loss for Epoch: 0.017457426536156048\n",
      "Epoch 5......Step: 267/303....... Average Loss for Epoch: 0.01745029157969389\n",
      "Epoch 5......Step: 268/303....... Average Loss for Epoch: 0.017448670759018677\n",
      "Epoch 5......Step: 269/303....... Average Loss for Epoch: 0.017452511763317878\n",
      "Epoch 5......Step: 270/303....... Average Loss for Epoch: 0.017444973390687395\n",
      "Epoch 5......Step: 271/303....... Average Loss for Epoch: 0.017435163376351124\n",
      "Epoch 5......Step: 272/303....... Average Loss for Epoch: 0.017432395526317552\n",
      "Epoch 5......Step: 273/303....... Average Loss for Epoch: 0.017475970157664337\n",
      "Epoch 5......Step: 274/303....... Average Loss for Epoch: 0.01752492293960204\n",
      "Epoch 5......Step: 275/303....... Average Loss for Epoch: 0.017502697357399895\n",
      "Epoch 5......Step: 276/303....... Average Loss for Epoch: 0.017482525466576866\n",
      "Epoch 5......Step: 277/303....... Average Loss for Epoch: 0.01747684540489305\n",
      "Epoch 5......Step: 278/303....... Average Loss for Epoch: 0.017472376866038326\n",
      "Epoch 5......Step: 279/303....... Average Loss for Epoch: 0.017464734236693084\n",
      "Epoch 5......Step: 280/303....... Average Loss for Epoch: 0.017456252400630288\n",
      "Epoch 5......Step: 281/303....... Average Loss for Epoch: 0.017471179909659452\n",
      "Epoch 5......Step: 282/303....... Average Loss for Epoch: 0.017457522115016236\n",
      "Epoch 5......Step: 283/303....... Average Loss for Epoch: 0.01747251590944221\n",
      "Epoch 5......Step: 284/303....... Average Loss for Epoch: 0.017506619233270765\n",
      "Epoch 5......Step: 285/303....... Average Loss for Epoch: 0.017485055454859608\n",
      "Epoch 5......Step: 286/303....... Average Loss for Epoch: 0.01747013431146637\n",
      "Epoch 5......Step: 287/303....... Average Loss for Epoch: 0.017505564464041997\n",
      "Epoch 5......Step: 288/303....... Average Loss for Epoch: 0.017496384318090148\n",
      "Epoch 5......Step: 289/303....... Average Loss for Epoch: 0.017545282466910703\n",
      "Epoch 5......Step: 290/303....... Average Loss for Epoch: 0.01753819167678212\n",
      "Epoch 5......Step: 291/303....... Average Loss for Epoch: 0.01756559508529418\n",
      "Epoch 5......Step: 292/303....... Average Loss for Epoch: 0.017552190099855605\n",
      "Epoch 5......Step: 293/303....... Average Loss for Epoch: 0.017560079894044497\n",
      "Epoch 5......Step: 294/303....... Average Loss for Epoch: 0.01754762195874335\n",
      "Epoch 5......Step: 295/303....... Average Loss for Epoch: 0.017527362467500113\n",
      "Epoch 5......Step: 296/303....... Average Loss for Epoch: 0.01751922432182206\n",
      "Epoch 5......Step: 297/303....... Average Loss for Epoch: 0.017500823391883663\n",
      "Epoch 5......Step: 298/303....... Average Loss for Epoch: 0.01748319698385144\n",
      "Epoch 5......Step: 299/303....... Average Loss for Epoch: 0.017477372106800112\n",
      "Epoch 5......Step: 300/303....... Average Loss for Epoch: 0.017547589677075544\n",
      "Epoch 5......Step: 301/303....... Average Loss for Epoch: 0.017547966757684056\n",
      "Epoch 5......Step: 302/303....... Average Loss for Epoch: 0.0175411311897221\n",
      "Epoch 5......Step: 303/303....... Average Loss for Epoch: 0.01752163855229864\n",
      "Epoch 5/5 Done, Total Loss: 0.01752163855229864\n",
      "Time Elapsed for Epoch: 6.518409699999978 seconds\n",
      "Total Training Time: 33.80273279999997 seconds\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "gru_model = train(train_loader, lr, model_type=\"GRU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Time: 0.06486040000004323\n",
      "sMAPE: 6.231725066693465%\n"
     ]
    }
   ],
   "source": [
    "gru_outputs, targets, gru_sMAPE = evaluate(gru_model, test_x, test_y, label_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800\n"
     ]
    }
   ],
   "source": [
    "print(len(gru_outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1149.1997378 ,  618.90084829,  938.33357972, ...,  289.60656023,\n",
      "        389.43621908,  578.39126877])]\n"
     ]
    }
   ],
   "source": [
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(gru_outputs[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(targets[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
